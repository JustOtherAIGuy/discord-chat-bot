WEBVTT

1
00:00:24.140 --> 00:00:36.912
hugo bowne-anderson: Hey, everyone! We're back in another call. And I appreciate that when when slightly over time. But I I felt everyone was super interested, engaged, and paige was dropping so many

2
00:00:37.660 --> 00:00:45.769
hugo bowne-anderson: I was, gonna say, knowledge bombs, but kind of not even not like it was knowledge, but it was how to explore together, and how and how to build together. And the fact that you know.

3
00:00:45.880 --> 00:01:05.398
hugo bowne-anderson: Mike was like never even knew about Project Mariner and jumped in was like that could be something useful. Incredibly cool. I do want to make clear, though, that as I said in that session, I appreciate how overwhelming the tooling landscape can be at the moment, and that's why I

4
00:01:06.070 --> 00:01:16.049
hugo bowne-anderson: I've said several times. I encourage you all to follow the core curriculum of this of this course, and then branch out and step out when when needed.

5
00:01:16.720 --> 00:01:18.539
hugo bowne-anderson: What else do I want to say about that

6
00:01:18.710 --> 00:01:40.309
hugo bowne-anderson: funnily, so you may recall, Jeff and I had a session just exploring how to use base 10 a couple of weeks ago, I was going to say, in the early days of this course it was 2 weeks ago. It was less. It was like a Thursday, right? It was like a week and a half ago far out. Time is elastic. But to pay Paige's point like go to hackathons, and that type of stuff is such a good point, and

7
00:01:40.740 --> 00:01:42.289
hugo bowne-anderson: I'm happy to

8
00:01:42.640 --> 00:02:05.969
hugo bowne-anderson: to help coordinate these types of things. But let's say a few of you wanted to do a hackathon to see how you could best utilize these gemini credits. I would encourage you all just to set up a meeting or ping me to organize something a bit more robust. But I do think in in the short term setting something up could be super cool. Someone else mentioned that

9
00:02:07.490 --> 00:02:23.060
hugo bowne-anderson: with respect to the tool overwhelm, it'd be great. They don't think that necessarily they'll be able to use all of them by the end of the course, and that's not an expectation it's more for you to play around with, but they said it'd be great to have ongoing support. Now, I think that's something depending on

10
00:02:23.610 --> 00:02:26.427
hugo bowne-anderson: like what my day job looks like.

11
00:02:27.150 --> 00:02:34.830
hugo bowne-anderson: I may be able to help out with after the after the course that I can't really commit at at the moment. But what I do want to say is

12
00:02:35.060 --> 00:03:00.629
hugo bowne-anderson: depending on what you all are interested in part of the vision, for the discord is for you all to keep communicating and corresponding, and were you to set up a session to play around with things that I would love to see that. And the 1st cohort in January. Of course, you know they've been very active in their part of the discord, and you saw them last week in the special guests. Channel, and they continue to collaborate. So that's all to say. I'm very.

13
00:03:01.850 --> 00:03:21.379
hugo bowne-anderson: The intention of this isn't merely a 4 week. Course, I suppose, is is what I'm saying. It is you know I love community as I, as I think, has become apparent. And anything we can do to help each other building after the course. I want to make sure that we're all aware that that's that's part of the mission here as as well. And I'm very open to any any suggestions about how to achieve that

14
00:03:21.750 --> 00:03:33.220
hugo bowne-anderson: also that, all having been said, it's time to jump in to workshop 5. I'm just

15
00:03:33.370 --> 00:03:37.629
hugo bowne-anderson: moving my zoom in order to share my screen.

16
00:03:44.480 --> 00:03:47.230
hugo bowne-anderson: Could someone just give me a thumb up to make sure my screen is visible.

17
00:03:48.110 --> 00:03:53.719
hugo bowne-anderson: Awesome. Many thumbs. I've got a thumb up and a thumb down here. So they, you know, they cancel each other out.

18
00:03:55.210 --> 00:04:19.449
hugo bowne-anderson: so look, today, I've said, what we're talking about is context, embeddings and vector stores. Now, we are. But really the goal. Here is kind of an introduction to ways to think about information retrieval, whether it's embeddings or vector. Stores or whatever we'll go into those. But I really want us to start thinking about how to retrieve information. So we'll get to that in a second.

19
00:04:21.100 --> 00:04:49.520
hugo bowne-anderson: What I do want to say is that we had it seems like you all have been really enjoying the the builders clubs, and I've started uploading them to Youtube. So I'll have a few of them for those who want to check them out very soon. But I appreciate that that the meetings have been at ad hoc, and that was by design to let you all facilitate when's best for you. But I also appreciate. People have busy schedules and families, and many other things going on. So we've decided to set the time.

20
00:04:49.520 --> 00:05:08.640
hugo bowne-anderson: And of course, if people want to change, there is flexibility there. But we've decided to set the time. And I've added them to the Maven Calendar and so you should have received optional calendar invites for Builders club aest builders, club Pt. And after this session I'm going to put out the the Eastern ones as well.

21
00:05:08.830 --> 00:05:11.900
hugo bowne-anderson: also wanted to say so. I want to say

22
00:05:12.190 --> 00:05:26.100
hugo bowne-anderson: there are lots of other optional sessions, also, such as the one with Paige just then. We've got one with Constantine from. Learn prompting. After this we've got. I'm very excited about the multimodal apps one from replicate in in a couple of hours. So

23
00:05:26.200 --> 00:05:52.600
hugo bowne-anderson: it's all. Actually, everything's optional, right? These are just like once again, outside the main scope of of the course. Having said that I would love as many people to attend them as possible for several reasons, the 1st is the unique opportunity to engage with the people who build these tools and who are providing credits. And and this this type of stuff and here, directly from the horses, horse's mouth. So to speak. I don't even know if that's a saying, actually

24
00:05:54.290 --> 00:06:08.320
hugo bowne-anderson: And the other thing is just seeing all the questions on discord, seeing how the speaker gets to engage with you all. It's it's just really nice. So if you're able to attend in real time, that's super super cool, but def no pressure.

25
00:06:09.218 --> 00:06:12.010
hugo bowne-anderson: The other thing I just wanted to.

26
00:06:12.370 --> 00:06:20.419
hugo bowne-anderson: This is kind of brutal, and I thought about not showing it, but it does lead into something I wanted to to discuss, and it does have to do with the tooling landscape. You can see Samuel Colvin.

27
00:06:21.480 --> 00:06:35.129
hugo bowne-anderson: He created pydantic, which we've just been talking about in in the chat. It's it's a wonderful wonderful tool. You can see he's tweeted out again towards Harrison Chase, who created Langchain, you know.

28
00:06:35.746 --> 00:06:50.010
hugo bowne-anderson: I've resisted dumping on Langchain publicly until now, despite how easy it would be, etc. Langsmith caused a well-known AI company to pivot away from Python because it's tracing mess hung for half a second on every function call

29
00:06:50.090 --> 00:07:06.279
hugo bowne-anderson: Langchain is blocked in at least one major public tech company for security reasons. So look, Flame wars aside. That's not really my intention here, although I think it's important to recognize that there's some brutal stuff happening in the space. There are several things that I really want to point out here.

30
00:07:06.760 --> 00:07:08.440
hugo bowne-anderson: The 1st is

31
00:07:09.170 --> 00:07:33.559
hugo bowne-anderson: that if you adopt tools straight away you may get burnt. You may get seriously burned. And this is a point I've been making the whole time. And why I've said, let's start doing this in spreadsheets, and be very mindful when adopting tools when adopting pi test. Stefan. And I really suggest adopting pi test because it has been around for a long time, big community supporting it. Right? So it's really important. Take these things into consideration when

32
00:07:33.810 --> 00:07:42.129
hugo bowne-anderson: adopting tools, and I think Harrison has done an incredible job with the the Langstar, so to speak, ecosystem, I do think, and this is my hot take.

33
00:07:42.760 --> 00:07:44.699
hugo bowne-anderson: He got too much adoption

34
00:07:45.340 --> 00:08:01.750
hugo bowne-anderson: too early, partially because Andrew Ng. Did a course with him on Langchain and I don't think the product and framework, open source or otherwise, was ready for prime. Such significant prime time at that point, because of Andrew Ng. A lot of a lot of people. People jump jumped on.

35
00:08:02.338 --> 00:08:08.070
hugo bowne-anderson: The other reason. And of course this is funny. Then Sam Samuel goes in like retweets.

36
00:08:08.100 --> 00:08:33.400
hugo bowne-anderson: Harrison with with something else. So. But the other point is, you see, because it's tracing mess right? So one of the most important things is not having a tracing mess, and you can see Harrison then posted tracing and Evals so core to building agents that if you're a framework and you're not building your own. I don't take your framework seriously. So I just wanted to really make it clear that

37
00:08:33.809 --> 00:08:35.890
hugo bowne-anderson: doing the tracing and having

38
00:08:37.429 --> 00:08:54.069
hugo bowne-anderson: visibility into your system super important. And to that point I just I got a bit of feedback that the difference between logs and traces and spans last time wasn't evident, and this is something that trips all of us up. And, in fact, last time I chatted with Jeff and Nathan about this before

39
00:08:54.070 --> 00:09:13.390
hugo bowne-anderson: last time, when Hamel gave a talk. Someone asked him about the difference between spans and traces, and he was like, I can't remember, like I literally can't remember. So we're all in good company if Hamel can't remember right. So I apologize for the fuzziness here, I'm going to create another table. But before this session I had a quick chat with Chat Gbt. To get my thoughts in order about the differences.

40
00:09:13.590 --> 00:09:17.559
hugo bowne-anderson: And this is a way I like of understanding the differences right? So

41
00:09:17.990 --> 00:09:45.240
hugo bowne-anderson: a log usually represents a single event or a message. Right? It could be a log, prompt model, output, error message, all of these things. And it's logs are generally collections of structured and unstructured data that then we can, you know, build. And it's usually like the raw, the raw events. And then we can build things to introspect from them, such as spans and and traces. Right? So logs are the foundation for constructing traces and spans.

42
00:09:46.170 --> 00:10:02.539
hugo bowne-anderson: What is a span? It's a single step or operation within a trace, and I haven't defined a trace yet. Right? But we'll get there. So, for example, calling the open AI Api Api, or retrieving from a vector, database or post-processing output.

43
00:10:02.630 --> 00:10:24.350
hugo bowne-anderson: So it would be, you know, the the input the query, the prompt plus the response that would be a span. So we can understand step level execution, timing metadata, structured observability. And I'll give an example in a in a second. A trace would just be the full journey of a request. So user makes a query to

44
00:10:24.690 --> 00:10:36.219
hugo bowne-anderson: to retrieval. Lm, call final output whatever it is, and the traces all of those things together, whereas the span would be, you know, one call within that. Okay, so let's just look at an example. This is the

45
00:10:36.590 --> 00:10:43.409
hugo bowne-anderson: transcript app that that I've built and shared with you all? And should I zoom in a bit with that.

46
00:10:43.960 --> 00:10:46.650
hugo bowne-anderson: it's actually incredibly tough with slides. So

47
00:10:47.080 --> 00:10:49.430
hugo bowne-anderson: what I'm going to do is

48
00:10:50.330 --> 00:10:53.270
hugo bowne-anderson: this is what I like to call the poor man's zoom in

49
00:10:54.880 --> 00:10:57.740
hugo bowne-anderson: Well, don't want to share. Want to do a slideshow.

50
00:10:58.280 --> 00:11:05.309
hugo bowne-anderson: so let's say you don't use this app. And you say, what did Stefan say about using pytest in production?

51
00:11:05.810 --> 00:11:07.070
hugo bowne-anderson: Okay? So

52
00:11:07.280 --> 00:11:30.499
hugo bowne-anderson: receiving the user, input and you can just get a log. So you log that as a query, that's a log. Then what happens with that request? Maybe it goes and searches some embeddings and vector stores. And that's what we're talking about today. You know, it looks at the course transcripts. It says, I want 5 returns. Then the log will be the retrieve chunks, etc, and this is an example where there are no retrieve chunks.

53
00:11:30.827 --> 00:11:41.320
hugo bowne-anderson: So the fallback is triggered where an agent, for example or a function call to go and search the Internet to find the question right? So Youtube search or transcript agent.

54
00:11:41.320 --> 00:11:50.750
hugo bowne-anderson: then maybe we do some re-ranking. The details aren't necessarily important, but these are all the logs that happen. Finally, you get a response.

55
00:11:50.990 --> 00:12:03.026
hugo bowne-anderson: You may do some post processing like check formatting and add citation, and then the final output will be the quote plus a link. So a log is essentially what you decide to log

56
00:12:03.740 --> 00:12:07.259
hugo bowne-anderson: with each of these calls and all this information within

57
00:12:08.846 --> 00:12:11.273
hugo bowne-anderson: a span will be

58
00:12:12.250 --> 00:12:33.089
hugo bowne-anderson: you know, everything logged essentially in one of of these, so it could be the initial input. Then the response, or there being no retrieve chunks that would be a span, and the trace would be everything. You've logged with respect to the whole flow from query to to end. So you'd have

59
00:12:33.710 --> 00:12:35.680
hugo bowne-anderson: you'd have a span about

60
00:12:36.720 --> 00:12:57.049
hugo bowne-anderson: the information retrieval from the vector store. Then you'd have another span about the tool used, and maybe another span about post-processing, and all of these would be the traits. And when we get more to a gentic systems later in the week, and next week we'll do a lot more introspection into these types of things. But it should be obvious now or no, it may be obvious now let me say

61
00:12:59.530 --> 00:13:23.650
hugo bowne-anderson: that when we're looking at you know what we did in spreadsheets, those types of things are looking at spans essentially, and in fact, because it's single call and response, they're looking at traces. So once again, these are all terms that just like, look at the the general message still is, look at your data, right? So they're always to look at your data at different levels of of granularity. So I hope that helped demystify

62
00:13:23.800 --> 00:13:31.730
hugo bowne-anderson: some of some of this, and we'll be going deeper and deeper into these types of things. As the course continues.

63
00:13:32.040 --> 00:13:36.630
hugo bowne-anderson: I'm okay. We are recording. That's good. I'm not concerned for a second. So

64
00:13:37.200 --> 00:13:49.349
hugo bowne-anderson: without further ado, and I don't have my eye on on discord at the moment. But I'll stop every now and then to to see whether there are any questions. If there's anything breaking Nathan or Jeff or William, please feel free to turn your microphone on and

65
00:13:49.660 --> 00:13:53.090
hugo bowne-anderson: and stop me. So I just want to kind of

66
00:13:53.950 --> 00:14:03.090
hugo bowne-anderson: ground ourselves in what we've done so far in workshop one, we built a basic app where we had a gradio front end.

67
00:14:03.660 --> 00:14:14.060
hugo bowne-anderson: We use llama index for some form of content. Context enrichment. We had an Llm Apis that we're calling, and we had some logging right? So we did that relatively, easily, straightforwardly.

68
00:14:15.210 --> 00:14:35.460
hugo bowne-anderson: And then I got you to rip out Llama index. And once again, that's all you know to Harrison's Point and to Samuel's point and to the point I've been making the whole time is Lama Index made it tough to look at your prompts. Right? So let's rip it out and see what's up. So that's essentially what I what I got you all to do there to be able to look at your traces in a spreadsheet, for example, right?

69
00:14:35.580 --> 00:14:37.230
hugo bowne-anderson: So ripping out the framework.

70
00:14:37.600 --> 00:15:02.019
hugo bowne-anderson: then last week we really jumped into evaluation, driven development and started thinking through not only looking at your data, but how to perform basic evaluations, to make your apps a bit more robust and your software development process a bit more robust where you have human labeling. Right? Then you think through some metrics, then you do some error analysis in to figure out where to put your focus. And then

71
00:15:02.040 --> 00:15:09.140
hugo bowne-anderson: you start to scale with Llms as judges and Fuzzy Matching and and that type of stuff. And I think something that's relevant to

72
00:15:09.700 --> 00:15:13.660
hugo bowne-anderson: the talk we just had with respect to Mike Powell's question around Ocr.

73
00:15:13.990 --> 00:15:24.530
hugo bowne-anderson: And this workshop where we'll be doing information retrieval. The amount of times I've seen people try to like use infinite rag or something really cool to improve their information. Retrieval

74
00:15:24.550 --> 00:15:46.920
hugo bowne-anderson: right? But they actually in the end got more lift by improving their Ocr part of the pipeline is wild. Right? So. And when you do this type of error, analysis and look at individual spans and traces, you can see where it where it starts to break down. And once you do this it'll become clear. Oh, if I improve my information retrieval, I'll get 5% lift.

75
00:15:46.920 --> 00:15:57.590
hugo bowne-anderson: If I improve the way I'm ingesting Pdfs, I'll actually get 20% lift right? So it's, you know, once again, use all the multi agentic multiturn, infinite rag, you know

76
00:15:57.620 --> 00:15:58.440
hugo bowne-anderson: one

77
00:15:58.450 --> 00:16:27.859
hugo bowne-anderson: trillion token context windows that that you want. Just know if you're looking to improve your business metrics. This this process will get you a long way. And then in workshop 4. Stefan took us all through the 2 iteration loops of testing in dev and and production, and even introduced the concept how to implement crcd, and these types of things. And we even gave you a blueprint for doing that with Github pull requests.

78
00:16:27.860 --> 00:16:30.959
hugo bowne-anderson: So I'm excited to see if any of you want to play around with.

79
00:16:31.440 --> 00:16:35.320
hugo bowne-anderson: So today, we're going to focus on context.

80
00:16:35.620 --> 00:16:44.520
hugo bowne-anderson: So the basic idea here. And this is something William Horton made made a point of, and IA hundred 10% agree with one of the

81
00:16:47.380 --> 00:16:48.320
hugo bowne-anderson: one of the

82
00:16:48.960 --> 00:16:59.810
hugo bowne-anderson: most impactful use cases for Llms that I've seen in businesses and in personal use is people being able to query internal documents?

83
00:16:59.930 --> 00:17:25.199
hugo bowne-anderson: Right? And so thinking about. Then how do we build systems which allow people to do this robustly, quickly, efficiently, right? So how do we get external context into the Llm system? Right? And how do we make sure it's not like just drawing information from its weights as well? So that's really what we're here to talk about today.

84
00:17:25.390 --> 00:17:32.789
hugo bowne-anderson: To put it another way, and we'll be going through this blog post in detail in the next workshop. This is from.

85
00:17:32.960 --> 00:17:38.540
hugo bowne-anderson: Let me just share the link in discord. And this is a blog post that I you know I'm

86
00:17:39.250 --> 00:17:43.330
hugo bowne-anderson: I really. I mentioned most workshops to be honest.

87
00:17:47.900 --> 00:17:51.482
hugo bowne-anderson: I just saw Jeff's yeah, totally.

88
00:17:53.350 --> 00:18:02.870
hugo bowne-anderson: So this is a blog post called building effective agents. And the basic idea here is that, you know, if we're going to talk about agents, we probably should define it, and we should probably think about

89
00:18:03.310 --> 00:18:05.699
hugo bowne-anderson: how do we move from a single Llm. Call

90
00:18:05.920 --> 00:18:09.734
hugo bowne-anderson: to something more agentic and

91
00:18:10.440 --> 00:18:22.960
hugo bowne-anderson: Barry and Eric at anthropic formulated as augmented Llms. Where we start building different types of workflows that we can think about as being on the agentic continuum. But even before that, knowing that Llms

92
00:18:22.990 --> 00:18:50.670
hugo bowne-anderson: you give it tokens, it gives tokens out okay? And then along the continuum to building agentic type things, then we integrate other types of tooling and properties into the system. So the 1st one to think about really is, is memory right? Well, it may not be the first, st but it's an important one, and we've discussed that, in brief. And that's why I had Eddie from Fondue to come and talk to you about simple ways to think about memory last week.

93
00:18:51.420 --> 00:18:52.979
hugo bowne-anderson: The next one is

94
00:18:53.250 --> 00:19:04.139
hugo bowne-anderson: tool calls, okay, and this is something we're going to be talking about later later this week. And, in fact, for anyone who wants to join the the workshop with ravine next next week, doing a lot of

95
00:19:04.370 --> 00:19:12.019
hugo bowne-anderson: tool calls, using Olama and Gradio and Gemma. 3 are all all locally, which will be super fun. But you could imagine, for example.

96
00:19:12.400 --> 00:19:33.379
hugo bowne-anderson: you want to get the weather today somewhere and the cutoff date of Vlm you're working with was who knows when? And so you can implement a basic tool call to a weather Api. Similarly, you'll notice that chat Gpt. Perhaps these days you'll ask it something. You won't tell it to search the web, but it will recognize that perhaps

97
00:19:33.420 --> 00:19:52.189
hugo bowne-anderson: searching the web will help it. Now, who knows whether that's true or not? To be honest? But you'll see it will dynamically search the web without even being told to, and that's more along the agentic side. But these types of tool uses are incredibly important. And then there's retrieval.

98
00:19:52.190 --> 00:20:08.370
hugo bowne-anderson: which is what we're here to talk talk about today. But the idea is, if you have a corpus of documents, how do you? How do you give that information to your Llm. And how do you? How do you query it. So these are the 3, the 3 pillars of thinking about building agents and thinking about

99
00:20:11.620 --> 00:20:27.299
hugo bowne-anderson: like giving your Llm systems more capabilities. Essentially. We're at the early stages of working with this type of quote unquote software and figuring out how to give it a bit more that it can do to help us? In the work we want to do. Okay? So

100
00:20:27.460 --> 00:20:36.710
hugo bowne-anderson: firstly, why do we even need to add context? Okay, so I think that a lot of this should be obvious. But we want to fill knowledge gaps.

101
00:20:36.940 --> 00:20:46.380
hugo bowne-anderson: Llms are trained on fixed data sets. Can't quote unquote, know everything we can give it external information for domain, specific proprietary, time-sensitive knowledge.

102
00:20:48.030 --> 00:21:16.049
hugo bowne-anderson: it can help ground and increase accuracy. So we've seen systems. And this is something in llama indexes favor. And for you, all of you who played around with the app I've built. I've seen some of you have asked the question that you knew wasn't in the Transcript, and it replied, this isn't in the Transcript, whatever, and you gave it a pass. You're like that's good. It didn't hallucinate something there, right. So grounding in the actual documents can work to constrain the models outputs now still know we do this through prompting, like

103
00:21:16.370 --> 00:21:33.180
hugo bowne-anderson: we literally in the prompt say, hey? If you can't find it, please do not. Pretty. Please do not hallucinate my Llm, you know. So, and that's also the page it like we've got people at Deepmind who are like, just put it in the prompt right? So. And I don't mean to be flippant about that. But it

104
00:21:34.330 --> 00:21:43.149
hugo bowne-anderson: it's a pretty interesting way to build software as far as I'm concerned, and it it works better than other ways. But it is a pretty wild thing to be doing. We get task specific relevance.

105
00:21:43.300 --> 00:21:46.170
hugo bowne-anderson: We can get dynamic and up to date information.

106
00:21:47.200 --> 00:22:01.699
hugo bowne-anderson: and this is actually one of my favorite uses. It can enable explainability. And I know William and I discussed this in in January, and there's some nuance to this, and I'm not actually only talking about rag here. I'll share these links. Actually let me. I've got them all

107
00:22:02.380 --> 00:22:04.480
hugo bowne-anderson: or 2 of them here.

108
00:22:06.220 --> 00:22:09.600
hugo bowne-anderson: And I'm just gonna oh, 3 of them. I'll paste them in discord.

109
00:22:13.040 --> 00:22:23.680
hugo bowne-anderson: I love this title. It's classic. Tim O'reilly. The r and Rag stands for stands for royalties. And what O'reilly has done is they've used rag systems

110
00:22:23.960 --> 00:22:34.537
hugo bowne-anderson: on their books. Sorry they've used the Reg system on their catalog of books so that I can ask a question when I'm in their platform, and it returns

111
00:22:35.440 --> 00:22:43.719
hugo bowne-anderson: sources from particular books, and then I can click through to them, and if the source is helpful for me, then

112
00:22:43.830 --> 00:23:02.309
hugo bowne-anderson: the rag system allows O'reilly to pay royalties to that that author. And I think this is an incredible use of this technology. To be perfectly honest, I think, William, and and correct me if I'm wrong, William. But William made a point that this doesn't mean it has to be rag right? There are other information retrieval systems that we can.

113
00:23:02.460 --> 00:23:08.568
hugo bowne-anderson: you know, used for attribution and that type of stuff. And I totally agree with that that critique

114
00:23:09.210 --> 00:23:26.039
hugo bowne-anderson: But it is something which allows us to think more more robustly through attribution and in classic Tim O'reilly style. He's talked about it as one way to think about fixing AI's original sin. And of course we do live in a space which we find incredibly useful.

115
00:23:26.660 --> 00:23:33.689
hugo bowne-anderson: but where the big labs have essentially sucked up the world's information without paying anything

116
00:23:34.100 --> 00:23:48.679
hugo bowne-anderson: for it. Of course, this isn't new. I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more

117
00:23:48.920 --> 00:23:49.840
hugo bowne-anderson: so.

118
00:23:51.730 --> 00:23:56.420
hugo bowne-anderson: The the stage having been set the 2 major questions, and these are

119
00:23:58.030 --> 00:24:16.120
hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information? We extract to the Llm. And the other question is, how do we get information out of our corpus of documents? And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't

120
00:24:16.390 --> 00:24:35.809
hugo bowne-anderson: really that challenging right? And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context? Let's say we've retrieved a bunch from our documents. Most common techniques

121
00:24:35.820 --> 00:24:47.730
hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective.

122
00:24:48.290 --> 00:25:06.349
hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior.

123
00:25:06.410 --> 00:25:36.200
hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here.

124
00:25:36.450 --> 00:25:37.440
hugo bowne-anderson: So

125
00:25:38.450 --> 00:25:46.189
hugo bowne-anderson: how do we get information out of our corpus of documents? Okay, so I think you're all like, have heard the term rag.

126
00:25:46.470 --> 00:26:00.529
hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn

127
00:26:01.350 --> 00:26:16.359
hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's

128
00:26:16.853 --> 00:26:42.380
hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time.

129
00:26:44.000 --> 00:27:11.729
hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right? You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well

130
00:27:11.730 --> 00:27:31.790
hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways.

131
00:27:31.790 --> 00:27:35.720
hugo bowne-anderson: And then we have hybrid approaches. Now the challenges.

132
00:27:35.780 --> 00:27:41.930
hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful.

133
00:27:42.080 --> 00:27:46.710
hugo bowne-anderson: meaningful parts? And you may notice that if you looked at

134
00:27:46.860 --> 00:27:54.190
hugo bowne-anderson: the code base of the rag system I built on our transcripts, the 1st approximation wasn't even rag yet.

135
00:27:54.190 --> 00:28:23.230
hugo bowne-anderson: and I had the issue that the Workshop Transcript was just too long, like it didn't fit in any context window. Essentially. So I did need to break it up in some way, or of the models I was using. And then thinking through, we saw with like summarization. It may seem like it's easy to think whether something is a good summary or not. But in summarization, when evaluating, we're balancing conciseness and completeness and a variety of things right. And there are trade-offs there, and tensions. And similarly.

136
00:28:23.230 --> 00:28:28.200
hugo bowne-anderson: with these types of information retrieval systems, we want to balance precision. So getting relevant results

137
00:28:28.300 --> 00:28:36.019
hugo bowne-anderson: and recall. So not missing important information, and depending on your use case, like, if you're trying to like.

138
00:28:37.340 --> 00:28:44.210
hugo bowne-anderson: if you're introspecting into your own medical documents or something, perhaps your fine getting

139
00:28:44.600 --> 00:28:59.469
hugo bowne-anderson: more results with some being less relevant. Because you're not missing important ones as well like you're casting the net wider. Right? Perhaps that's a design choice you make, and that. So that comes down to your particular use case.

140
00:29:00.110 --> 00:29:01.120
hugo bowne-anderson: So

141
00:29:01.340 --> 00:29:11.579
hugo bowne-anderson: there are lots of different techniques. I'm not going to go through all of these, but I will just say that nearly all the time. Now you'll use transformer based

142
00:29:12.960 --> 00:29:19.830
hugo bowne-anderson: embeddings. And so we'll go through what embeddings are. But once again it's taking strings and embedding them in a vector space of of some sort.

143
00:29:20.290 --> 00:29:25.161
hugo bowne-anderson: You may not need openai or anything along those lines. There are some wonderful

144
00:29:26.440 --> 00:29:39.789
hugo bowne-anderson: embeddings, models which are way more cost, efficient and supported far more in the open source space that you can find on hugging face. And we'll see, you know several of those today, but I think it is important to recognize that the lineage here

145
00:29:40.120 --> 00:29:50.368
hugo bowne-anderson: that you know bag of words to tfidf to word to vet kind of led us where we are today. So time permitting, we'll go through a bit of that in today's workshop.

146
00:29:50.790 --> 00:30:16.417
hugo bowne-anderson: So what is our job. Our job is to find relevant information in the Corpus based on the prompt. Okay, so we need to quote unquote, understand the prompt. So we to do that. We want to extract the intent or key information from the query, okay, and I would encourage you all. And I I don't think about this enough honestly myself. And oh, there are far smarter people who think about this. But

147
00:30:16.960 --> 00:30:41.199
hugo bowne-anderson: really thinking about building intent based systems where we actually, we don't just want to take a query and return. The most likely thing we actually want to deeply understand, quote unquote the intent of the user. Okay, so it extracting the intent or key information from their query. Then, understanding the prompt, so searching the corpus to find the most relevant chunks documents that align with the prompts intent.

148
00:30:41.670 --> 00:30:44.020
hugo bowne-anderson: So how do we do this?

149
00:30:45.870 --> 00:30:47.990
hugo bowne-anderson: We have out.

150
00:30:48.220 --> 00:31:08.839
hugo bowne-anderson: you know, perhaps, structured data, some unstructured data. I kind of, you know, we talk about natural languages being unstructured data, which is a very like computational way to think about it, because clearly natural language is structured to claim it's unstructured is complete nonsense, right? But you know, in in the technical space we do call it that. So we take all of these things. That's our corpus.

151
00:31:09.100 --> 00:31:10.213
hugo bowne-anderson: Then we

152
00:31:10.910 --> 00:31:17.040
hugo bowne-anderson: use some vector database or something simpler. And I'll talk through how to think about what tools to adopt in a second

153
00:31:17.040 --> 00:31:46.320
hugo bowne-anderson: in a text embedding model. Then, based on the query, we retrieve some chunks and then generate the response, using a large language model. Okay? And you'll note that we are using, quote unquote AI and Embeddings and that type of stuff. But we're not using generative capabilities until the very end. To be very clear. Right? And that's important to recognize. That's another thing. Think about what parts of your system need generative capabilities. Maybe it's the only the final step right? And you don't need to ping expensive Apis anywhere else in the system.

154
00:31:47.250 --> 00:31:48.210
hugo bowne-anderson: So then.

155
00:31:49.020 --> 00:32:08.060
hugo bowne-anderson: I think it's important to think about why even care about embeddings? Okay? And the punchline may be already obvious. But it's garbage in garbage out right. If we're putting crap into our system, we'll get crap out. So we want to make sure that how we're encoding our information has as much high signal for our use case as possible.

156
00:32:09.160 --> 00:32:10.200
hugo bowne-anderson: So then.

157
00:32:10.350 --> 00:32:31.122
hugo bowne-anderson: I've said in words what embeddings are, but to say a bit more essentially. It's language to math right or maths, as we say in in Australia and and a few other places. Right? So word to Vec is an example where we take words and use some sort of

158
00:32:33.070 --> 00:32:53.390
hugo bowne-anderson: computational system and map based system to convert them into numbers. Okay, and of course, bag of words is one way. Tfidf is another word to vet, then transformer based stuff. So there are lots of ways to do this. And something that we saw around a decade ago. Now some some wild things started happening, and it seems quaint today. But

159
00:32:53.390 --> 00:33:04.649
hugo bowne-anderson: we all saw this emerge, and when Radeem was building. You know a bunch of word, 2 vex stuff at Gensim which we'll see a bit of today. Hopefully. You know. Suddenly it became apparent that if you embed these words.

160
00:33:04.650 --> 00:33:18.590
hugo bowne-anderson: what you get is king minus queen. So this vector is almost the same vector as man minus. Woman. Okay. Now, what that means in vector, arithmetic is that man?

161
00:33:20.950 --> 00:33:25.539
hugo bowne-anderson: Oh, let me get this right? Yeah. Man minus woman plus queen.

162
00:33:26.550 --> 00:33:42.990
hugo bowne-anderson: you actually end up with with King right? And what that means is that you've got man minus woman, plus some sort of female royalty, or something of that gives you male royalty. So some. All that's to say is that some form of semantic relationships

163
00:33:43.180 --> 00:34:10.379
hugo bowne-anderson: are preserved which is incredibly important and useful. Now, I do want to say this example, is particularly gendered and almost archaic. I, you know, having said that, you know Australia is a member of the Commonwealth, and we we joke in Australia that England will likely become a republic before Australia does. So. But having said that, yeah, it seems a bit traditional and gendered. Having said that

164
00:34:10.840 --> 00:34:27.810
hugo bowne-anderson: most of human literature combined. Is that so? That's likely. Why this emerged as follows, and you can see other forms of relationships. So these are semantic relationships that we see in vector spaces. And you get syntactic relationships as well. So we have the superlative and whatever

165
00:34:28.120 --> 00:34:29.270
hugo bowne-anderson: superlative.

166
00:34:29.780 --> 00:34:37.679
hugo bowne-anderson: comparative and not whatever the the 1st one is so biggest, minus big is similar to smallest, smallest minus minus small.

167
00:34:38.002 --> 00:35:04.059
hugo bowne-anderson: Now, this is an example that I've gone through in detail in the notebook that I hope we have time to talk about. Today. I can record a video. It's pretty self explanatory if we don't have time to go through it all. But it's the Ag news data set. From hugging face massive data set involving a lot of different news news articles from all over the place, and what we actually see when we perform it. Sorry to jump around when we perform an embedding.

168
00:35:04.060 --> 00:35:28.109
hugo bowne-anderson: This is this is some form of like unsupervised learning vibes. In some sense like we perform an embedding. We don't have label data per se. Then we can start to cluster things in the embedding space right now. This is this is one. Once again, a decade ago a friend of mine, he was working on a product which ingested a lot of Wikipedia pages, and then did word 2 vec. And then did some clustering, and we immediately saw

169
00:35:28.180 --> 00:35:45.029
hugo bowne-anderson: geography as a cluster, history as a cluster political science as a cluster that's fascinating and means we're on the right track. It doesn't mean we've got everything sorted, but it means that we're able to encode important information through through this process. Now, with this Ag news data set.

170
00:35:45.160 --> 00:35:56.819
hugo bowne-anderson: what we see is when I performed a clustering that you've got all the code for in the notebook we see cluster 2 Wall Street oil and economy cloud stocks, oil prices, store stocks.

171
00:35:57.130 --> 00:36:06.739
hugo bowne-anderson: financial market cluster, 2, right cluster, 3 researchers seek to untangle email. 3 victim of a success. Microsoft

172
00:36:06.920 --> 00:36:34.630
hugo bowne-anderson: switching titles at Dell Dell. 6 geeks had a digital nightmare that changed the culture. It's about to get far creepier, whatever tech right? So we've got, we've got a finance cluster, it seems, and a tech cluster. Okay, and once again, this was not through hand labeling or anything along those lines. That isn't to say, of course, don't hand label, but encoding text in vector, spaces, through embedding models, captures.

173
00:36:34.670 --> 00:36:44.990
hugo bowne-anderson: a huge amount of semantic information. And what we see with the Ag news data set is when we do this and visualize using Tsne, which is one of a handful of

174
00:36:48.590 --> 00:36:51.870
hugo bowne-anderson: types of embeddings and visualization that

175
00:36:52.100 --> 00:37:12.040
hugo bowne-anderson: seem to encode a lot of information and high signal information. As you can see here. So when we do a t snee visualization here. What we see is all of these clusters right? And what you can see is that you know this one will be the finance one, and or this one will be the tech one, and and so on. Right? So we see the clusters, clusters emerge, which which I find incredibly powerful still.

176
00:37:12.520 --> 00:37:17.650
hugo bowne-anderson: Now, how do we measure distance in embedding space?

177
00:37:18.280 --> 00:37:35.209
hugo bowne-anderson: The short answer is, I don't care at all how you do it. You do it. Whatever way works for you. If you set up an evaluation system and and even a minimum one and see that one does well use that one. Okay, if there's 1 that does slightly better

178
00:37:35.520 --> 00:37:55.809
hugo bowne-anderson: use that unless it's significantly more expensive. I mean, you know, Linkedin is like rabbit about. Oh, cosine similarity is dead, and I'm just like, Can you like leave my feed, please? Right? There are lots of different ones. Cosine similarity has always gotten me a long way. Similarly we had a fantastic question

179
00:37:55.890 --> 00:38:17.480
hugo bowne-anderson: yesterday on discord around using graph rag. And these types of things. And I think these types of technologies are are fantastic and maybe maybe may become state of the art, and we should consider adopting them. Having said that there are a lot of people on Linkedin who will tell you rag is dead because you need infinite rag or graph rag, and I would encourage you all

180
00:38:18.990 --> 00:38:20.670
hugo bowne-anderson: to not listen to Linkedin

181
00:38:21.578 --> 00:38:29.149
hugo bowne-anderson: and once again, you haven't evaluate a basic evaluation system. If rag is working great, if improving, your trunking strategy

182
00:38:29.280 --> 00:38:55.169
hugo bowne-anderson: makes it a bit better improving how you read Pdfs makes it a bit better if it's not good enough, then try increasingly sophisticated technologies. If you're really hungry, if your rags working well and you really want to explore graph rag. Not only would I never stop you doing that. I would encourage you to follow your passions definitely. Just know that there's a cost to exploring all the tooling landscape? And being potentially distracted by shiny things. Okay.

183
00:38:56.930 --> 00:38:57.730
hugo bowne-anderson: Now.

184
00:38:59.070 --> 00:39:12.809
hugo bowne-anderson: people often ask me, when do I use vector, databases? When should I go to something more enterprisy once again. I think you could all probably write a slide like this. Now, knowing my philosophy and Stefan's philosophy and the general philosophy of this course.

185
00:39:13.330 --> 00:39:20.449
hugo bowne-anderson: start with something small, build an Mvp. Then progress as needs arise. So in development.

186
00:39:20.820 --> 00:39:24.250
hugo bowne-anderson: you can store everything in a numpy array. Right?

187
00:39:24.580 --> 00:39:29.980
hugo bowne-anderson: If you need speed. Maybe you want to start thinking about polars or something like that. Or if you want to introspect more

188
00:39:30.190 --> 00:39:34.549
hugo bowne-anderson: and look at your data frames, maybe even pandas right? Now.

189
00:39:34.900 --> 00:39:41.899
hugo bowne-anderson: remember, though, we're like comparing queries against things in wherever we're storing, embedding if that's too slow.

190
00:39:42.270 --> 00:39:45.519
hugo bowne-anderson: that's because Numpy isn't optimized for this and pandas and polars

191
00:39:45.740 --> 00:39:50.900
hugo bowne-anderson: and optimize for this. So then maybe start thinking about adopting a vector index right

192
00:39:51.670 --> 00:39:58.680
hugo bowne-anderson: after that, if you're dealing with larger data sets and larger embeddings that maybe can't fit in memory.

193
00:40:01.860 --> 00:40:08.270
hugo bowne-anderson: Then think about adopting something like chroma or quadrant as vector databases. And I'll be showing you chroma today.

194
00:40:08.840 --> 00:40:11.360
hugo bowne-anderson: And once again, why, chroma, it's

195
00:40:11.470 --> 00:40:18.950
hugo bowne-anderson: the most popular, well supported, open source package with a big community around it that I don't think will go away in the next several years.

196
00:40:19.330 --> 00:40:35.779
hugo bowne-anderson: Right? And then, if you want to adopt something, enterprise ready, maybe you want vector systems like pine cone. And we Va, these also include things like being able to view your traces and telemetry and all of those types of things. But those those are not the reasons. I'd get something enterprise ready. And I

197
00:40:37.040 --> 00:40:41.080
hugo bowne-anderson: I want to be careful, because there are a lot of good reasons to get enterprise

198
00:40:41.320 --> 00:40:45.517
hugo bowne-anderson: ready systems. I I think one of the number. One reasons is having

199
00:40:46.160 --> 00:41:11.270
hugo bowne-anderson: a throat to choke, and what I mean by that is, if you have an enterprise ready tool, you can call someone on the phone hopefully and and be like, Hey, what's up? And that will respond immediately and problem solve for you. And that's 1 of the biggest reasons to adopt enterprise, ready tools. Essentially, this is a table that describes a lot of things I just said, start with matrices and arrays.

200
00:41:11.430 --> 00:41:15.710
hugo bowne-anderson: If you want structured vector operations with metadata. Think about tables.

201
00:41:16.080 --> 00:41:21.280
hugo bowne-anderson: then go to vector indices, to indexes, indices for faster search

202
00:41:21.820 --> 00:41:30.229
hugo bowne-anderson: vector Debs for persistent storage. And then vector systems for enterprise ready. And once again, these are merely heuristics. You do you? But these are some of the the

203
00:41:30.660 --> 00:41:35.769
hugo bowne-anderson: the patterns that I've seen be more productive for people when choosing what what tools to adopt.

204
00:41:36.490 --> 00:41:37.320
hugo bowne-anderson: So

205
00:41:37.480 --> 00:41:45.856
hugo bowne-anderson: now we start getting into the fun stuff. I mean, that was all all fun. But this is the the real fun stuff. We were talking about the systems we're we're building

206
00:41:46.380 --> 00:41:48.179
hugo bowne-anderson: I think I've said this before. But I

207
00:41:48.320 --> 00:41:51.430
hugo bowne-anderson: you know I've I've been teaching things like this for

208
00:41:51.630 --> 00:41:56.260
hugo bowne-anderson: I mean far out decades. Now that need to

209
00:41:56.450 --> 00:42:23.050
hugo bowne-anderson: talk to my therapist about that. But like a lot of my previous teaching work like had to focus on Apis and teaching people how to like wrangle code and that that type of stuff. And now, with the advent of AI coding assistance, which I'm really excited to have seen a bunch of you experimenting with Vibe coding. And and this this type of stuff. And I think it was. Was it? Was it your comment, Greg? During pages talk that you know with AI assisted coding, we can move

210
00:42:23.200 --> 00:42:27.110
hugo bowne-anderson: faster than the speed of thought. Now, right? Which is which I mean is.

211
00:42:27.320 --> 00:42:39.049
hugo bowne-anderson: it's it's it's incredible what we're capable of now. But I'm really excited about the now the ability not just to teach Apis, but really to teach our system building building systems. And so in a rag system.

212
00:42:39.490 --> 00:42:40.780
hugo bowne-anderson: we essentially

213
00:42:41.080 --> 00:42:58.149
hugo bowne-anderson: have a user prompt which we create an embedding for using some embedding model. And then we have a vector database of our documents or corpus in embeddings. And we use cosine similarity, or whatever it is. Once again, I don't really care which one you use, and we can

214
00:42:58.170 --> 00:43:26.029
hugo bowne-anderson: compare them on Linkedin. But we, what we do is we take the query embedding and compare it to all the embeddings in the corpus, and just get the top n, where N. Is 3, 5, 7, 9, depending on what you're interested in. Get the top chunks. Then we combine the chunks and feed them to an Llm. Api and as we saw before, maybe in a template, or maybe saying, these are the chunks. Give a response, and then we return the generated response

215
00:43:26.030 --> 00:43:28.300
hugo bowne-anderson: to the user. Now.

216
00:43:28.300 --> 00:43:39.549
hugo bowne-anderson: lots of ways to do this as we've seen. You can have gemini here, Openai. Here you can have open AI here, hugging faces and embedding model chroma can be your vector, database.

217
00:43:41.030 --> 00:43:46.119
hugo bowne-anderson: some really wonderful ways to do this, actually. And this is what we're going to do

218
00:43:47.720 --> 00:44:12.347
hugo bowne-anderson: in this workshop is use a hugging face embedding model and use chroma and numpy to to store our embeddings. And we're actually going to use Gpt 2, which we can just get from hugging face. In order to call that as an Api, and to be clear like it might be like Gpt. 2. Is an ancient technology, now totally agree in some some ways.

219
00:44:13.140 --> 00:44:39.970
hugo bowne-anderson: but if we only need this for a generative response, perhaps doing that type of thing is actually actually quite chill. And for anyone who hasn't. If you want to experiment with building your own models. Training. Gpt. 2 from scratch is super fun, relatively straightforward. I'll include a link later. I'm pretty sure I remember. But Sebastian Rajkar has some great resources on training Gpt to from scratch using using pytorch.

220
00:44:40.500 --> 00:44:44.300
hugo bowne-anderson: So this is what we'll be doing today. So

221
00:44:44.900 --> 00:44:55.780
hugo bowne-anderson: that slides for after this. So I'm going to bring up the notebook I would love if there are any questions in discord that people would like to chat about. Now I'm happy to

222
00:44:57.670 --> 00:45:04.519
hugo bowne-anderson: answer. Is there anything Nathan or Jeff, that you think we should chat about now, or can we do it all in in discord.

223
00:45:12.960 --> 00:45:19.260
Nathan Danielsen: Well, there's a good one about. How do you evaluate the accuracy of embeddings?

224
00:45:20.090 --> 00:45:22.230
Nathan Danielsen: Any thoughts you want to share on that, Hugo.

225
00:45:23.210 --> 00:45:24.140
hugo bowne-anderson: Yeah, I think

226
00:45:25.440 --> 00:45:32.081
hugo bowne-anderson: that's a good question. I think that's something that we'll we'll get to, and we may not get to it. In this

227
00:45:33.070 --> 00:45:34.476
hugo bowne-anderson: this workshop?

228
00:45:36.730 --> 00:45:45.100
hugo bowne-anderson: And once again there are trade-offs between precision and and accuracy. But essentially

229
00:45:45.550 --> 00:45:50.757
hugo bowne-anderson: what and sorry precision and recall as well, which maybe are even more important

230
00:45:52.160 --> 00:46:18.720
hugo bowne-anderson: than accuracy. But you can literally look at the retrieve sources versus what you'd expect to see, and also use binary labeling as as well. In order to test test and evaluate these these things. I'm wondering, Nathan or William, because I know, William, you've also taken Jason's rag course. If there's anything you'd add to that with respect to accuracy in particular.

231
00:46:23.060 --> 00:46:26.570
William Horton: No, I think you I think you pretty much covered what I would say about it. Yeah.

232
00:46:28.290 --> 00:46:29.529
hugo bowne-anderson: And also.

233
00:46:29.890 --> 00:46:43.359
hugo bowne-anderson: this is a somewhat solved problem. So I will include resources for how to, how, how to think through this. And once again. Thinking through whether precision is more important than than recall. Depending on your use case is is super important.

234
00:46:45.880 --> 00:46:54.710
hugo bowne-anderson: So can everyone see github. Now, codespace awesome.

235
00:46:56.610 --> 00:46:58.759
hugo bowne-anderson: I do actually wonder.

236
00:47:01.720 --> 00:47:09.740
hugo bowne-anderson: I do wonder whether I do want to execute the code here. I mean it. It actually doesn't add a huge amount of value to execute this in real time. But I

237
00:47:10.350 --> 00:47:11.520
hugo bowne-anderson: I will do it.

238
00:47:14.020 --> 00:47:20.359
hugo bowne-anderson: You see, on getting my environments, you're gonna hit. Run? All.

239
00:47:23.990 --> 00:47:28.990
hugo bowne-anderson: Okay. So what I really want to do here is to just help also

240
00:47:29.410 --> 00:47:31.949
hugo bowne-anderson: understand more the role of embeddings in semantic search.

241
00:47:32.260 --> 00:47:37.849
hugo bowne-anderson: learn how to build and query vector stores to enhance performance, develop foundations

242
00:47:38.050 --> 00:47:46.234
hugo bowne-anderson: for rag and visualize embeddings. And I said, That's optionally. But it's definitely not optional for me with how I'm feeling today.

243
00:47:46.890 --> 00:47:53.832
hugo bowne-anderson: I do think like the visualization and how you get signal on performance. There is incredibly useful as as well.

244
00:47:54.590 --> 00:47:57.069
hugo bowne-anderson: So we're going to generate some embeddings.

245
00:47:57.270 --> 00:48:05.849
hugo bowne-anderson: We're going to build a vector store, retrieve chunks and have optional visualization. So I see Priya has her hand up. If you wouldn't mind

246
00:48:06.960 --> 00:48:14.129
hugo bowne-anderson: asking in discord unless it's gonna unless it's absolutely essential, like, you don't understand something, but

247
00:48:14.980 --> 00:48:17.310
hugo bowne-anderson: do feel free to talk. If if you want.

248
00:48:18.061 --> 00:48:24.739
Priya Krishnamoorthy: I did ask in discord. I was just wondering if you might be able to address it before.

249
00:48:25.350 --> 00:48:26.010
hugo bowne-anderson: Absolutely.

250
00:48:26.010 --> 00:48:28.859
Priya Krishnamoorthy: No point. I did ask it in discord. Thank you.

251
00:48:29.930 --> 00:48:31.710
hugo bowne-anderson: What's the question? I may as well.

252
00:48:34.350 --> 00:48:38.954
Priya Krishnamoorthy: Oh, were you trying to say rags are

253
00:48:39.860 --> 00:48:48.900
Priya Krishnamoorthy: a sort of a hype because you did compare it? You did say you could do it with numpy, which I agree with.

254
00:48:49.190 --> 00:48:50.050
hugo bowne-anderson: Are you saying?

255
00:48:50.050 --> 00:49:08.260
hugo bowne-anderson: No, no, I didn't say that. And to be clear, if you do it with numpy, that's still rag. You're just not using a vector database. You're still doing it and storing it somewhere. Yeah, so definitely, not hype. And vector databases. I don't think are hype. They're incredibly useful. I just wouldn't use them when prototyping.

256
00:49:10.120 --> 00:49:10.810
Priya Krishnamoorthy: Okay.

257
00:49:10.810 --> 00:49:14.774
hugo bowne-anderson: Yeah, but I I appreciate that question, and it's an important clarification. And

258
00:49:15.380 --> 00:49:22.919
hugo bowne-anderson: I do appreciate that. I say a lot of words as well. So helping to gain signal. There, William, did you have something you wanted to add.

259
00:49:24.130 --> 00:49:28.580
William Horton: I was. Gonna say, I think it's like a common idea. I see on Twitter, too, where

260
00:49:28.750 --> 00:49:52.430
William Horton: I think people think that rag means like vector Dbs, and so it is an important point. You're making that like, rag doesn't require you to use vector Dbs, it doesn't even require to use vector similarity like you could use other forms of retrieval. So like a lot of times when people say, rag is dead, they mean, like, you don't need a vector, dB, in which case I would agree like, you don't always need a vector, dB, but that doesn't mean like, rag is dead.

261
00:49:53.450 --> 00:49:58.660
hugo bowne-anderson: Totally awesome. So

262
00:49:58.770 --> 00:50:10.980
hugo bowne-anderson: I do want to just talk a bit about the precursors to embeddings, and I think they're historically useful to know, but also kind of practically useful to understand how we got where we are today. So oh.

263
00:50:11.410 --> 00:50:15.689
hugo bowne-anderson: of course, why would. Okay, I'm actually in the interest of time.

264
00:50:17.760 --> 00:50:21.369
hugo bowne-anderson: I could try to debug whatever happened there. But I don't think it's

265
00:50:21.520 --> 00:50:27.009
hugo bowne-anderson: useful. I probably just didn't start my environment correctly. I'm just gonna go to the notebooks.

266
00:50:27.610 --> 00:50:36.899
hugo bowne-anderson: Oh, I bet you no, I'm gonna have to debug. I let me get this

267
00:50:42.940 --> 00:50:47.480
hugo bowne-anderson: right. I am oof

268
00:50:49.850 --> 00:50:52.020
hugo bowne-anderson: I'm pretty sure my environment had

269
00:50:53.910 --> 00:50:55.610
hugo bowne-anderson: so I could learn, let me just

270
00:51:00.140 --> 00:51:05.469
hugo bowne-anderson: okay. Well, this is gonna be a bit silly. But what I'm going to do is

271
00:51:13.320 --> 00:51:15.449
hugo bowne-anderson: let me just make sure I get this right? Yeah.

272
00:51:25.490 --> 00:51:29.040
hugo bowne-anderson: yeah, that's the right environment. Then what I'm going to do is

273
00:51:33.440 --> 00:51:41.349
hugo bowne-anderson: see you into the workshop. And I think I've got yeah UV pip

274
00:51:42.210 --> 00:51:44.829
hugo bowne-anderson: install. I'm going to install the requirements.

275
00:51:47.910 --> 00:51:57.479
hugo bowne-anderson: So that should add this to this end. As I think you notice, code spaces had some downtime recently and environments weren't working my video. Greg reported that they worked for him, which is

276
00:51:57.760 --> 00:52:06.256
hugo bowne-anderson: super cool. And I actually think I there's like more code assistant stuff within code spaces now. So I wonder if that's what they they were working on.

277
00:52:09.162 --> 00:52:11.050
Pastor Soto: Think you have it.

278
00:52:11.620 --> 00:52:13.659
Pastor Soto: I think you have a different environment on this.

279
00:52:14.290 --> 00:52:16.300
hugo bowne-anderson: I can't understand what you're saying.

280
00:52:17.360 --> 00:52:20.179
Pastor Soto: I do have a notebook.

281
00:52:22.340 --> 00:52:24.369
hugo bowne-anderson: Can anyone else understand? Pastor, the the audio.

282
00:52:26.260 --> 00:52:28.410
William Horton: He also posted in

283
00:52:28.560 --> 00:52:35.630
William Horton: discord. He says, maybe the your notebook has a different environment, like the the kernel you opened for the notebook

284
00:52:35.920 --> 00:52:41.069
William Horton: might not have been the same environment as what you had set up.

285
00:52:41.390 --> 00:52:43.515
hugo bowne-anderson: I'm I'm pretty sure it was

286
00:52:45.230 --> 00:52:48.266
hugo bowne-anderson: But while that's while that's running, let's

287
00:52:52.330 --> 00:52:54.039
hugo bowne-anderson: Let me think about how I

288
00:52:54.400 --> 00:52:58.070
hugo bowne-anderson: want to do this I may be able to import everything in the notebook. But this is

289
00:52:58.590 --> 00:53:02.160
hugo bowne-anderson: this is, gonna take a bit a bit of time. So

290
00:53:02.350 --> 00:53:04.850
hugo bowne-anderson: let me think. Let me just think about how I want to.

291
00:53:05.710 --> 00:53:07.160
hugo bowne-anderson: Okay. What we'll see is

292
00:53:07.400 --> 00:53:23.430
hugo bowne-anderson: we begin with a toy data set to understand the basics right? So we have some documents that they're little sentences. Dogs bark at the car. Cat slept on the mat, etc, and what we can do is just have a look at them in a pandas data frame. So that's what we do here.

293
00:53:23.530 --> 00:53:29.999
hugo bowne-anderson: And the 1st way to create some form of embeddings or convert our text to

294
00:53:30.615 --> 00:53:45.450
hugo bowne-anderson: vectors is bag of words, right? So it converts each document into a vector where each feature corresponds to a unique word from the corpus. The value of each feature is the count of occurrences of the word in the document. Okay, so let me just go.

295
00:53:46.190 --> 00:53:51.408
hugo bowne-anderson: Okay, we're getting there. Some wheels, failures or warnings.

296
00:53:53.010 --> 00:54:17.912
hugo bowne-anderson: so what we get there, for example, is this document? Dog barks at the car for the word dog. We'd have one for the word barks, we'd have one. And similarly, for the rest of them, for any other word in the Corpus, we'd have 0 right or any other word in our features. So, cat, it would be 0 sleeps would be 0. Matt would be 0. So we get this big matrix where the rows are sentences essentially with

297
00:54:19.475 --> 00:54:39.869
hugo bowne-anderson: non-negative integers. So 0, if the word doesn't occur there, and then the number of occurrences, if it if it does okay, and when we get this notebook up and running we'll see how that works. And this is a lesson to myself to actually push the a version of the notebook which has the rendered results in them. So we could could go through it.

298
00:54:40.050 --> 00:54:41.140
hugo bowne-anderson: Now,

299
00:54:43.060 --> 00:54:53.079
hugo bowne-anderson: we're using count vectorizer, and that essentially tokenizes the text and creates a matrix where each row represents a document, and each column represents a word. Okay? Now.

300
00:54:53.630 --> 00:55:02.110
hugo bowne-anderson: bag of words is surprisingly powerful for a lot of different things, even though it doesn't even take into account word order. So bag of words will

301
00:55:02.530 --> 00:55:04.800
hugo bowne-anderson: embed, build bridges, not walls

302
00:55:05.490 --> 00:55:15.700
hugo bowne-anderson: the same give it the same embedding it would give to build walls, not bridges right? And they have 2 very different different meanings as exemplified by global Western politics.

303
00:55:15.700 --> 00:55:16.450
William Horton: There you go!

304
00:55:16.837 --> 00:55:20.520
hugo bowne-anderson: Could you zoom in a little bit? We got a request absolutely.

305
00:55:20.520 --> 00:55:22.229
William Horton: Like a little bit larger. Thank you.

306
00:55:22.450 --> 00:55:27.770
hugo bowne-anderson: Yeah. Now, tfidf.

307
00:55:27.940 --> 00:55:38.349
hugo bowne-anderson: what it does is it's kind of like bag of words. It's a bit of an improvement and adjust the frequency of words based on how common they are across all documents. Okay? So the word the

308
00:55:38.630 --> 00:55:42.390
hugo bowne-anderson: it will wipe that quite.

309
00:55:42.770 --> 00:56:04.559
hugo bowne-anderson: quite low, because it occurs across most documents. So it isn't that important that it's occurred in one in particular. Right? Why is it called Tfidf? It's term frequency, inverse document frequency. So remember, bag of words tells you the frequency of the term. Then what we do is we take the ratio over how often it occurs in the entire document or corpus. Right?

310
00:56:04.730 --> 00:56:11.040
hugo bowne-anderson: So let's now go back here. And oh, it looks like if the demo gods are shining today.

311
00:56:13.950 --> 00:56:22.860
hugo bowne-anderson: This may now work. So let me just I may need to restart this completely. But

312
00:56:23.090 --> 00:56:29.790
hugo bowne-anderson: oh, no, this is nice. Oh, yeah, White, no.

313
00:56:29.930 --> 00:56:36.329
hugo bowne-anderson: Hugo, I'm gonna restart the kernel. I'm sorry and clear all outputs.

314
00:56:37.670 --> 00:56:41.999
Geoff Pidcock: Whilst you do that, Hugo, would you mind making the point a little bit larger to zoom in a bit.

315
00:56:42.320 --> 00:56:44.840
hugo bowne-anderson: Yeah, I will. If this notebook works.

316
00:56:46.430 --> 00:56:54.350
hugo bowne-anderson: Okay, we're good. Okay. So now we have this working, we can see that

317
00:56:56.330 --> 00:57:05.790
hugo bowne-anderson: this 1st set. All I'm doing is turning these sentences into a data frame essentially right where? That's the 1st sentence, that's the second, that's the 3.rd And so then, when we do bag of words.

318
00:57:06.050 --> 00:57:22.651
hugo bowne-anderson: What we see is that we have all the words as columns, and then still the sentences as as rows right. So the 0 sentence was, dog barks a car. So we'll look at the ones here. Okay, at barks. Car dog. Okay, that's great.

319
00:57:23.710 --> 00:57:30.170
hugo bowne-anderson: And so that's essentially how this encoding works. Now, when we do the Tfidf.

320
00:57:31.900 --> 00:57:39.460
hugo bowne-anderson: what we'll see, and just quickly, I'm sorry for scrolling around. It was dog barks at the car, so we get the there we get the there

321
00:57:40.052 --> 00:57:42.170
hugo bowne-anderson: and so what we see now

322
00:57:42.740 --> 00:57:46.440
hugo bowne-anderson: is all the 0 entries are the same, but

323
00:57:47.310 --> 00:57:51.419
hugo bowne-anderson: the words that give more signals, such as dog

324
00:57:52.140 --> 00:58:07.989
hugo bowne-anderson: a bit more highly weighted, and the ones that give less signal are a bit less. Okay? So finally, because dog actually peers quite a bit. It's not weighted as much. But the, for example, occurred us across several. So it's 0 point 3 3. Okay.

325
00:58:08.310 --> 00:58:21.840
hugo bowne-anderson: so just quickly, what's the key difference? Bag of words counts frequency in the document doesn't consider word importance across the corpus or all documents. Tfidf will weigh words by importance, and it helps reduce. The influence

326
00:58:21.960 --> 00:58:42.370
hugo bowne-anderson: of common words, increases the weight of unique words in the data set. And for anyone who's done like classic natural language processing. It probably works with stop words and that type of stuff where we literally, you know, we have a list of stop words which is super common, like the A etc, that we remove from the corpus before doing any serious natural language processing on it.

327
00:58:42.880 --> 00:58:48.510
hugo bowne-anderson: Couple next steps for those interested. Okay. So now.

328
00:58:48.760 --> 00:59:15.530
hugo bowne-anderson: I want to introduce us to word to Vec, and this is a method for learning continuous vector, representations of words which start to capture semantic relationships and context. Okay, so, word to Vec, in contrast to the previous ones, quote unquote, understands the context of words. And so it can capture subtle semantic relationships between words. So, for example, the Queen and king, example and man and woman like

329
00:59:16.220 --> 00:59:40.502
hugo bowne-anderson: it's quote unquote, understood or captured. The idea that these words often appear in like, probably situations where there are crowns and thrones and courts and jesters, and that type of stuff. So that that's what it means by contextual right. And I actually really don't like this word, understand? But I don't care enough right now to correct myself. But I will put in quotation mark. So

330
00:59:41.140 --> 00:59:56.569
hugo bowne-anderson: and the difference here we map words to a continuous space and capturing context, and semantics and king and queen are closer because they share a similar context, while king and dog are farther apart, and we can model subtle word relationships from our corpus.

331
00:59:57.180 --> 01:00:21.360
hugo bowne-anderson: such as that. But once again, it's garbage in garbage out. And we are modeling stuff based on historical texts. And I think this is actually a really nice example. I kind of joked about it before. But you know, we need to recognize that the texts we're using capture this relationship which may not be an important relationship to us. Or it may encode a bias that we don't necessarily want in our systems. Right? So let's now.

332
01:00:21.670 --> 01:00:35.094
hugo bowne-anderson: what I'm going to do is show you in a kind of a silly, toy way how to train a word. 2 Vec model. Then we're going to import a previous one. And actually, I'm going to click, run all

333
01:00:35.780 --> 01:00:45.370
hugo bowne-anderson: you know, that's real. That's the Og Yolo Yolo data. Science vibes the run all and will.

334
01:00:45.750 --> 01:00:49.010
hugo bowne-anderson: Just a sec, of course. Why.

335
01:00:52.800 --> 01:00:56.309
hugo bowne-anderson: good Lord, okay, let me just

336
01:01:01.540 --> 01:01:05.100
hugo bowne-anderson: can I ask, Greg, were you able to execute the the code in this.

337
01:01:06.430 --> 01:01:07.700
Greg Gandenberger: Yeah.

338
01:01:08.520 --> 01:01:14.580
Greg Gandenberger: I hit an error at the very bottom of B. But this notebook, once I got the and stuff installed worked just ran fine.

339
01:01:14.750 --> 01:01:17.252
hugo bowne-anderson: Okay, so look everyone. I sincerely

340
01:01:17.910 --> 01:01:24.680
Naoya Kanai: I posted in the chat. But in the discord. But I think it's you just have to downgrade numpy to make that run.

341
01:01:26.060 --> 01:01:32.059
hugo bowne-anderson: And this is fascinating because I I did that yesterday. I just wanna.

342
01:01:39.564 --> 01:01:43.260
Greg Gandenberger: Would it be helpful for me to push a branch with the executed notebooks.

343
01:01:43.450 --> 01:01:49.229
hugo bowne-anderson: That if you could do that right now, I'd love that, and then I'll debug afterwards, and I appreciate everyone's patience here.

344
01:01:50.470 --> 01:01:54.109
hugo bowne-anderson: But hey, this, this happens right?

345
01:02:00.390 --> 01:02:01.310
hugo bowne-anderson: So

346
01:02:02.240 --> 01:02:11.099
hugo bowne-anderson: while that's being pushed, I'm just going to continue through the material here. And I'm going to go back to

347
01:02:16.686 --> 01:02:20.540
hugo bowne-anderson: and yeah, I I did. Actually, you may even notice in this notebook.

348
01:02:22.960 --> 01:02:31.480
hugo bowne-anderson: Let's see that, due to some wacky Jensen calling different versions of sci-fi

349
01:02:31.830 --> 01:02:39.650
hugo bowne-anderson: sub modules. I actually had to monkey patch something from numpy. So Jensen would work. So

350
01:02:39.930 --> 01:02:58.540
hugo bowne-anderson: irrespective of these challenges. What we're doing here is we have this funny little corpus, and we train a word, 2 Vec. Model based on it. And then we look at the embedding of King based on this model. Okay, now, this isn't the type of model you'd want. You don't want to train one of this, but it's a toy example for how you do it yourself

351
01:03:00.190 --> 01:03:01.530
hugo bowne-anderson: most of the time.

352
01:03:02.748 --> 01:03:04.980
hugo bowne-anderson: What you'll do is

353
01:03:05.440 --> 01:03:12.922
hugo bowne-anderson: use a pre-trained model essentially, and that's what we're gonna do going to do next. But you can then, you know, find the most similar words,

354
01:03:14.410 --> 01:03:18.575
hugo bowne-anderson: and and so on. Priya has asked the question about

355
01:03:19.760 --> 01:03:28.000
hugo bowne-anderson: my virtual environment. And yes, Priya, that's a great question. I did have a virtual environment, and it still didn't work so welcome to.

356
01:03:28.810 --> 01:03:37.739
hugo bowne-anderson: I mean, I would. I wouldn't even call this dependency. Hell! But it is something that exists. Which branch am I looking at? Greg? Exact 5.

357
01:03:38.730 --> 01:03:39.560
Greg Gandenberger: Yeah. Great.

358
01:03:39.840 --> 01:03:43.169
hugo bowne-anderson: Thanks. Bro, I really appreciate it

359
01:03:48.130 --> 01:03:49.760
hugo bowne-anderson: cool. So

360
01:03:55.980 --> 01:03:58.919
hugo bowne-anderson: so what we see here is, we've got this corpus.

361
01:03:59.130 --> 01:04:14.670
hugo bowne-anderson: We've embedded King, and we've got an embedding for King. And it's a vector. That's all. That's all we want to see there. Right? And then we can measure similarity between words. And what we see is that and you know the Gen. Team Api is model dot. Wv. Dot. Similarity.

362
01:04:14.770 --> 01:04:35.439
hugo bowne-anderson: Similarity between king and queen is really small. Similarity between king and dog is a bit bigger, and that may seem a bit silly, and it actually is silly. It shouldn't be the case. It's because our corpus is just these sentences essentially right? So we're not getting. This is garbage in garbage out right? This model sucks so we wouldn't use it. But I did want to give you a sense of how you can train your own model

363
01:04:36.200 --> 01:04:53.929
hugo bowne-anderson: if you want. And finally, what we see is that we can also just get the most similar words we've chosen. Top end is equal to 3. And this is what we do when we do rag essentially, or information retrieval. Right. We will say we want the top 3 closest and feed them to an Llm. And say, generate the response.

364
01:04:54.270 --> 01:05:01.700
hugo bowne-anderson: So we'll see for King. We get Prince, we get dog, and we get Princess. Okay? And Greg, I don't. Did you actually also push

365
01:05:02.220 --> 01:05:08.220
hugo bowne-anderson: notebook? B, thank you. And C doesn't have an executed code in it. So

366
01:05:10.860 --> 01:05:17.120
hugo bowne-anderson: All that having been said, you'll notice in the next cell that I'm actually

367
01:05:17.350 --> 01:05:23.380
hugo bowne-anderson: using a different model, not the one I've just trained, and we're using a glove model, which I'll

368
01:05:24.080 --> 01:05:25.570
hugo bowne-anderson: show you in a second.

369
01:05:26.790 --> 01:05:35.949
hugo bowne-anderson: But what I want to show you is that where we want the most similar to king plus woman

370
01:05:36.140 --> 01:05:47.749
hugo bowne-anderson: minus man. So the positive are the ones you sum the negatives the one, and we want the top the top one, and what we get is result of king minus man plus woman is queen with a similarity of point 8 5.

371
01:05:48.010 --> 01:06:11.207
hugo bowne-anderson: Now I wanted to share that with you to show that this isn't just something that happens in the abstract. You know you can get models from hugging face and see this emerge yourself. And you know, time permitting. I, this is real fun, right? To see what type of things are captured. What type type of things aren't in the end in your day job. Perhaps you won't have time to really dig into all the all the embeddings, but it can be it can be so much fun. And

372
01:06:15.040 --> 01:06:16.529
hugo bowne-anderson: so what we see

373
01:06:18.100 --> 01:06:23.274
hugo bowne-anderson: there wasn't a lot on hugging face there. But yeah, you can get a strong sense for all the different types of

374
01:06:25.840 --> 01:06:41.399
hugo bowne-anderson: God models as well. And yeah, to the point Sidharta made like we got dog near King and and that type of stuff. And that's clearly nonsense. So it's once again a sign that if you're embedding models not great, you're going to get out. Crap. Essentially. Pardon, pardon my French.

375
01:06:41.780 --> 01:06:42.670
hugo bowne-anderson: So

376
01:06:44.170 --> 01:06:49.878
hugo bowne-anderson: After that I wanted to jump in and show you the

377
01:06:51.100 --> 01:06:56.519
hugo bowne-anderson: the aging use data set, which is the one that I talked about in slides. Briefly, where.

378
01:06:57.410 --> 01:07:02.123
hugo bowne-anderson: And I only downloaded 1% of it here because it's it's quite large.

379
01:07:02.660 --> 01:07:32.229
hugo bowne-anderson: and we also have labels which I'm not interested in right now. But what we can do is we can create a bag of words representation of it as we did before using scikit-learn feature, extraction. Right then, we can immediately create our Tdf idf, matrix, okay? And then what we can do is so the labels with some sort of classification. Right? Once we have a Tf, idf matrix. What we can do is do some sort of

380
01:07:32.610 --> 01:07:34.660
hugo bowne-anderson: logistic regression

381
01:07:34.760 --> 01:07:49.279
hugo bowne-anderson: which is kind of akin to naive Bayes, actually like bag of words would be naive Bayes. And I think this is a slightly different variation of it. But what we're doing is we're doing a train test split on the data set and trying to predict

382
01:07:49.480 --> 01:08:18.470
hugo bowne-anderson: the labels. And in all honesty, what you see here is for these different classes. We have precisions of 0 point 8 5.7 7.7 3.7 2. These recalls all. So once again, if you ask me, is 85% precision good, I would say, I have no idea right? Depending on the particular question or all of these, what? What you're trying to solve. I can say that

383
01:08:18.830 --> 01:08:24.190
hugo bowne-anderson: for doing something as basic as Tfidf, and then using a logistic regression on it.

384
01:08:24.540 --> 01:08:43.729
hugo bowne-anderson: This is a looks like a pretty good baseline to me to be honest. So the only point there is that you know, these earlier techniques can be pretty powerful, but they don't capture semantics and all of these things. But this is something you can run locally if you can. I clearly can't. Greg can but

385
01:08:44.330 --> 01:08:56.280
hugo bowne-anderson: actually, funnily, I could have done it locally, just not on codespaces. But you can get pretty powerful results out out of the box. Without necessarily going to an openai. Right? So

386
01:08:58.279 --> 01:09:22.270
hugo bowne-anderson: yep, I want to move past that because I want to jump into using word to vet for information retrieval. And once again, you probably wouldn't use these Gen. SIM models as is, but I think it's it's good to see them at play. So what I did was, I had a basic example query, I wanted breaking news and I wanted the the one document that would be most similar to that

387
01:09:22.760 --> 01:09:46.980
hugo bowne-anderson: and it gave the result. Canadian robot a candidate to save Hubble ap NASA said Tuesday. It is moving ahead with plans to send a robot to the rescue of the aging Hubble Space Telescope. Okay, so it doesn't say breaking news there, but it captured through word to Vec something which seemed to be breaking news. So I encourage you to play around with all of that. We do have

388
01:09:47.729 --> 01:09:52.509
hugo bowne-anderson: 5min left, and I I don't want.

389
01:09:54.540 --> 01:10:00.090
hugo bowne-anderson: Let me get this right because we do have guest speakers in another zoom call after Williams talk so

390
01:10:00.570 --> 01:10:03.011
hugo bowne-anderson: what I'll do I will.

391
01:10:05.280 --> 01:10:13.890
hugo bowne-anderson: go through this briefly, and then, if if it seems like it would help send a video through of me going for it, but it is pretty much all

392
01:10:14.380 --> 01:10:22.567
hugo bowne-anderson: in here. We will focus on transformer embeddings.

393
01:10:24.380 --> 01:10:44.109
hugo bowne-anderson: and I mean, these are the ones we all use today. And when you do lama index or use open AI is whatever like, it's all transformers. It transforms all the way down. But the key takeaways are embeddings of the backbone of semantic retrieval in rag systems. They map text into high dimensional latent space or vector space latent is just a term for this space.

394
01:10:44.110 --> 01:11:10.570
hugo bowne-anderson: It's latent because it's really a middle step to the final final output, and it makes it easier to compare and retrieve semantically relevant information. And rag systems rely on similarity searches. Okay, william has just posted a wonderful talk he gave last time on on thinking through like, whether is 85% precision, good and talk about how perfect metrics don't don't exist. And you really want to think about your use cases. So

395
01:11:10.730 --> 01:11:14.399
hugo bowne-anderson: we can pip install sentence transformers from hugging face.

396
01:11:15.380 --> 01:11:43.010
hugo bowne-anderson: And then we import a sentence transformer. And this is this is the this is the real deal before using openai embeddings. Maybe after like, try stuff like this, my people. Mini, lm, l. 6, v. 2. It's an incredible model. It'll be super cheap for you compared to other embedding models. Look at the support we have like 83, as Nathan said, you can run it on a CPU server, super performant

397
01:11:43.050 --> 01:12:00.780
hugo bowne-anderson: 83 million downloads last month. So that that's proof of principle, and also means there's a huge community behind it. And so what I did here was I just took 2 silly sentences. This is an example sentence. Each sentence is converted into an embedding.

398
01:12:02.010 --> 01:12:02.980
hugo bowne-anderson: and

399
01:12:03.130 --> 01:12:13.230
hugo bowne-anderson: we have the sentence, then the embedding and then we can see the similarity between these 2. So we create a similarity matrix and what you'll see is that

400
01:12:13.840 --> 01:12:36.030
hugo bowne-anderson: along the diagonal you have the how similar documents are to themselves. Essentially right? So you'd expect that to be close to one. There'll be some sampling error. But then, you see, like, and it's symmetric, because this is the similarity of 1st sentence with second, and that second sentence with first, st and you see, point 4 2 is pretty pretty similar, right? And that's because they're both about sentences. Okay.

401
01:12:36.290 --> 01:12:46.240
hugo bowne-anderson: then, what we do is we install data sets. Then we load the Ag news data set. Remember, to load 1% of it. Unless you want to wait a bit longer.

402
01:12:47.350 --> 01:12:56.582
hugo bowne-anderson: We can start looking at some of them and their embeddings, and you can start to explore what? Inspect your data, see what it see, what it looks like?

403
01:12:57.550 --> 01:13:01.239
hugo bowne-anderson: And after that we start doing some similarity search.

404
01:13:01.862 --> 01:13:25.950
hugo bowne-anderson: Then we can build our similarity matrix, and we can think about the sentences most similar to the 1st one, and the 1st one was Wall Street bears, clawback, etc. Then we can see the next one was oil. Yeah. So stocks end up. So it's finding stock type vibes right? And then the least similar sentence.

405
01:13:26.380 --> 01:13:31.430
hugo bowne-anderson: The least similar sentence to Wall Street is about sociology. Okay?

406
01:13:32.250 --> 01:13:53.569
hugo bowne-anderson: which may be telling. But of course, as we all know, some of the most interesting sociology is the sociology of Wall Street people. Some of my favorite videos on Youtube are like from the nineties. These dudes on Wall Street traders with like 4 phones being like, buy, buy, buy, sell, sell. There's actually a wonderful video of Robert Downey, Jr. Going in there and and saying how insane they all look

407
01:13:54.940 --> 01:13:58.593
hugo bowne-anderson: which is something coming from Rdj as well. Right?

408
01:13:59.170 --> 01:14:21.009
hugo bowne-anderson: So after this I go through, and we start looking at the clusters, and we've chatted about this a bit, but this is all the code for it. These are the clusters, and I'm going to speed through through this now, and I think everyone can go through the notebook because this is really just the code, and we see the clusters emerge, so do feel free to start introspect to introspect into that.

409
01:14:21.890 --> 01:14:28.349
hugo bowne-anderson: Now I I just want to have a look to see how much more I like most of this is output.

410
01:14:32.260 --> 01:14:38.939
hugo bowne-anderson: William, do you mind if I just go for a few more minutes, and maybe you have 25min in total. Is that is that cool.

411
01:14:39.697 --> 01:14:46.100
William Horton: Yeah, I don't mind. I I timed my talk should be about 20min, and I was leaving time for QA. So we should be good.

412
01:14:46.100 --> 01:14:53.670
hugo bowne-anderson: Amazing. I really appreciate it, man, and of course we can have more Q&A in in the discord about it afterwards. But

413
01:14:54.270 --> 01:14:55.790
hugo bowne-anderson: Whoa, whoa! Whoa! So

414
01:14:55.960 --> 01:15:12.179
hugo bowne-anderson: now, having done all this, we can think about how to do these embeddings for information retrieval. Okay, so now I'm I'm taking 10%. I want to take a bit more because 1% for information retrieval seems like I may not get the results I want.

415
01:15:13.720 --> 01:15:21.650
hugo bowne-anderson: Then I take a query, how did the recent financial policies affect the stock market? I have.

416
01:15:22.050 --> 01:15:39.620
hugo bowne-anderson: I've embedded all of the all of the texts, and then I start looking for the most similar ones. Okay, the most similar ones are stocks up on conflicting economic reports. New York stocks little changed on Ibm stocks. Open flat. Okay? So it seems like it's somewhat relevant.

417
01:15:39.770 --> 01:15:49.224
hugo bowne-anderson: Once again, we'd want to think about tradeoffs between precision and and recall. And if these are the results that we know we want in our test set, for example.

418
01:15:49.940 --> 01:15:52.503
hugo bowne-anderson: Oh, tell me about it, William. I

419
01:15:54.050 --> 01:15:56.482
hugo bowne-anderson: yeah, I'm sorry if I've triggered anyone.

420
01:15:57.960 --> 01:15:59.270
hugo bowne-anderson: But remember.

421
01:15:59.420 --> 01:16:06.089
hugo bowne-anderson: it's not time. It's time in the market. Not timing the market is what my guy my guy tells me, anyway.

422
01:16:06.280 --> 01:16:34.009
hugo bowne-anderson: But then what I've done here is, I just wanted to demonstrate how we don't need Openai or any of that. That stuff you can get like really state of the art results, of course, but at what cost also. But what you can do is you can import Gpt 2 from hugging face. I'm giving it the input text. So I'm saying the question and the query, and generating a response. And so

423
01:16:34.380 --> 01:17:02.147
hugo bowne-anderson: it then prints. The question is, how did that? Was my question, and then it says the answer is, the recent economic downturn has been a major factor in the market's recent decline. The stock markets have been in a downward spiral, and then it starts drawing on on the sources. Okay? So once again, this is a toy example of how this retreat, the innards of this retrieval process works. You get the sources through these embeddings. And then you get a generative response. Okay,

424
01:17:06.630 --> 01:17:07.740
hugo bowne-anderson: next.

425
01:17:07.980 --> 01:17:21.090
hugo bowne-anderson: I did want to quickly talk about how to use vector databases. Right? So we've done this all in an array of sorts above you. Once you get to a certain size, you will want to use vector databases.

426
01:17:21.820 --> 01:17:31.709
hugo bowne-anderson: It's a it's once again. It's jargon. It's like an array, right? It's a way to store vectors. and those are embeddings right? And it

427
01:17:32.070 --> 01:17:47.349
hugo bowne-anderson: it's real advantages. It'll index the vectors to make search faster. Right? So once again, we've got all these sentences. I'm doing 1% now, in order to not overwhelm myself or the system for illustrative purposes. I printed a bunch of stuff

428
01:17:47.750 --> 01:17:51.110
hugo bowne-anderson: there, and what we

429
01:17:51.590 --> 01:17:58.610
hugo bowne-anderson: then I install chroma. dB, and there's far more output here than even what we're doing. Essentially. So

430
01:17:59.140 --> 01:18:00.780
hugo bowne-anderson: to get the query.

431
01:18:01.130 --> 01:18:24.710
hugo bowne-anderson: what we do below is, we encode a text prompt into an embedding and search for the 3 most similar entries in the chroma database right? And one reason to use chroma again is this, not only, for you know, if you've got too much out of memory. And and that type of stuff. It's persistent here, right? Like it saves something locally, whereas a numpy array, I mean, I can save that to to my hard drive. But it doesn't, doesn't persist natively. So

432
01:18:25.700 --> 01:18:29.519
hugo bowne-anderson: what we do in chroma is we create a collection

433
01:18:30.017 --> 01:18:45.902
hugo bowne-anderson: we load the transformer and then we generate the embeddings, and then we store them in chroma. Okay? And it tells us we've stored that many sentences in chroma. Then I ask, what are the latest economic trends? And we

434
01:18:48.710 --> 01:19:12.149
hugo bowne-anderson: we retrieve the top 3, using using this collection dot query and then display the results. And I haven't used anything generative to give a response yet, but we see the top 3 results here, and then you can use Gpt. 2, or Openai or Gemini with all your great new credits, and so on to generate responses based on this. But this is really to demystify the fact that

435
01:19:12.320 --> 01:19:21.489
hugo bowne-anderson: the generative process happens at the end. We're doing a bunch of embeddings in order to figure out what's the most relevant stuff in in our documents?

436
01:19:23.790 --> 01:19:25.530
hugo bowne-anderson: And there are lots of different

437
01:19:25.810 --> 01:19:31.950
hugo bowne-anderson: strategies. You can use in order to improve improve the information. Retrieval part, but this is very much

438
01:19:32.060 --> 01:19:34.009
hugo bowne-anderson: the basics of it, and I do.

439
01:19:34.160 --> 01:19:38.100
hugo bowne-anderson: I do encourage you, checking out William's talk that he linked to, and I'll put it

440
01:19:39.320 --> 01:19:49.230
hugo bowne-anderson: in the course. Wiki, I'm going to put 2 other talks in the course. Wiki Brian Bischoff gave a really interesting talk around information retrieval generally last

441
01:19:49.590 --> 01:20:13.309
hugo bowne-anderson: last course, and in fact, his talk was one of one of the favorites. And Eric, ma who is at Moderna and talked about agents. Last week we had a Q. And a about about how they use rag and one thing. And this is a real product question. And why I think about, I really encourage you all to think about personas and and what type of questions people will ask. Eric came in, and this is something I've seen before. But Eric came in and said, Yo, when we do.

442
01:20:13.610 --> 01:20:32.630
hugo bowne-anderson: you know all this rag stuff. We forget that one of the 1st questions people ask are, Hey, what is this corpus all about? Right? And embeddings are a lot of the time a horrible solution to solve that that problem. Right? So you know, these types of things are very interesting thinking about what what your product, what you want it to be. So

443
01:20:32.810 --> 01:20:37.349
hugo bowne-anderson: finally, I do want to say and I'll put this in the course. Wiki.

444
01:20:37.550 --> 01:20:43.872
hugo bowne-anderson: Oh, we've got lots of great ideas for homework, or what you can do as as a project. I do appreciate that.

445
01:20:44.750 --> 01:21:04.040
hugo bowne-anderson: that it's more expansive than than dictatorial. There are enough dictators in the world without me telling you what homework you have to do. But what I would like you to think about is enhance your Linkedin profile out by adding a rag workflow to handle indexing and and querying.

446
01:21:04.990 --> 01:21:11.640
hugo bowne-anderson: you could index the provided data set. And this is when I share the slides, and I'll put this in

447
01:21:13.480 --> 01:21:16.883
hugo bowne-anderson: in discord as well. We've got

448
01:21:18.320 --> 01:21:28.704
hugo bowne-anderson: a folder of lots of different Linkedin profiles that you can use for this as well, and thank you to Stefan for that. It's when I thought you had to do all this manually.

449
01:21:29.650 --> 01:21:31.929
hugo bowne-anderson: And he he showed me how to do that.

450
01:21:33.209 --> 01:21:53.169
hugo bowne-anderson: So incorporate some retrieval processes into your app. Okay? And I would encourage you to think about exploring advanced retrieval methods like semantic search or chunking strategies. And there are so many ideas that I thought to create a notebook around, different things to do, and different ways to

451
01:21:53.170 --> 01:22:11.270
hugo bowne-anderson: think about this and so once again generating embeddings, thinking about different. And we haven't talked about chunking yet, but when you have very large things you want to do them in different sized chunks, and we can can talk about that moving forward. I would also encourage you to think about.

452
01:22:11.670 --> 01:22:13.450
hugo bowne-anderson: Do you want your app to.

453
01:22:13.550 --> 01:22:30.339
hugo bowne-anderson: you know, reach out to people based on it? Or do you want to match profiles to particular jobs? Do you have a job listing that you want to match profiles to? Or do you have profiles that you want to match a job listing to using all of these different tools? The other thing.

454
01:22:31.630 --> 01:22:37.660
hugo bowne-anderson: at the risk of biting off more than anyone can chew, but it is highly relevant.

455
01:22:38.403 --> 01:22:40.220
hugo bowne-anderson: I'll once again post this

456
01:22:40.520 --> 01:22:51.780
hugo bowne-anderson: in. I'm very excited because this app that I built to query our workshop transcripts. You may recall last time I showed you the basic Q&A. Which

457
01:22:56.440 --> 01:23:16.109
hugo bowne-anderson: essentially says you're a helpful assistant. Answer questions based only on the provided context. And I wasn't able yet to show you the simple rag version which is the one that I stood up on modal and served to you all. But I encourage those interested to have a look through this this code, and so I've got my embedding functions. I've got my system prompt.

458
01:23:16.479 --> 01:23:36.389
hugo bowne-anderson: I'm doing a bit of counting tokens and figuring out cost and that type of stuff. Then a bit of chunking, working with metadata, extracting the workshop material, splitting into paragraphs, bit of preprocessing, creating the chunks, and so on. Then some vector storage stuff. So using chroma and vector dB, adding the chunks to the collection.

459
01:23:36.410 --> 01:23:58.680
hugo bowne-anderson: then querying the collection, then retrieval functions. And I, if you're interested, please do look through this code very happy to chat about it very happy to see pull requests about it as well, and then some logging. So. But I just do want to say, in 781 lines of code, I was really able to to build something which, at least as a 1st approximation, stands up pretty well and

460
01:23:58.680 --> 01:24:08.879
hugo bowne-anderson: gives me a bunch of logs and that type of stuff and allows you all to interact with it. So once again, I'll include this all in ideas for project and homework stuff.

461
01:24:09.176 --> 01:24:19.240
hugo bowne-anderson: But it would be exciting if anyone wants to work on this app with me. That would be fun. The other thing, I I suppose, worth mentioning is, as you all know

462
01:24:19.400 --> 01:24:34.869
hugo bowne-anderson: it. Currently, we're only working with workshop one from this cohort, and I want to start adding more workshops to it as well. So that's enough out of me. I'm very excited to

463
01:24:34.990 --> 01:24:57.399
hugo bowne-anderson: introduce William, one of our builders in residence to the proverbial virtual stage, to talk about what is production. This, this is one of the things I love about this course as well. Is, you know, we have conversations and new material and new ways of speaking with each other. Come, come out of it. I met William earlier this year. When he took

464
01:24:57.731 --> 01:25:12.310
hugo bowne-anderson: the 1st cohort of this course got along like a house on fire. I learned a lot from him, and you know all the stuff he's put into production. It included health I've been very excited to learn about, and everything he can say.

465
01:25:12.420 --> 01:25:19.609
hugo bowne-anderson: I also just want to congratulate William on his new position at Maven. Health. Is that? Is that correct? William?

466
01:25:20.940 --> 01:25:22.340
William Horton: Maven maven clinic yep.

467
01:25:22.340 --> 01:25:33.579
hugo bowne-anderson: Maven Clinic. I'm I'm sorry. And I don't know if Nathan has conveyed this to you yet, but him and his wife have been used maven clinic for some time, and and love it so.

468
01:25:33.580 --> 01:25:42.810
Nathan Danielsen: Yeah, I'm I'm just gonna chime in. We're super promoters at my work. And previous place, anyone who's gonna have a kid. I'm like, have you heard about Maven Clinic? Because it's like that? Amazing?

469
01:25:47.240 --> 01:25:48.839
Nathan Danielsen: There you go. Congratulations.

470
01:25:49.716 --> 01:25:50.429
William Horton: Thank you.

471
01:25:51.320 --> 01:25:52.739
hugo bowne-anderson: Well, let's jump in, William.

472
01:25:53.640 --> 01:25:55.748
William Horton: Alright! Let me share my

473
01:25:56.380 --> 01:25:57.069
hugo bowne-anderson: You are totally.

474
01:25:57.070 --> 01:26:03.620
William Horton: Give me. Yeah, I got it here. See screen.

475
01:26:05.030 --> 01:26:07.789
William Horton: Okay, I share that.

476
01:26:07.900 --> 01:26:10.169
William Horton: People can see the slides right.

477
01:26:11.110 --> 01:26:12.510
hugo bowne-anderson: Yep. Looks great.

478
01:26:12.510 --> 01:26:14.981
William Horton: Okay, great and I will say, like,

479
01:26:15.550 --> 01:26:17.772
William Horton: I'll probably go up till the end.

480
01:26:18.290 --> 01:26:39.760
William Horton: So people have any questions just like, drop them in discord, and I'll definitely follow up after if I don't have time to to answer them. Live. Yeah, thank you, Hugo, for the Intro this is what is production? And the 1st thing I always like to answer is like, Why am I giving this talk? This one in particular

481
01:26:39.930 --> 01:26:57.749
William Horton: is because people say production a lot. The original inspiration for this actually was. Hugo was talking to Joe Reese, I think. And there's this argument over whether vibe coding can be used in production, and this talk is not at all about vibe coding. But it got my ears turning to think like.

482
01:26:58.010 --> 01:27:21.910
William Horton: Well, what does that mean? Like? We can only answer that question if we know what is in production. And this top left thing is a is an article that made Hacker news like 2 days ago. That has a similar viewpoint of vibe coding. If you don't need production grade perfection, and I promise. I didn't coordinate the slides with Hugo, but I did also excerpt some tweets from last night's like agent framework fight, and

483
01:27:21.910 --> 01:27:35.679
William Horton: specifically Samuel called and said, it's pretty brave of you to start referring to other people's libraries as not really production ready. So there's like a lot that's loaded into these terms and questions of like, what is good for production and what is not.

484
01:27:36.004 --> 01:28:04.599
William Horton: It can seem like, there's this big dichotomy people are creating like production on one side and not production, not production ready on the other. So it it seems worthwhile for to me to actually like investigate what we mean when we're talking about these things. This course also says production a lot in the description of the course, he said, a space to build production grade systems that scale beyond Poc, the 1st workshop we had a presentation from base 10 AI inference in production.

485
01:28:04.890 --> 01:28:13.709
William Horton: What does it take to run inference in production like we're we're using these words. But sometimes I think we lose track of what we're actually meaning. So

486
01:28:13.820 --> 01:28:40.960
William Horton: we're going to spend 20min today diving into it. My other kind of motivation is, I think, production can be intimidating. Especially, I've seen like data scientists and non software engineers I've worked with and sometimes be intimidated by people with engineering backgrounds who are like, is it production ready? Or how do you productionize this? And so that's another thing. I kind of want to interrogate and look at and say, like, do we have to be creating such? This like, split?

487
01:28:41.360 --> 01:28:46.270
William Horton: So yeah, in the end, what we're gonna get into is, what does production really mean?

488
01:28:46.795 --> 01:29:06.510
William Horton: Before I get too far into the material, I do want to introduce myself. And this is important. Not just so you know me. But I think people's backgrounds play a large part in how they define production I just mentioned. Obviously, like data, scientists view it differently from engineers. And so I want to give you a sense of where I'm coming from. In this

489
01:29:06.670 --> 01:29:30.380
William Horton: production debate. So who am I from beginning of my career to up till now. I'm going to give you a short overview. I got it into software engineering through a boot camp called app Academy in New York. At the time I learned ruby on rails and react. And yeah, I did that 3 months full time. And then I got my 1st job, which was at a small investment bank in New York. Literally like a CTO me.

490
01:29:30.540 --> 01:29:38.069
William Horton: One other dev 2 investment bankers doing deals. I learned python. I was there for a year. They ran out of money.

491
01:29:38.210 --> 01:30:00.949
William Horton: so then I joined a company called Blink Health. This is now called blinkrx discount prescription drugs. I was doing full stack engineering there, similar to like Goodrx, or like hims and hers. If you familiar with those companies. Then I started getting more interested in data. And Ml, so I worked at compass the real estate brokerage. I started working on our like

492
01:30:01.280 --> 01:30:23.769
William Horton: real estate Mls. Ingestion. If you're familiar with the industry like, that's where all the listings come from. And then they started an AI Ml. Team. I moved on to that team. The last 4 years I've been in included health doing Ml. Engineering, and as Gen. AI. Has gotten more popular, have been focused on tooling for that. And then, like very, very recently, I have joined the AI platform team at Maven Clinic. So

493
01:30:24.030 --> 01:30:52.669
William Horton: that's like a a short tour of my career. And I think that's all to say, I've worked across a bunch of different industries. I've worked at companies all the way from 3 people to like 500 engineers plus across different job titles, different parts of the stack. And I've done traditional quote unquote, soft, one software engineering as well as more. Ml focused work. So I think I have a pretty broad view of these things. And and I'm gonna try to bring that to this discussion.

494
01:30:52.990 --> 01:31:21.015
William Horton: But another very important thing is, I do not have a Cs degree. I mean, I mentioned, I went to a boot camp. I studied social sciences, undergrad my main programming experiences were taking intro Cs. Which I did well at, I will say and then, like statistical analysis of surveys of people in South America, my thesis was on religion and politics in Latin America. So yeah, that was what I did. Undergrad for programming.

495
01:31:21.430 --> 01:31:23.240
William Horton: what does that mean for this talk?

496
01:31:23.480 --> 01:31:44.700
William Horton: I'm not so interested in ending up a 1 right answer. And I want to like highlight that because I think it might frustrate some people who do come from more of a Cs background. But, this is like where I'm coming from. And this is a fairly philosophical talk, and if we don't come up with something definitive in the end. You should blame all the people who made me read things like Hegel

497
01:31:45.150 --> 01:31:47.980
William Horton: Marks Foucault.

498
01:31:48.580 --> 01:31:56.710
William Horton: If you don't know who people are, you're probably the the better for it. I spent a lot of time reading through these pages in college and

499
01:31:56.790 --> 01:32:26.590
William Horton: not getting any single answer to the questions that I had. So that's just a little preface for what we're doing here today. So what I want to do is start with the smallest definition of production. So I want to like, get to something that I feel like I can defend. And then we might build up from there. And and really, I'm gonna start from my own experience. So I mentioned, like, I've been working as a software engineer. I know that what I did at those companies. I put software into production, at least in some cases. Like

500
01:32:26.790 --> 01:32:45.620
William Horton: if I didn't, I don't know what I was doing there. I I. Still, I kind of have to start from that point where we called it production. We were building stuff. It was real software we were paid to build. So you know, I'm taking a couple of projects from that experience and using that to inform how I define production. So

501
01:32:45.951 --> 01:33:05.198
William Horton: a couple of projects I'm gonna refer to in the talk as examples of things that were real production software. So one thing we had at Blinkrx was due to different business business agreements. We had to quickly transfer a large percent of customers from one pharmacy chain to another. And that's a team that I worked on.

502
01:33:05.870 --> 01:33:21.360
William Horton: It was a relatively urgent task, and I've just put a this is a map of different pharmacies in DC, at compass. One of the big projects my team worked on was personalized recommendations on the homepage. So you come to our site. You view a bunch of different listings.

503
01:33:21.360 --> 01:33:50.829
William Horton: Can we create listings that are of interest to you, based on the characteristics of what you're looking at. And then it included health. The project I'm going to use is a project. I worked on cross functionally to surface care gap data. So using your medical claims, can we inform providers when you're due for a screening like colon cancer screening, or the kind of things people are supposed to do every year or every several years.

504
01:33:51.010 --> 01:34:13.979
William Horton: These are all projects I worked on. These are all projects that I'm relatively confident represent production software. And so this is like the basis I'm gonna work off to try to start to come up with a definition of what is and isn't production. So to start, it really is to me the environment where users interacted with the software that I delivered in those projects. So we put some software out there.

505
01:34:14.040 --> 01:34:28.789
William Horton: And people were using it. And because there were real people using it, that to me represents production, and that's kind of the base definition I feel like I can defend is this stuff that we build and put out there.

506
01:34:29.410 --> 01:34:33.519
William Horton: Okay, so before I get too far into like, what production is.

507
01:34:33.720 --> 01:34:37.400
William Horton: A lot of this talk is gonna be debunking sorry.

508
01:34:37.530 --> 01:34:41.820
William Horton: debunking common misconceptions. So I'm gonna take some water.

509
01:34:43.900 --> 01:34:45.940
William Horton: Because I think there's there's a lot

510
01:34:46.310 --> 01:34:52.780
William Horton: the people think is required of production that actually isn't so what production isn't

511
01:34:52.940 --> 01:34:56.160
William Horton: to me. One myth is that it's uniformly high risk.

512
01:34:57.670 --> 01:35:04.760
William Horton: look, some people write software for NASA. Some people write software that's used like by surgeons in an operating room.

513
01:35:05.140 --> 01:35:06.680
William Horton: You probably don't.

514
01:35:06.980 --> 01:35:21.499
William Horton: I never have written something that that is that high risk? A lot of people will spend their whole career and never write something like that. And so that's just something to keep in mind is production isn't 1 level of risk. So looking at my projects like

515
01:35:21.650 --> 01:35:32.800
William Horton: real estate recommendations on the homepage is not a life or death decision. You know, it's not even something we can actually get right every time, like some people won't click through the recommendation. Some people will

516
01:35:32.980 --> 01:35:53.670
William Horton: some. Some of that's just based on probability. If we don't do a good job at it, it could be financial risk to the company, but even then there'd have to kind of be a consistent pattern of really poor recommendations. On the other hand, I have worked on projects that meant life or death. Healthcare screenings can be a life or death decision.

517
01:35:53.770 --> 01:36:12.540
William Horton: And the a question in that case the timescale is pretty long, like we were mostly talking about like colon cancer. You're supposed to get every 10 years right? Some of these you get every one year. So it's a it's a longer time scale. It's not like real time risk. But these decisions are going to determine people's health outcomes. And then pharmacy transfer.

518
01:36:12.780 --> 01:36:27.036
William Horton: It's kind of a wide range of risk, because it depends on the medication the person's taking. But in that case it was more urgent because we were talking about patients who are actually going to show up to a pharmacy and get their medication or not get their medication. And so it is.

519
01:36:27.800 --> 01:36:44.979
William Horton: It was pretty crucial that we work in an abbreviated timeframe. So you know, across different projects I've had, you can see there's varying levels of how quickly we have to work and how carefully we have to work. And these all were production software.

520
01:36:45.670 --> 01:36:53.329
William Horton: basically, the definition of production should match the risk level of the actual use cases that we're doing.

521
01:36:54.960 --> 01:36:59.799
William Horton: So that's that's myth. One myth, 2. I think this is one of the most common

522
01:37:00.010 --> 01:37:03.138
William Horton: people think production means large scale.

523
01:37:04.260 --> 01:37:13.319
William Horton: I think that this one doesn't make sense to me on like a basic level, because, like, if we set some threshold for production like Google scale.

524
01:37:13.390 --> 01:37:39.690
William Horton: That means that like 80% of software engineers are never working in production, which, like, just strikes me as kind of an absurd like assertion. So I I don't really like, buy this one on its face, and like any threshold you choose, like, there's gonna be a bunch of people who are doing work that that don't fit that threshold. So I don't think you can really have a hard definition of scale in your in your version of production. I do want to make an analogy, and

525
01:37:40.060 --> 01:37:45.790
William Horton: I'm somewhat skeptical of analogies to like other engineering disciplines. But I think this one is pretty apt like

526
01:37:46.140 --> 01:38:04.319
William Horton: talk about bridge building like civil engineering. All of these bridges are in production like on the left. That's like a bridge that you might walk over in the park and on the right. This one I actually picked especially this is like a bridge outside of Savannah, Georgia, that I've driven over and been terrified because, like, I have a huge phobia of tall bridges

527
01:38:04.500 --> 01:38:07.849
William Horton: like, I'm always thinking the card's gonna go over the side. But anyway.

528
01:38:08.120 --> 01:38:14.060
William Horton: these are both in production. They're both used. People want to go across them. It would be bad if they failed. Right?

529
01:38:14.430 --> 01:38:19.040
William Horton: I think often, like modern software engineering practices is like this. It's like

530
01:38:19.160 --> 01:38:23.189
William Horton: you're building a concrete suspension bridge through the park.

531
01:38:23.320 --> 01:38:26.880
William Horton: and I guess you're like thought process is well.

532
01:38:27.010 --> 01:38:31.970
William Horton: someday in the future they might want to put an interstate through the park. So we need to be ready to scale. And it's like

533
01:38:32.160 --> 01:38:43.830
William Horton: they're gonna put an interstate through the park. There's gonna be a lot more changes like, besides, just fixing your footbridge so like, you might as well build the footbridge now. And like, we can address later. Later. So

534
01:38:44.100 --> 01:39:02.810
William Horton: yeah, like, this is really one thing that gets me about like a lot of modern software engineering practices like that is what it looks like when you take this like high scale infrastructure like you're doing like distributed micro services. And your app gets like one request every hour like this. This is what it looks like

535
01:39:03.020 --> 01:39:04.709
William Horton: like this. That's just

536
01:39:05.160 --> 01:39:16.020
William Horton: it doesn't look great. It's kind of absurd. So yeah, like. And I've worked on projects across a bunch of scale like pharmacy. Transfer was a thousand thousands of users.

537
01:39:16.310 --> 01:39:35.290
William Horton: Compass was higher traffic right? Like I looked this up because I didn't remember the numbers from my time there, but, like February of this year, compass got almost 5 million visitors a month. When you break that down by second, it still doesn't meet the scale that I think a lot of people would qualify as like high scale like, it's only 2 queries per second. So

538
01:39:36.620 --> 01:39:46.739
William Horton: And then help for the healthcare screening project I talked about. We had to process data on like millions of users that were covered. But then actual service was only serving like

539
01:39:47.060 --> 01:39:50.030
William Horton: order, magnitude like hundreds of staff. And so

540
01:39:50.180 --> 01:39:53.829
William Horton: like, there's a wide variety of of

541
01:39:54.060 --> 01:40:08.930
William Horton: scales that I've seen in production apps. And all of these are real like production apps. They are not like boy apps just because they're only serving like less than one request per second, or even one request per minute.

542
01:40:09.514 --> 01:40:15.890
William Horton: So my 3rd myth is like production. Some people think is like error, free, or 100 reliable.

543
01:40:16.431 --> 01:40:24.599
William Horton: I know this to not be true, because I myself have ship bugs to production. I think everybody who has worked as a software engineer has done it.

544
01:40:25.520 --> 01:40:30.770
William Horton: So people wanted to find production as the place where, like errors aren't acceptable.

545
01:40:30.930 --> 01:40:51.969
William Horton: But I think production is the place where errors are inevitable, like, I've never worked anywhere where we didn't. Ship bugs. Production like this is just a an observation of reality. And so you know, you can even look at like companies that are, I think, doing a pretty good job with reliability like I looked at Aws Sla's and Google, like

546
01:40:52.710 --> 01:41:12.630
William Horton: their salespeople, would be super happy if they could sell 100 uptime like if that was feasible. I'm sure that they would love to say that they're going to keep your stuff up 100, but they don't. In the sla. They have 99.5 for Ec. 2 instances, or Gemini is 99.9. They recognize that, like some small fraction of the time.

547
01:41:12.680 --> 01:41:24.679
William Horton: they will encounter some errors in production, and they have to account for that. So I think, like, if these organizations are recognizing that, I think that, like smaller organizations, can recognize that as well.

548
01:41:25.280 --> 01:41:26.239
William Horton: You don't

549
01:41:26.410 --> 01:41:36.370
William Horton: want to have errors in production, but at the same time you can't define production as having no errors that just doesn't reflect reality. So

550
01:41:36.640 --> 01:41:49.529
William Horton: those are my, maybe my pet peeves about what production isn't. I do want to spend time on what production is now. So I said before, roughly, I want to say, production is the environment where users interact with the software

551
01:41:49.990 --> 01:41:55.460
William Horton: is that it like is that the minimal definition that I can defend about production.

552
01:41:57.040 --> 01:42:10.899
William Horton: I think that's a good definition, but I don't think it would be that like satisfying to end the talk with that. So I want to add some assumptions and and build a little bit on what kind of our minimal definition of of production is. So

553
01:42:11.050 --> 01:42:13.530
William Horton: I think it's safe to add this assumption that

554
01:42:13.630 --> 01:42:24.160
William Horton: you want the software to do what it's intended to do for the end user. That's like an assumption about production that I think I could support logically.

555
01:42:25.200 --> 01:42:35.119
William Horton: because software is doing something for us in the end. Right? Like, we're writing or reading emails. We're listening to music. In just the same way that this bridge is

556
01:42:35.340 --> 01:42:39.780
William Horton: allowing us to walk over it. The software is is being used by us.

557
01:42:40.010 --> 01:42:40.900
William Horton: So

558
01:42:41.080 --> 01:42:47.519
William Horton: we want it to do what it's meant to do. It might not right. I just went through a whole thing about how you have errors in production. Things can fail.

559
01:42:47.600 --> 01:43:16.469
William Horton: But I think the point in this kind of assumption we're going to build on is that you want it to do what it's intended to do. And so we adopt practices that that lead us to be able to accomplish that. So one thing we can say about production is, it's the final stage in a series of environments. I actually think this is one of the most defensible things about production. Like this is a pretty common pipeline, something like local staging production. The reason why I think I can like logically argue for this is because

560
01:43:16.740 --> 01:43:46.620
William Horton: if you didn't have other stages like you wouldn't need a term called production, you would just like call it the server, or like the computer. That would be where the code is. So so the fact that we have a term for it, I think somewhat implies that there's there's other things that come earlier. We're not just putting all the code in one place. So you know, this is a common setup. Where you have some amount of local development. People might call it staging Qa uat. And then things make it to production.

561
01:43:48.100 --> 01:43:52.620
William Horton: Production includes observability. This is something that's been talked about in the course.

562
01:43:53.410 --> 01:44:03.609
William Horton: And I think that this builds off of our kind of ideas, because you want to know that it's working for the user. If if you intend for production software to work as it's meant to work.

563
01:44:03.780 --> 01:44:26.640
William Horton: Well, how will you know that it's working unless you add some observability? I think, like scale observability can be overdone. You know you might not need all the complexity that some projects need for yours. It really can be as simple as starting with your server logs like here, like, I see 200. That means that the request succeeded. I see errors. It failed. But

564
01:44:26.900 --> 01:44:34.019
William Horton: if you don't have some amount of observability, you're not really fulfilling that contract of

565
01:44:34.380 --> 01:44:37.871
William Horton: getting the software to work for the end user.

566
01:44:38.580 --> 01:44:55.549
William Horton: this is another one. That kind of comes out of these points, which is production means real use user data. At the very least, you don't want to lose it or corrupt it like when people come and click on listings. And we want to make recommendations. At the very least, we want to record the clicks they made

567
01:44:55.930 --> 01:45:18.359
William Horton: and actually like, have it be those and not like recommend them a house in La when they were looking in New York, like just on the basics of the app functioning. We want to try to retain the data correctly, and in some cases this might lead to more requirements. This is not all production, but you might face regulation. If you're in healthcare, there's hipaa. Gdpr, so

568
01:45:19.350 --> 01:45:24.349
William Horton: the software meeting real users means that we will end up with real user data.

569
01:45:24.841 --> 01:45:31.280
William Horton: This is the last one which is probably debatable, but I think is interesting. Production. Code is updated over time.

570
01:45:31.812 --> 01:45:38.270
William Horton: How many of our current faxes would be unnecessary if we only shipped once and never updated like.

571
01:45:38.470 --> 01:46:02.650
William Horton: it's just something to think about that's interesting. But the reality is like, I've never worked at somewhere where we didn't update code. Even people who work on embedded stuff. Now, ship firmware updates like. And so this is the last piece to kind of think about that leads to some of these practices that are common in production like we make get commits because we want to know what's codes included, with what Cicd being able to roll back. This is because

572
01:46:02.810 --> 01:46:10.779
William Horton: as just a observation of reality, most production code you're working on is going to be updated and iterated over time.

573
01:46:12.150 --> 01:46:21.569
William Horton: so yeah, those are some things that we can say about production. You could probably argue about these 2, but I think that they build towards some useful concept of what actually is going on.

574
01:46:21.890 --> 01:46:26.010
William Horton: In conclusion, you might still be wondering what is production.

575
01:46:26.230 --> 01:46:39.660
William Horton: And that's okay. The point of the talk is not to give you an answer. It's really to empower you to be in this scenario, and the engineer says this is instruction ready, and you say, why not? And you can start a conversation over what

576
01:46:39.710 --> 01:46:52.490
William Horton: production means at your company. Going back to this guy, Karl Marx, I want you to seize the definition of production. And what does that mean? It? It means ultimately, knowing that production is relatively

577
01:46:52.500 --> 01:47:10.640
William Horton: to your product and your users needs to not let anybody sell you on an overexpansive view or more scale than you need. And ultimately, it means trying to provide quality software for your particular users. And to me, that's really like the fundamentals of production.

578
01:47:11.150 --> 01:47:14.079
William Horton: Thank you all so much. I know we're

579
01:47:14.320 --> 01:47:20.109
William Horton: having to move into other things, but feel free to leave questions and discord, and I will definitely follow up on them.

580
01:47:21.020 --> 01:47:29.089
hugo bowne-anderson: Thank you so much, William. That was an incredible, incredible talk. I do want to say a few things that resonated with me, and then we'll

581
01:47:29.710 --> 01:47:32.549
hugo bowne-anderson: jump over. The 1st is just thinking about

582
01:47:32.720 --> 01:47:49.479
hugo bowne-anderson: production as being a work in progress itself. Right? And you know, we can deploy, and we often do deploy a model, and that's in production. But then a wonderful stage in is then, when you can start experimenting with new models and have challenger models that you're running in parallel and develop the infrastructure

583
01:47:49.480 --> 01:48:03.669
hugo bowne-anderson: to do that. So the idea of then experimentation is further along the continuum of production. So these it isn't like these discrete units, but it is always a process. And I love how you spoke to those those types of things. I also love the idea of

584
01:48:03.820 --> 01:48:13.150
hugo bowne-anderson: production being the environment where users interact with the software. And I would even maybe extend it to think interact with the software or software artifacts.

585
01:48:13.420 --> 01:48:15.680
hugo bowne-anderson: Because I. So you know, I

586
01:48:15.930 --> 01:48:23.999
hugo bowne-anderson: so Netflix, for example, very famous for their batch processing and recommendation system and that type of stuff arguably for Netflix.

587
01:48:24.060 --> 01:48:46.760
hugo bowne-anderson: the biggest value data. And Ml and AI can provide is helping them with logistics and figuring out where, because they're a studio, not just a streamer. Now, right? Figuring out what location is best to film. Given the people, they need to fly in for it the cost of certain things that expresses itself as software, but it actually expresses itself in documents produced by software that executives read. Okay, so that's the expression of a software artifact.

588
01:48:46.800 --> 01:49:04.760
hugo bowne-anderson: Another thing there was a famous example of, you know. I think it was at Netflix. But I need to look into this of an Ml. System. It was an executive trying to figure out where to put billboards like which highways in Los Angeles to put billboards for their product right? And

589
01:49:05.020 --> 01:49:22.690
hugo bowne-anderson: that whole machine learning and data science process ended up in a slide deck that they would ingest to make multi 1 million dollar decisions there. So think it isn't just like recommendation systems. All of these things. It is the production of software and artifacts that can help people make decisions or automate things. Yeah.

590
01:49:25.340 --> 01:49:35.389
hugo bowne-anderson: awesome. So we'll jump to the other call. I am reminded, because Constantine, who is our guest speaker, is at. Can can you see discord? William.

591
01:49:37.537 --> 01:49:39.410
William Horton: It's like I can see you sharing it now. Yeah.

592
01:49:39.410 --> 01:49:43.919
hugo bowne-anderson: Yeah, yeah, yeah, exactly. I don't know quite how. Well, anyway,

593
01:49:44.830 --> 01:50:07.671
hugo bowne-anderson: learn prompting is very kindly giving 3 months free of their learn prompting plus lessons. It's all about prompt engineering and prompting and that type of stuff. I will be giving them all the information today. So I'm going to put this form in the channel once again, once Constantine starts talking. But this is this is the deadline. Similarly, with hugging face, I've emailed you all and put it in

594
01:50:08.570 --> 01:50:23.869
hugo bowne-anderson: discord several times. But I need you to join the organization I've set up in order to provision you with credits there. So please do that in the next couple of hours, so I can can do that all right. Thanks, everyone, and I'll see you in the next talk.

