Question	Response
"What are the key learning outcomes of this workshop series on LLM-powered applications?
"	The key learning outcomes of this workshop series are to teach participants a practical, first-principles approach to building LLM-powered applications, focusing on workflows, iteration, and hands-on development. The course emphasizes building production-grade systems that scale beyond proof of concepts, with a deep dive into the non-deterministic nature of these systems, observability, testing, and evaluation, rather than just exploring the latest tools or trends. Participants will also learn how to iterate, debug, and adapt their applications, and connect micro-level LLM performance to macro-level business metrics.
"How is this course different from others that focus on tools like LangChain or agentic frameworks?
"	This course is different because it focuses on principles and workflows—such as evaluation-driven development, observability, testing, and iteration—rather than on specific tools like LangChain or agentic frameworks. While tools are introduced, the emphasis is on teaching foundational skills and approaches that will remain relevant even as specific tools change, rather than providing a deep dive into every new tool or trend.
"Will the course include hands-on projects or just lectures?
"	The course will include hands-on projects in addition to lectures. The focus is on practical, first-principles approaches to building LLM-powered apps, with an emphasis on workflows, iteration, and hands-on development. Participants will have opportunities to build production-grade systems and immediately start building apps together during the workshops.
"How do you balance theory and practice in the course?
"	The course balances theory and practice by focusing on first principles and practical workflows for building LLM-powered applications. While it introduces relevant tools and concepts, the emphasis is on hands-on development, iteration, and evaluation, ensuring participants learn both the foundational principles and how to apply them in real-world scenarios.
"How is this course tailored for both data scientists and software engineers?
"	This course is designed for both data scientists and software engineers by focusing on practical, hands-on workflows for building LLM-powered applications, rather than just tools or trends. It emphasizes principles like iteration, evaluation, and observability, which are relevant to both backgrounds, and brings together instructors and builders with expertise in both data science and software engineering to support participants from either field.
"What is the duration of the course?
"	I'm sorry, but based on the provided transcript, the specific duration of the course is not mentioned.
"Can you explain what “evaluation-driven development” means in more detail?
"	Evaluation-driven development (EDD) is an iterative approach to building LLM-powered applications where you continuously build, monitor, evaluate, and improve your system. Instead of relying solely on traditional specs and deployment cycles, EDD emphasizes creating evaluation harnesses (like test sets and automated scripts) to assess your app’s performance—often even before real users exist. This process helps you identify issues such as non-determinism and hallucinations early, guiding improvements based on measurable outcomes rather than just initial demo results.
"What are some examples of an “evaluation harness” mentioned in the intro?
"	An evaluation harness is described as a reproducible script or system used to evaluate your LLM-powered application. Examples mentioned include building a synthetic dataset and labeling outputs by hand, then using this labeled data to create a basic evaluation harness—such as a test set and an LLM-as-a-judge setup—to measure things like latency, cost, and business-relevant metrics. This allows you to systematically assess and improve your MVP even before real users exist.
"How do you measure success in a non-deterministic LLM system?
"	Success in a non-deterministic LLM system is measured through evaluation-driven development, which involves building, monitoring, evaluating, and iterating rapidly. This includes creating a minimum viable evaluation (MVE) with test sets and automated evaluation harnesses, logging and analyzing inputs and outputs, and aligning micro-level LLM performance with macro-level business metrics to ensure the system meets desired outcomes.
"What’s the difference between fine-tuning, prompting, and using embeddings?
"	Based on the transcript, prompting involves crafting specific instructions or queries to guide the LLM's output, while fine-tuning refers to training the model further on domain-specific data to improve its performance in specialized tasks. Embeddings are used to represent data (like text) as vectors for tasks such as retrieval or similarity search, and people often fine-tune embeddings and re-rankers for better retrieval performance. The transcript notes that you can often achieve your goals with prompting rather than fine-tuning, and fine-tuning is more commonly used for domain-specific expertise or when migrating to smaller models.
Could you explain how observability applies to LLM apps?	Observability in LLM apps refers to the ability to monitor, log, and trace the data flowing through your system, including inputs, outputs, and the behavior of the LLM. This is crucial because LLM-powered software is non-deterministic, so capturing data in a human-readable form and evaluating results allows you to quantify performance, identify failure modes, and iterate effectively. Robust observability helps ensure you can improve your app by understanding how it behaves in both development and production environments.
