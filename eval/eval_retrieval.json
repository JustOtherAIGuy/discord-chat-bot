{"question": "Are there any guest lectures?", "response": "Yes, there are several guest lectures scheduled as part of the course. \n\nNext week, you can look forward to talks from notable speakers including:\n\n1. **William Horton** - He will discuss the concept of \"production\" in the context of AI systems.\n2. **Constantine from Learn Prompting** - He will provide insights into prompting techniques and you'll receive free access to their educational platform.\n3. **Paige Bailey from Google DeepMind** - She will talk about building AI systems using Gemini and how to leverage a $300 credit for Gemini APIs.\n4. **Shredder from Replicate** - He will conduct a session on building multimodal applications from scratch.\n5. **Greg Cicerelli** - A former VP of data science at GitHub, he will demonstrate how to build with AI assistance.\n6. **Philip Carter from Huntingcomb** - He will discuss building an MCP (Multi-Channel Platform) from the trenches.\n7. **Alan Nickel** - He will talk about building reliable agents and managing state.\n\nThese lectures will provide valuable insights and practical knowledge from experienced professionals in the field. Be sure to attend and engage with the content!", "context_info": {"num_chunks": 10, "context_tokens": 10260, "completion_tokens": 236, "embedding_tokens": 15360, "workshops_used": ["WS1", "WS3", "WS4"], "chunks": [{"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"workshop_id": "WS4", "original_chunk_id": "334c243a-3488-4cde-a7c9-f1f7345397fb", "source": "sentence_chunking", "speaker": "Unknown", "token_count": 1055, "timestamp": "Chunk 18", "position": 17}, "id": "WS4_334c243a-3488-4cde-a7c9-f1f7345397fb", "relevance": 1.0}, {"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "f9e33134-8ebe-4b68-90f8-9d442e7bb591", "workshop_id": "WS4", "position": 17, "timestamp": "Chunk 18", "token_count": 1055}, "id": "WS4_f9e33134-8ebe-4b68-90f8-9d442e7bb591", "relevance": 1.0}, {"text": "As well learn prompting. They published the prompt report which was really the 1st comprehensive 78 hugo bowne-anderson: report on prompting techniques. They did it with Openai, with a bunch of with 20 different organizations, and they'll be coming to give a talk about prompt engineering in the software development lifecycle and prompt injection attacks. But they're giving. They've got a wonderful educational platform. They'll be giving 3 months of plus access and then prodigy. I don't know if you know prodigy, but it's human in the loop annotation, fine-tuning and Nlp. Workflow building from Innis, Montani and Matthew Honnibal, who built Spacey 79 hugo bowne-anderson: and and maintain it. So that's all to say we have instructions for getting most of them, I think, replicate. For example. Yes, I need. There are a couple which I still need to just follow up on several things, but we'll sort all of those out in the near future, but most of them have instructions on how to do. Get them. So, please. 80 hugo bowne-anderson: Pretty, please do that. 81 hugo bowne-anderson: and I just encourage you all to 82 hugo bowne-anderson: use. I mean, use these tools as much as you can while building during the course, because we're here to support and talk about them and and build with you. I do want to say I do appreciate. Everyone likely has more than full time, jobs and families, and all of these things or subsets of these. So I do understand, particularly in our line of work. Time is an incredibly precious 83 hugo bowne-anderson: resource. So 84 hugo bowne-anderson: all that's to say is, if you don't have much time to build outside, that's totally cool. If you, if all you do is attend these workshops and nothing else. You'll get a huge amount out of the course if you keep building also, and keep in the discords, and then join the office. Hours and pods will set up, you'll get even even more so. I like to think the world's your oyster in proverbial oyster in a lot of ways. So just getting started in terms of 85 hugo bowne-anderson: set up. 86 hugo bowne-anderson: We use several tools, and my goal is to use as few tools as possible. I will rarely adopt a new tool 87 hugo bowne-anderson: if I don't have to. So in the past we used in the first.st So in the 1st cohort, we use Google drive. This is for slides supporting material. This type of stuff we use github and codespaces for code and also codespaces, feel free to clone the repository and do what you want with it, but we want to make sure that you can get set up and up and running as quickly as possible, and I'll introduce you to code spaces later. It's kind of 88 hugo bowne-anderson: Google colab on steroids or Vs code superpowered in the browser. We use discord for the Async Chat announcements, and the Q. And a 89 hugo bowne-anderson: we use zoom for these workshops, and that's what we did in cohort one. Those were the 4 tools we used now. I didn't even quite need to put notion here to. To be honest, because you don't get to. You don't need to see the notion at all. 90 hugo bowne-anderson: But what I decided to build this time was a course, Wiki, which I would very much love you all to keep private. That's really for this course, as with everything in this course I do think. 91 hugo bowne-anderson: You all are paying like a handsome sum of money to to learn and get access to all of this. So please respect that everyone else in the room is is doing the same as well, and and so don't share share too much with with friends, and we'll put out a lot of free content as as well. Having said that this is the course wiki which we'll keep updated. So I will actually spend a couple of minutes 92 hugo bowne-anderson: going through this. So we have. Yep stuff about us when the sessions are little welcome here. This was the setup before week one and anything you haven't done there feel free to do afterwards. There's the course schedule 93 hugo bowne-anderson: and so you click on that, and it goes to another page, and you'll see. Week one workshop one. We're in that right now. Okay, and you'll see there's something called a workshop. One page coming soon. And so that's empty. But what I'm going to do is put the slides there, put the recording there.", "metadata": {"position": 5, "speaker": "Unknown", "token_count": 1052, "source": "sentence_chunking", "timestamp": "Chunk 6", "original_chunk_id": "4636c618-f768-4f46-a0a6-6fe122036f92", "workshop_id": "WS1"}, "id": "WS1_4636c618-f768-4f46-a0a6-6fe122036f92", "relevance": 1.0}, {"text": "As well learn prompting. They published the prompt report which was really the 1st comprehensive 78 hugo bowne-anderson: report on prompting techniques. They did it with Openai, with a bunch of with 20 different organizations, and they'll be coming to give a talk about prompt engineering in the software development lifecycle and prompt injection attacks. But they're giving. They've got a wonderful educational platform. They'll be giving 3 months of plus access and then prodigy. I don't know if you know prodigy, but it's human in the loop annotation, fine-tuning and Nlp. Workflow building from Innis, Montani and Matthew Honnibal, who built Spacey 79 hugo bowne-anderson: and and maintain it. So that's all to say we have instructions for getting most of them, I think, replicate. For example. Yes, I need. There are a couple which I still need to just follow up on several things, but we'll sort all of those out in the near future, but most of them have instructions on how to do. Get them. So, please. 80 hugo bowne-anderson: Pretty, please do that. 81 hugo bowne-anderson: and I just encourage you all to 82 hugo bowne-anderson: use. I mean, use these tools as much as you can while building during the course, because we're here to support and talk about them and and build with you. I do want to say I do appreciate. Everyone likely has more than full time, jobs and families, and all of these things or subsets of these. So I do understand, particularly in our line of work. Time is an incredibly precious 83 hugo bowne-anderson: resource. So 84 hugo bowne-anderson: all that's to say is, if you don't have much time to build outside, that's totally cool. If you, if all you do is attend these workshops and nothing else. You'll get a huge amount out of the course if you keep building also, and keep in the discords, and then join the office. Hours and pods will set up, you'll get even even more so. I like to think the world's your oyster in proverbial oyster in a lot of ways. So just getting started in terms of 85 hugo bowne-anderson: set up. 86 hugo bowne-anderson: We use several tools, and my goal is to use as few tools as possible. I will rarely adopt a new tool 87 hugo bowne-anderson: if I don't have to. So in the past we used in the first.st So in the 1st cohort, we use Google drive. This is for slides supporting material. This type of stuff we use github and codespaces for code and also codespaces, feel free to clone the repository and do what you want with it, but we want to make sure that you can get set up and up and running as quickly as possible, and I'll introduce you to code spaces later. It's kind of 88 hugo bowne-anderson: Google colab on steroids or Vs code superpowered in the browser. We use discord for the Async Chat announcements, and the Q. And a 89 hugo bowne-anderson: we use zoom for these workshops, and that's what we did in cohort one. Those were the 4 tools we used now. I didn't even quite need to put notion here to. To be honest, because you don't get to. You don't need to see the notion at all. 90 hugo bowne-anderson: But what I decided to build this time was a course, Wiki, which I would very much love you all to keep private. That's really for this course, as with everything in this course I do think. 91 hugo bowne-anderson: You all are paying like a handsome sum of money to to learn and get access to all of this. So please respect that everyone else in the room is is doing the same as well, and and so don't share share too much with with friends, and we'll put out a lot of free content as as well. Having said that this is the course wiki which we'll keep updated. So I will actually spend a couple of minutes 92 hugo bowne-anderson: going through this. So we have. Yep stuff about us when the sessions are little welcome here. This was the setup before week one and anything you haven't done there feel free to do afterwards. There's the course schedule 93 hugo bowne-anderson: and so you click on that, and it goes to another page, and you'll see. Week one workshop one. We're in that right now. Okay, and you'll see there's something called a workshop. One page coming soon. And so that's empty. But what I'm going to do is put the slides there, put the recording there.", "metadata": {"position": 5, "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "68f221e0-c4c3-4ad0-a098-296aa3fc237e", "timestamp": "Chunk 6", "workshop_id": "WS1", "token_count": 1052}, "id": "WS1_68f221e0-c4c3-4ad0-a098-296aa3fc237e", "relevance": 1.0}, {"text": "It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right. So I do want to just quickly go to the next notebook to notebook I have a brief summary of this before, but to give a high level summary. Of what we're doing here. I wanted to give you an introduction to actually summarization is something that isn't that we're still working on here, right. So think about how to evaluate factual consistency in in in summarization. So in this, what we do is we load and preprocess a data set containing source text. Correct summaries and incorrect summaries. So distractor summaries, right. Then we hand label, we annotate summaries for factual consistency using pass fail and then calculate metrics like accuracy, precision, and recall. We identify common failure modes in incorrect summaries to better understand the model behavior. Then we train an LLM as a judge to evaluate summaries based on the human annotations, providing scalability for larger data sets. And then we monitor the LLM judgments over time by validating against human labeled samples to ensure alignment and consistency Okay. So that's what we do here. If you're super excited to jump in sooner, but I'll put a video in this channel, in the channel Later today. And you can see like I use Gemini to do a whole bunch of summarization, okay. And then I export it to a spreadsheet and annotate it. And then we look at pass rates and fail rates and we can see like how different prompts perform based around my human annotation. Once we have that. We can define evaluation criteria Do we want consistency, relevance, clarity. Then we prepared a few shot examples where we have a few label examples to give an LLM as a judge. Then we write our few shot prompt saying, you are an expert evaluator of tech summaries. For each summary, evaluate its quality based on these three things. And Bob's your uncle, so to speak. We can generate LLM judgments, compare them with human annotations. And then we can automate the scalable evaluation and monitor and iterate and move from there. So I know I kind of, that was a whirlwind tour through that notebook but And I appreciate that that's a lot to take in. I do want to say everything in this notebook is what we've been talking about for an hour and a half now as well and foreshadowed last week also so All of these things, this process essentially is under the rubric of evaluation-driven development. So if there's one takeaway I want you to have. It's here. It's on one of my first slides. The one takeaway. Of course, once you have one takeaway, you can't find it, right. Yes. The one takeaway is that As you build out an MVP, build out an MVE. With any minimal viable product, build out a minimum viable evaluation harness that moves beyond vibes as quickly as possible. It doesn't have to be anything super robust. And it's not that different to ML powered software, right. This process. There are some details that are, but both workflows are grounded in test sets. Iteration and failure analysis. So\u2026 um what Do I want to say i've already shown you the app I built that I'd love you all to contribute to and we'll share a repo later with respect. To that. And we already had a look at the logs together um so What I'd like you to do is update the MVP from workshop two and start building out some evaluations. Use everything that I've given you today in order to do this. And I'm really excited to see in the homework channel What you will do with this type of thing and what happens in Builders Club with respect to that. The next workshop, I'm beyond excited for. It will be Stefan. It's my favorite talk in workshop in this course except for this one. No, I'm kidding. But I do love this workshop and we have A lot of guest speakers on Thursday, we've got Innes Montani from Spacey, Pavitra from Quansite. Vincent from Marimo and Catherine Jarmel from KJ Amistan talking a lot about all the work they do with LLMs from human in the loop development to building applications with open source tools. And a lot of Pavitra's work at QuanSight. She works with Travis Oliphant, who created NumPy back in the day, right. But there, you know, I've said we've had like the rug pulled out from under us with respect to open source tooling and they're working on a lot of interesting tooling for building AI powered applications using open source Vinsome's just a An absolute joy who has so many cool tricks and tips And\u2026 Catherine does a lot of work on adversarial threats, so very excited for that. And I'll be getting up at 4.30 a.m. Local time for Innices Talk. I'm very excited to do that, actually.", "metadata": {"timestamp": "Chunk 17", "speaker": "Unknown", "workshop_id": "WS3", "original_chunk_id": "e9c0557a-5b5f-45dd-af0c-dde43e745e04", "source": "sentence_chunking", "token_count": 1055, "position": 16}, "id": "WS3_e9c0557a-5b5f-45dd-af0c-dde43e745e04", "relevance": 1.0}, {"text": "It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right. So I do want to just quickly go to the next notebook to notebook I have a brief summary of this before, but to give a high level summary. Of what we're doing here. I wanted to give you an introduction to actually summarization is something that isn't that we're still working on here, right. So think about how to evaluate factual consistency in in in summarization. So in this, what we do is we load and preprocess a data set containing source text. Correct summaries and incorrect summaries. So distractor summaries, right. Then we hand label, we annotate summaries for factual consistency using pass fail and then calculate metrics like accuracy, precision, and recall. We identify common failure modes in incorrect summaries to better understand the model behavior. Then we train an LLM as a judge to evaluate summaries based on the human annotations, providing scalability for larger data sets. And then we monitor the LLM judgments over time by validating against human labeled samples to ensure alignment and consistency Okay. So that's what we do here. If you're super excited to jump in sooner, but I'll put a video in this channel, in the channel Later today. And you can see like I use Gemini to do a whole bunch of summarization, okay. And then I export it to a spreadsheet and annotate it. And then we look at pass rates and fail rates and we can see like how different prompts perform based around my human annotation. Once we have that. We can define evaluation criteria Do we want consistency, relevance, clarity. Then we prepared a few shot examples where we have a few label examples to give an LLM as a judge. Then we write our few shot prompt saying, you are an expert evaluator of tech summaries. For each summary, evaluate its quality based on these three things. And Bob's your uncle, so to speak. We can generate LLM judgments, compare them with human annotations. And then we can automate the scalable evaluation and monitor and iterate and move from there. So I know I kind of, that was a whirlwind tour through that notebook but And I appreciate that that's a lot to take in. I do want to say everything in this notebook is what we've been talking about for an hour and a half now as well and foreshadowed last week also so All of these things, this process essentially is under the rubric of evaluation-driven development. So if there's one takeaway I want you to have. It's here. It's on one of my first slides. The one takeaway. Of course, once you have one takeaway, you can't find it, right. Yes. The one takeaway is that As you build out an MVP, build out an MVE. With any minimal viable product, build out a minimum viable evaluation harness that moves beyond vibes as quickly as possible. It doesn't have to be anything super robust. And it's not that different to ML powered software, right. This process. There are some details that are, but both workflows are grounded in test sets. Iteration and failure analysis. So\u2026 um what Do I want to say i've already shown you the app I built that I'd love you all to contribute to and we'll share a repo later with respect. To that. And we already had a look at the logs together um so What I'd like you to do is update the MVP from workshop two and start building out some evaluations. Use everything that I've given you today in order to do this. And I'm really excited to see in the homework channel What you will do with this type of thing and what happens in Builders Club with respect to that. The next workshop, I'm beyond excited for. It will be Stefan. It's my favorite talk in workshop in this course except for this one. No, I'm kidding. But I do love this workshop and we have A lot of guest speakers on Thursday, we've got Innes Montani from Spacey, Pavitra from Quansite. Vincent from Marimo and Catherine Jarmel from KJ Amistan talking a lot about all the work they do with LLMs from human in the loop development to building applications with open source tools. And a lot of Pavitra's work at QuanSight. She works with Travis Oliphant, who created NumPy back in the day, right. But there, you know, I've said we've had like the rug pulled out from under us with respect to open source tooling and they're working on a lot of interesting tooling for building AI powered applications using open source Vinsome's just a An absolute joy who has so many cool tricks and tips And\u2026 Catherine does a lot of work on adversarial threats, so very excited for that. And I'll be getting up at 4.30 a.m. Local time for Innices Talk. I'm very excited to do that, actually.", "metadata": {"original_chunk_id": "c3c1a303-20c6-4f43-b071-38ab1db3a9e8", "position": 16, "token_count": 1055, "workshop_id": "WS3", "speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 17"}, "id": "WS3_c3c1a303-20c6-4f43-b071-38ab1db3a9e8", "relevance": 1.0}, {"text": "And how we really need to think about building software that's powered by machine learning and Llms which share a lot of similarities, as Stefan pointed out. But instead of you know, writing some specs. 47 hugo bowne-anderson: building, then deploying, and then maybe working on v. 1.2 in a month, or something like that. What we really need to do, due to the non-deterministic nature, the flip-floppy nature and the nature of Llm. Powered software that it brings in a lot of data from the real world. And we don't know how it will behave. 48 hugo bowne-anderson: We need to build quickly and then deploy and then monitor and evaluate and iterate rapidly on this cycle. So that's what this course is all about, and we'll see on the building side. Prompt engineering will be important. Embeddings will be important. Fine tuning business logic. When deploying. We want to think through unit tests and versioning and continuous integration. And those things are some of the very exciting things that Stefan will be talking about in his workshop next week. 49 hugo bowne-anderson: Want to think about tracing and general observability when monitoring and when evaluating. We want to think about not only monitoring how good individual Llm. Calls are, but how they relate to the business metrics that you're you're trying to meet as well. 50 hugo bowne-anderson: So what this course is. It's a practical 1st principles. Approach to building Llm. Powered apps focusing on workflows, iteration and hands-on development. Okay, it's also a space to build production grade systems that scale beyond proof of concepts and demos. It's a deep dive into the non-deterministic nature of these systems, observability, testing, and evaluation, which are the backbones of AI systems. There is a world out there 51 hugo bowne-anderson: called Linkedin, among other things which wants you to believe that the backbone of robust AI systems is the latest and hottest models, multi-agentic frameworks, infinite rag systems conversations about is rag dead because of 10 trillion token, context windows. And this type of stuff 52 hugo bowne-anderson: all of that stuff aside. I mean, there are places for that. That is not what this course is about. Of course we're going to introduce you to a lot of those tools. But this is about the workflow and the non-determinism, the observability, testing and evaluation. It's also an opportunity to learn from practitioners not only the ones who are here and will be here the entire time, but we have a lot of guest guest lectures from people. 53 hugo bowne-anderson: from people I admire deeply. I mean, we invited them because we think they're they're at the top top of the game. People like Hamil Hussein from parlance labs, Eric, Ma. From Moderna, Ravin Kumar from Google Deepmind, who I ravin's talk last time on because he he was on the team that built notebook. Lm. And mariner at Google and Deepmind. Just getting insight into that Ines Montani from from Spacey, who's an old friend, and 54 hugo bowne-anderson: her work's just just so so wonderful. And that's kind of a short list of of a lot of the wonderful guest lectures and talks that we have. So 55 hugo bowne-anderson: I hinted at this. But what this course isn't, it is not a deep dive into every Llm. Tool or trend. Of course we'll be doing. We'll be doing a bunch on agents actually, in the second half of the course, and rag and embeddings and more. But the focus is on workflows, not tools, I honestly think, and I'm happy to chat about those and message about those to be honest. But you will have enough resources on that. What we're teaching is what we've really noticed is a huge gap 56 hugo bowne-anderson: in education around this stuff. This is not a plug and play AI recipe book. They don't even exist. If someone tries to tell you that or sell you that I'd love to hear it. What you'll learn is how to iterate, debug and adapt not just follow step by step. Templates. Okay, once again, this is not some, you know. Hyper, scaled, load balance, multi agent, guide. This course is about getting you moving, not solving every scaling challenge, but really approaching the workflow 57 hugo bowne-anderson: of how to get things in production and then keep iterating on them. 58 hugo bowne-anderson: So I'm gonna put this. This is incredibly important. This is an onboarding form that actually 25 of you have completed already. And I'm so happy with with that number so far.", "metadata": {"workshop_id": "WS1", "original_chunk_id": "fa9ad653-c805-4e70-ab36-c785417b4b44", "timestamp": "Chunk 4", "source": "sentence_chunking", "speaker": "Unknown", "position": 3, "token_count": 1040}, "id": "WS1_fa9ad653-c805-4e70-ab36-c785417b4b44", "relevance": 1.0}, {"text": "And how we really need to think about building software that's powered by machine learning and Llms which share a lot of similarities, as Stefan pointed out. But instead of you know, writing some specs. 47 hugo bowne-anderson: building, then deploying, and then maybe working on v. 1.2 in a month, or something like that. What we really need to do, due to the non-deterministic nature, the flip-floppy nature and the nature of Llm. Powered software that it brings in a lot of data from the real world. And we don't know how it will behave. 48 hugo bowne-anderson: We need to build quickly and then deploy and then monitor and evaluate and iterate rapidly on this cycle. So that's what this course is all about, and we'll see on the building side. Prompt engineering will be important. Embeddings will be important. Fine tuning business logic. When deploying. We want to think through unit tests and versioning and continuous integration. And those things are some of the very exciting things that Stefan will be talking about in his workshop next week. 49 hugo bowne-anderson: Want to think about tracing and general observability when monitoring and when evaluating. We want to think about not only monitoring how good individual Llm. Calls are, but how they relate to the business metrics that you're you're trying to meet as well. 50 hugo bowne-anderson: So what this course is. It's a practical 1st principles. Approach to building Llm. Powered apps focusing on workflows, iteration and hands-on development. Okay, it's also a space to build production grade systems that scale beyond proof of concepts and demos. It's a deep dive into the non-deterministic nature of these systems, observability, testing, and evaluation, which are the backbones of AI systems. There is a world out there 51 hugo bowne-anderson: called Linkedin, among other things which wants you to believe that the backbone of robust AI systems is the latest and hottest models, multi-agentic frameworks, infinite rag systems conversations about is rag dead because of 10 trillion token, context windows. And this type of stuff 52 hugo bowne-anderson: all of that stuff aside. I mean, there are places for that. That is not what this course is about. Of course we're going to introduce you to a lot of those tools. But this is about the workflow and the non-determinism, the observability, testing and evaluation. It's also an opportunity to learn from practitioners not only the ones who are here and will be here the entire time, but we have a lot of guest guest lectures from people. 53 hugo bowne-anderson: from people I admire deeply. I mean, we invited them because we think they're they're at the top top of the game. People like Hamil Hussein from parlance labs, Eric, Ma. From Moderna, Ravin Kumar from Google Deepmind, who I ravin's talk last time on because he he was on the team that built notebook. Lm. And mariner at Google and Deepmind. Just getting insight into that Ines Montani from from Spacey, who's an old friend, and 54 hugo bowne-anderson: her work's just just so so wonderful. And that's kind of a short list of of a lot of the wonderful guest lectures and talks that we have. So 55 hugo bowne-anderson: I hinted at this. But what this course isn't, it is not a deep dive into every Llm. Tool or trend. Of course we'll be doing. We'll be doing a bunch on agents actually, in the second half of the course, and rag and embeddings and more. But the focus is on workflows, not tools, I honestly think, and I'm happy to chat about those and message about those to be honest. But you will have enough resources on that. What we're teaching is what we've really noticed is a huge gap 56 hugo bowne-anderson: in education around this stuff. This is not a plug and play AI recipe book. They don't even exist. If someone tries to tell you that or sell you that I'd love to hear it. What you'll learn is how to iterate, debug and adapt not just follow step by step. Templates. Okay, once again, this is not some, you know. Hyper, scaled, load balance, multi agent, guide. This course is about getting you moving, not solving every scaling challenge, but really approaching the workflow 57 hugo bowne-anderson: of how to get things in production and then keep iterating on them. 58 hugo bowne-anderson: So I'm gonna put this. This is incredibly important. This is an onboarding form that actually 25 of you have completed already. And I'm so happy with with that number so far.", "metadata": {"workshop_id": "WS1", "speaker": "Unknown", "source": "sentence_chunking", "token_count": 1040, "original_chunk_id": "daf7c4dc-e364-4546-853e-48f32a7e6cd2", "position": 3, "timestamp": "Chunk 4"}, "id": "WS1_daf7c4dc-e364-4546-853e-48f32a7e6cd2", "relevance": 1.0}, {"text": "And so that's empty. But what I'm going to do is put the slides there, put the recording there. Maybe put some transcripts there or some takeaways, and I'd actually 94 hugo bowne-anderson: welcome all of you. If you want to do something with the Transcript in Chat Gpt, or whatever it is, please do feel free to contribute, and I'm I'm not linking to the notion page for people to contribute, just because collaborative editing on notion is not my favorite thing to do. But if you have suggestions or want to send stuff to me just just ping me and I'd love to to add things or figure it out. 95 hugo bowne-anderson: We've got our compute credits. 96 hugo bowne-anderson: I I wonder if I put 97 hugo bowne-anderson: yes, I did put the shared commitments there, so we'll go through that in a second, and we'll keep keep adding adding things here. And actually, one thing which I, some of you have asked about are the guest lectures from cohort one, and I've created a little guest lecture, Archive and there's there's actually one or 2 missing here, including William Horton, who gave a wonderful talk on, on thinking through when to launch, and how how to think about 98 hugo bowne-anderson: what metrics you want to measure before launch, and how sometimes that can be challenging, and how you really need to iterate on that. But these are all talks from the previous ones. And if you click on 99 hugo bowne-anderson: the page, let's click on Nathan's. 100 hugo bowne-anderson: Yeah, we have an AI generated summary. We have a Youtube video. I'll put the slides in soon. Takeaways AI generated, and and so on. So in all honesty, this is the 1st time we've created something like this. So feedback, incredibly incredibly welcome. And by this I mean this type of document that links to different resources and Youtube videos and and that type of stuff. I'm really excited to have it grow with with all of you as well. 101 hugo bowne-anderson: I 102 hugo bowne-anderson: I think that is pretty much it. Just before getting to 103 hugo bowne-anderson: content. Now, Williams, talk isn't like Williams. Talk is actually 104 hugo bowne-anderson: You remember. That was, that was a 2\u00a0h session that turned into a 4\u00a0h session and zoom. That also happens. By the way, and I will apologize for that. Y'all are not in like required to stick around, but sometimes we 105 hugo bowne-anderson: really get and get into it. But zoom cut. Cut up that video so so many times that I'm needing to stitch certain things back together. So a little bit about what I call shared commitments. I don't want to get to like consultancy or whatever, but I do. I do just want to make clear you all. Do you. Right. For sure. We we want you here to bring everything that you are. But I do want to say this course does work best when everyone's engaged and present, so, if possible. 106 hugo bowne-anderson: keep your camera on. Helps us teach better when we can see who we're working with. Keep your mic off unless you're speaking. 107 hugo bowne-anderson: Ask questions early and often. We love participation. On top of that. 108 hugo bowne-anderson: It's very rare that questions in a course like this are uncorrelated. What I mean by that is, if you have a question almost guaranteed. Several other people have the same, if not a related question. So you're doing everyone a favor and help each other out on discord as well. You're building with awesome peers. I just do want to give a shout out to 109 hugo bowne-anderson: to Greg Gandenberger, did I pronounce that correctly, Greg, close enough. 110 greg: You got it. Yep. 111 hugo bowne-anderson: Awesome who's already been helping out and getting people fixed up with with bugs in my end example, which are invisible bugs. In fact, among other things, Greg has already issued 2 pull requests to the repository we'll get to, and that is so cool, so please do as much as time permits, but engage with each other and and help each other out. So 112 hugo bowne-anderson: really, really fantastic to have you all here. And 113 hugo bowne-anderson: wow. This slide still scares me, but it's it's real, it's it's real pain.", "metadata": {"timestamp": "Chunk 7", "token_count": 1032, "original_chunk_id": "cd24552f-c047-4a87-8f03-0216ada15b8e", "position": 6, "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS1"}, "id": "WS1_cd24552f-c047-4a87-8f03-0216ada15b8e", "relevance": 1.0}, {"text": "And so that's empty. But what I'm going to do is put the slides there, put the recording there. Maybe put some transcripts there or some takeaways, and I'd actually 94 hugo bowne-anderson: welcome all of you. If you want to do something with the Transcript in Chat Gpt, or whatever it is, please do feel free to contribute, and I'm I'm not linking to the notion page for people to contribute, just because collaborative editing on notion is not my favorite thing to do. But if you have suggestions or want to send stuff to me just just ping me and I'd love to to add things or figure it out. 95 hugo bowne-anderson: We've got our compute credits. 96 hugo bowne-anderson: I I wonder if I put 97 hugo bowne-anderson: yes, I did put the shared commitments there, so we'll go through that in a second, and we'll keep keep adding adding things here. And actually, one thing which I, some of you have asked about are the guest lectures from cohort one, and I've created a little guest lecture, Archive and there's there's actually one or 2 missing here, including William Horton, who gave a wonderful talk on, on thinking through when to launch, and how how to think about 98 hugo bowne-anderson: what metrics you want to measure before launch, and how sometimes that can be challenging, and how you really need to iterate on that. But these are all talks from the previous ones. And if you click on 99 hugo bowne-anderson: the page, let's click on Nathan's. 100 hugo bowne-anderson: Yeah, we have an AI generated summary. We have a Youtube video. I'll put the slides in soon. Takeaways AI generated, and and so on. So in all honesty, this is the 1st time we've created something like this. So feedback, incredibly incredibly welcome. And by this I mean this type of document that links to different resources and Youtube videos and and that type of stuff. I'm really excited to have it grow with with all of you as well. 101 hugo bowne-anderson: I 102 hugo bowne-anderson: I think that is pretty much it. Just before getting to 103 hugo bowne-anderson: content. Now, Williams, talk isn't like Williams. Talk is actually 104 hugo bowne-anderson: You remember. That was, that was a 2\u00a0h session that turned into a 4\u00a0h session and zoom. That also happens. By the way, and I will apologize for that. Y'all are not in like required to stick around, but sometimes we 105 hugo bowne-anderson: really get and get into it. But zoom cut. Cut up that video so so many times that I'm needing to stitch certain things back together. So a little bit about what I call shared commitments. I don't want to get to like consultancy or whatever, but I do. I do just want to make clear you all. Do you. Right. For sure. We we want you here to bring everything that you are. But I do want to say this course does work best when everyone's engaged and present, so, if possible. 106 hugo bowne-anderson: keep your camera on. Helps us teach better when we can see who we're working with. Keep your mic off unless you're speaking. 107 hugo bowne-anderson: Ask questions early and often. We love participation. On top of that. 108 hugo bowne-anderson: It's very rare that questions in a course like this are uncorrelated. What I mean by that is, if you have a question almost guaranteed. Several other people have the same, if not a related question. So you're doing everyone a favor and help each other out on discord as well. You're building with awesome peers. I just do want to give a shout out to 109 hugo bowne-anderson: to Greg Gandenberger, did I pronounce that correctly, Greg, close enough. 110 greg: You got it. Yep. 111 hugo bowne-anderson: Awesome who's already been helping out and getting people fixed up with with bugs in my end example, which are invisible bugs. In fact, among other things, Greg has already issued 2 pull requests to the repository we'll get to, and that is so cool, so please do as much as time permits, but engage with each other and and help each other out. So 112 hugo bowne-anderson: really, really fantastic to have you all here. And 113 hugo bowne-anderson: wow. This slide still scares me, but it's it's real, it's it's real pain.", "metadata": {"workshop_id": "WS1", "timestamp": "Chunk 7", "position": 6, "source": "sentence_chunking", "original_chunk_id": "111da23e-e35a-44f8-9213-5a5c03d93dcb", "speaker": "Unknown", "token_count": 1032}, "id": "WS1_111da23e-e35a-44f8-9213-5a5c03d93dcb", "relevance": 1.0}]}}{"question": "Did anyone talk about builders in residence?", "response": "Yes, the workshop transcript mentions \"builders in residence\" several times. These individuals are highlighted for their generosity in sharing time and expertise with participants. Nathan, one of the builders in residence, is mentioned in the context of scheduling Builders Club sessions and ensuring attendance. \n\nAdditionally, William Horton, another builder in residence, is introduced towards the end of the transcript, where he is set to discuss the concept of \"production\" in the upcoming sessions. The builders in residence play a crucial role in facilitating discussions and providing insights based on their experiences in the field.\n\nIf you're interested in engaging with the builders in residence, consider attending the Builders Club sessions or participating in discussions on Discord, where they are available for tailored feedback and support.", "context_info": {"num_chunks": 10, "context_tokens": 10419, "completion_tokens": 145, "embedding_tokens": 15360, "workshops_used": ["WS1", "WS3", "WS5", "WS4"], "chunks": [{"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"workshop_id": "WS3", "speaker": "Unknown", "token_count": 1068, "timestamp": "Chunk 2", "original_chunk_id": "5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "source": "sentence_chunking", "position": 1}, "id": "WS3_5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "relevance": 1.0}, {"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"token_count": 1068, "timestamp": "Chunk 2", "source": "sentence_chunking", "position": 1, "original_chunk_id": "8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "workshop_id": "WS3", "speaker": "Unknown"}, "id": "WS3_8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "relevance": 1.0}, {"text": "We've got 448 hugo bowne-anderson: a folder of lots of different Linkedin profiles that you can use for this as well, and thank you to Stefan for that. It's when I thought you had to do all this manually. 449 hugo bowne-anderson: And he he showed me how to do that. 450 hugo bowne-anderson: So incorporate some retrieval processes into your app. Okay. And I would encourage you to think about exploring advanced retrieval methods like semantic search or chunking strategies. And there are so many ideas that I thought to create a notebook around, different things to do, and different ways to 451 hugo bowne-anderson: think about this and so once again generating embeddings, thinking about different. And we haven't talked about chunking yet, but when you have very large things you want to do them in different sized chunks, and we can can talk about that moving forward. I would also encourage you to think about. 452 hugo bowne-anderson: Do you want your app to. 453 hugo bowne-anderson: you know, reach out to people based on it. Or do you want to match profiles to particular jobs. Do you have a job listing that you want to match profiles to. Or do you have profiles that you want to match a job listing to using all of these different tools. The other thing. 454 hugo bowne-anderson: at the risk of biting off more than anyone can chew, but it is highly relevant. 455 hugo bowne-anderson: I'll once again post this 456 hugo bowne-anderson: in. I'm very excited because this app that I built to query our workshop transcripts. You may recall last time I showed you the basic Q&A. Which 457 hugo bowne-anderson: essentially says you're a helpful assistant. Answer questions based only on the provided context. And I wasn't able yet to show you the simple rag version which is the one that I stood up on modal and served to you all. But I encourage those interested to have a look through this this code, and so I've got my embedding functions. I've got my system prompt. 458 hugo bowne-anderson: I'm doing a bit of counting tokens and figuring out cost and that type of stuff. Then a bit of chunking, working with metadata, extracting the workshop material, splitting into paragraphs, bit of preprocessing, creating the chunks, and so on. Then some vector storage stuff. So using chroma and vector dB, adding the chunks to the collection. 459 hugo bowne-anderson: then querying the collection, then retrieval functions. And I, if you're interested, please do look through this code very happy to chat about it very happy to see pull requests about it as well, and then some logging. So. But I just do want to say, in 781 lines of code, I was really able to to build something which, at least as a 1st approximation, stands up pretty well and 460 hugo bowne-anderson: gives me a bunch of logs and that type of stuff and allows you all to interact with it. So once again, I'll include this all in ideas for project and homework stuff. 461 hugo bowne-anderson: But it would be exciting if anyone wants to work on this app with me. That would be fun. The other thing, I I suppose, worth mentioning is, as you all know 462 hugo bowne-anderson: it. Currently, we're only working with workshop one from this cohort, and I want to start adding more workshops to it as well. So that's enough out of me. I'm very excited to 463 hugo bowne-anderson: introduce William, one of our builders in residence to the proverbial virtual stage, to talk about what is production. This, this is one of the things I love about this course as well. Is, you know, we have conversations and new material and new ways of speaking with each other. Come, come out of it. I met William earlier this year. When he took 464 hugo bowne-anderson: the 1st cohort of this course got along like a house on fire. I learned a lot from him, and you know all the stuff he's put into production. It included health I've been very excited to learn about, and everything he can say. 465 hugo bowne-anderson: I also just want to congratulate William on his new position at Maven. Health. Is that. Is that correct. William. 466 William Horton: Maven maven clinic yep. 467 hugo bowne-anderson: Maven Clinic. I'm I'm sorry. And I don't know if Nathan has conveyed this to you yet, but him and his wife have been used maven clinic for some time, and and love it so. 468 Nathan Danielsen: Yeah, I'm I'm just gonna chime in. We're super promoters at my work.", "metadata": {"original_chunk_id": "9f8ae0f7-6303-4dd8-be08-90ffec099e9d", "speaker": "Unknown", "workshop_id": "WS5", "position": 20, "source": "sentence_chunking", "timestamp": "Chunk 21", "token_count": 1064}, "id": "WS5_9f8ae0f7-6303-4dd8-be08-90ffec099e9d", "relevance": 1.0}, {"text": "We've got 448 hugo bowne-anderson: a folder of lots of different Linkedin profiles that you can use for this as well, and thank you to Stefan for that. It's when I thought you had to do all this manually. 449 hugo bowne-anderson: And he he showed me how to do that. 450 hugo bowne-anderson: So incorporate some retrieval processes into your app. Okay. And I would encourage you to think about exploring advanced retrieval methods like semantic search or chunking strategies. And there are so many ideas that I thought to create a notebook around, different things to do, and different ways to 451 hugo bowne-anderson: think about this and so once again generating embeddings, thinking about different. And we haven't talked about chunking yet, but when you have very large things you want to do them in different sized chunks, and we can can talk about that moving forward. I would also encourage you to think about. 452 hugo bowne-anderson: Do you want your app to. 453 hugo bowne-anderson: you know, reach out to people based on it. Or do you want to match profiles to particular jobs. Do you have a job listing that you want to match profiles to. Or do you have profiles that you want to match a job listing to using all of these different tools. The other thing. 454 hugo bowne-anderson: at the risk of biting off more than anyone can chew, but it is highly relevant. 455 hugo bowne-anderson: I'll once again post this 456 hugo bowne-anderson: in. I'm very excited because this app that I built to query our workshop transcripts. You may recall last time I showed you the basic Q&A. Which 457 hugo bowne-anderson: essentially says you're a helpful assistant. Answer questions based only on the provided context. And I wasn't able yet to show you the simple rag version which is the one that I stood up on modal and served to you all. But I encourage those interested to have a look through this this code, and so I've got my embedding functions. I've got my system prompt. 458 hugo bowne-anderson: I'm doing a bit of counting tokens and figuring out cost and that type of stuff. Then a bit of chunking, working with metadata, extracting the workshop material, splitting into paragraphs, bit of preprocessing, creating the chunks, and so on. Then some vector storage stuff. So using chroma and vector dB, adding the chunks to the collection. 459 hugo bowne-anderson: then querying the collection, then retrieval functions. And I, if you're interested, please do look through this code very happy to chat about it very happy to see pull requests about it as well, and then some logging. So. But I just do want to say, in 781 lines of code, I was really able to to build something which, at least as a 1st approximation, stands up pretty well and 460 hugo bowne-anderson: gives me a bunch of logs and that type of stuff and allows you all to interact with it. So once again, I'll include this all in ideas for project and homework stuff. 461 hugo bowne-anderson: But it would be exciting if anyone wants to work on this app with me. That would be fun. The other thing, I I suppose, worth mentioning is, as you all know 462 hugo bowne-anderson: it. Currently, we're only working with workshop one from this cohort, and I want to start adding more workshops to it as well. So that's enough out of me. I'm very excited to 463 hugo bowne-anderson: introduce William, one of our builders in residence to the proverbial virtual stage, to talk about what is production. This, this is one of the things I love about this course as well. Is, you know, we have conversations and new material and new ways of speaking with each other. Come, come out of it. I met William earlier this year. When he took 464 hugo bowne-anderson: the 1st cohort of this course got along like a house on fire. I learned a lot from him, and you know all the stuff he's put into production. It included health I've been very excited to learn about, and everything he can say. 465 hugo bowne-anderson: I also just want to congratulate William on his new position at Maven. Health. Is that. Is that correct. William. 466 William Horton: Maven maven clinic yep. 467 hugo bowne-anderson: Maven Clinic. I'm I'm sorry. And I don't know if Nathan has conveyed this to you yet, but him and his wife have been used maven clinic for some time, and and love it so. 468 Nathan Danielsen: Yeah, I'm I'm just gonna chime in. We're super promoters at my work.", "metadata": {"original_chunk_id": "6e8cf94e-8275-443e-bd2d-34ba35a70773", "token_count": 1064, "workshop_id": "WS5", "speaker": "Unknown", "position": 20, "source": "sentence_chunking", "timestamp": "Chunk 21"}, "id": "WS5_6e8cf94e-8275-443e-bd2d-34ba35a70773", "relevance": 1.0}, {"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"token_count": 1055, "original_chunk_id": "f9e33134-8ebe-4b68-90f8-9d442e7bb591", "position": 17, "speaker": "Unknown", "workshop_id": "WS4", "source": "sentence_chunking", "timestamp": "Chunk 18"}, "id": "WS4_f9e33134-8ebe-4b68-90f8-9d442e7bb591", "relevance": 1.0}, {"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"position": 17, "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "334c243a-3488-4cde-a7c9-f1f7345397fb", "workshop_id": "WS4", "timestamp": "Chunk 18", "token_count": 1055}, "id": "WS4_334c243a-3488-4cde-a7c9-f1f7345397fb", "relevance": 1.0}, {"text": "And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say. Fine tuning isn't really useful these days, cause if I I would, that sounds like something I I would say. What I mean is that most of the time 213 hugo bowne-anderson: you need. You think you need to fine tune. You can do other things such as prompt your way out of it. 214 hugo bowne-anderson: There are cases, for example. 215 hugo bowne-anderson: well, most of the cases I see are actually for vision models and these types of things. But there are cases where you want it to be really good on domain, specific expertise or previous conversations, and then fine tuning would be useful. The other use case that I like, for fine tuning is migrating to smaller models. 216 hugo bowne-anderson: So if you build something and you're using like a state of the art, relatively expensive vendor based model, and you want to migrate to a smaller open weight. Model yourself, taking some of the conversations with Gpt. Whatever, and fine tuning on those can be incredibly useful. I also love that. It seems like that's in violation of Openai's terms and services, and I love that they would have 217 hugo bowne-anderson: the attitude to to be like this is our copyright, or something like that. I find it fascinating. But yeah, I just actually haven't haven't seen a lot of uses of fine tuning that haven't been achievable without others. I would. I would welcome 218 hugo bowne-anderson: William to add anything, he said, and he's he's linked to Emmanuel's wonderful talk. Why, fine tuning is dead. 219 William Horton: Yeah, I think that I think the main thing is that you can get pretty far without having to fine tune your model. And and so a lot of both people and and companies doing this work will start by using the strongest model that's available via Api. Before they go down that road. I I also would add to the list of things, Hugo said, where people still get some 220 William Horton: value out of fine tuning. I have also seen people fine tuning their own embeddings and re rankers for retrieval specifically, is a place where I think people still find a benefit, especially because those models tend to be smaller as well. So the inference cost is is not as high, and the training is not as extensive. That's another place where I think people still do it and get maybe more value than if you were trying to do fine tuning like for the whole Llm. 221 hugo bowne-anderson: Absolutely love it. And I also just noticed in the chat Vishut went. This is probably when I spoke about proof of concept purgatory. Vishrud has faced issues when he used Mvp. Without context. In North America it's used for most valuable player totally agree, and it shows how much 222 hugo bowne-anderson: how most people I speak with at least professionally work in tech and know what know that it means. That right. 223 hugo bowne-anderson: awesome. Well, I'm just. I can't see everything in discord at the moment, but so much fantastic conversation and 224 hugo bowne-anderson: questions as as well. So if anyone. 225 hugo bowne-anderson: I'm gonna open a window right now. But if anyone doesn't have access to github, could you 226 hugo bowne-anderson: just put your handle in the workshop. One channel and I will add you almost in real time. 227 hugo bowne-anderson: And the reason for that is with Github has changed certain things so that I actually can't assign anyone else to be admin currently as it's under my personal. 228 hugo bowne-anderson: Okay, we have Vinysh. Oh, oh, I love this and I do hope that 229 hugo bowne-anderson: you're all okay with us. Just taking 2\u00a0min to 230 hugo bowne-anderson: add a few more people. Oh, this is great. 231 hugo bowne-anderson: amazing. 232 Nathan Danielsen: Well, while people are doing that, yeah. Want to comment on the the rich conversations that's happening in discord right now. And from the 1st kind of like run of this course. This is what that the conversations are. One of the highlights as well. So just calling that out. That's a lot of good stuff. A lot of smart people sharing resources there. So check out the discord for the session. 233 hugo bowne-anderson: I really appreciate that. And in fact, I know it. I mean, we we started building this right to do a court.", "metadata": {"source": "sentence_chunking", "position": 12, "timestamp": "Chunk 13", "original_chunk_id": "9a58ab61-5b20-4005-89ac-26935dae8118", "workshop_id": "WS1", "token_count": 1044, "speaker": "Unknown"}, "id": "WS1_9a58ab61-5b20-4005-89ac-26935dae8118", "relevance": 1.0}, {"text": "And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say. Fine tuning isn't really useful these days, cause if I I would, that sounds like something I I would say. What I mean is that most of the time 213 hugo bowne-anderson: you need. You think you need to fine tune. You can do other things such as prompt your way out of it. 214 hugo bowne-anderson: There are cases, for example. 215 hugo bowne-anderson: well, most of the cases I see are actually for vision models and these types of things. But there are cases where you want it to be really good on domain, specific expertise or previous conversations, and then fine tuning would be useful. The other use case that I like, for fine tuning is migrating to smaller models. 216 hugo bowne-anderson: So if you build something and you're using like a state of the art, relatively expensive vendor based model, and you want to migrate to a smaller open weight. Model yourself, taking some of the conversations with Gpt. Whatever, and fine tuning on those can be incredibly useful. I also love that. It seems like that's in violation of Openai's terms and services, and I love that they would have 217 hugo bowne-anderson: the attitude to to be like this is our copyright, or something like that. I find it fascinating. But yeah, I just actually haven't haven't seen a lot of uses of fine tuning that haven't been achievable without others. I would. I would welcome 218 hugo bowne-anderson: William to add anything, he said, and he's he's linked to Emmanuel's wonderful talk. Why, fine tuning is dead. 219 William Horton: Yeah, I think that I think the main thing is that you can get pretty far without having to fine tune your model. And and so a lot of both people and and companies doing this work will start by using the strongest model that's available via Api. Before they go down that road. I I also would add to the list of things, Hugo said, where people still get some 220 William Horton: value out of fine tuning. I have also seen people fine tuning their own embeddings and re rankers for retrieval specifically, is a place where I think people still find a benefit, especially because those models tend to be smaller as well. So the inference cost is is not as high, and the training is not as extensive. That's another place where I think people still do it and get maybe more value than if you were trying to do fine tuning like for the whole Llm. 221 hugo bowne-anderson: Absolutely love it. And I also just noticed in the chat Vishut went. This is probably when I spoke about proof of concept purgatory. Vishrud has faced issues when he used Mvp. Without context. In North America it's used for most valuable player totally agree, and it shows how much 222 hugo bowne-anderson: how most people I speak with at least professionally work in tech and know what know that it means. That right. 223 hugo bowne-anderson: awesome. Well, I'm just. I can't see everything in discord at the moment, but so much fantastic conversation and 224 hugo bowne-anderson: questions as as well. So if anyone. 225 hugo bowne-anderson: I'm gonna open a window right now. But if anyone doesn't have access to github, could you 226 hugo bowne-anderson: just put your handle in the workshop. One channel and I will add you almost in real time. 227 hugo bowne-anderson: And the reason for that is with Github has changed certain things so that I actually can't assign anyone else to be admin currently as it's under my personal. 228 hugo bowne-anderson: Okay, we have Vinysh. Oh, oh, I love this and I do hope that 229 hugo bowne-anderson: you're all okay with us. Just taking 2\u00a0min to 230 hugo bowne-anderson: add a few more people. Oh, this is great. 231 hugo bowne-anderson: amazing. 232 Nathan Danielsen: Well, while people are doing that, yeah. Want to comment on the the rich conversations that's happening in discord right now. And from the 1st kind of like run of this course. This is what that the conversations are. One of the highlights as well. So just calling that out. That's a lot of good stuff. A lot of smart people sharing resources there. So check out the discord for the session. 233 hugo bowne-anderson: I really appreciate that. And in fact, I know it. I mean, we we started building this right to do a court.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 13", "token_count": 1044, "position": 12, "workshop_id": "WS1", "original_chunk_id": "b4168834-631b-4b73-b888-0cd8a51cec8e"}, "id": "WS1_b4168834-631b-4b73-b888-0cd8a51cec8e", "relevance": 1.0}, {"text": "It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right. So I do want to just quickly go to the next notebook to notebook I have a brief summary of this before, but to give a high level summary. Of what we're doing here. I wanted to give you an introduction to actually summarization is something that isn't that we're still working on here, right. So think about how to evaluate factual consistency in in in summarization. So in this, what we do is we load and preprocess a data set containing source text. Correct summaries and incorrect summaries. So distractor summaries, right. Then we hand label, we annotate summaries for factual consistency using pass fail and then calculate metrics like accuracy, precision, and recall. We identify common failure modes in incorrect summaries to better understand the model behavior. Then we train an LLM as a judge to evaluate summaries based on the human annotations, providing scalability for larger data sets. And then we monitor the LLM judgments over time by validating against human labeled samples to ensure alignment and consistency Okay. So that's what we do here. If you're super excited to jump in sooner, but I'll put a video in this channel, in the channel Later today. And you can see like I use Gemini to do a whole bunch of summarization, okay. And then I export it to a spreadsheet and annotate it. And then we look at pass rates and fail rates and we can see like how different prompts perform based around my human annotation. Once we have that. We can define evaluation criteria Do we want consistency, relevance, clarity. Then we prepared a few shot examples where we have a few label examples to give an LLM as a judge. Then we write our few shot prompt saying, you are an expert evaluator of tech summaries. For each summary, evaluate its quality based on these three things. And Bob's your uncle, so to speak. We can generate LLM judgments, compare them with human annotations. And then we can automate the scalable evaluation and monitor and iterate and move from there. So I know I kind of, that was a whirlwind tour through that notebook but And I appreciate that that's a lot to take in. I do want to say everything in this notebook is what we've been talking about for an hour and a half now as well and foreshadowed last week also so All of these things, this process essentially is under the rubric of evaluation-driven development. So if there's one takeaway I want you to have. It's here. It's on one of my first slides. The one takeaway. Of course, once you have one takeaway, you can't find it, right. Yes. The one takeaway is that As you build out an MVP, build out an MVE. With any minimal viable product, build out a minimum viable evaluation harness that moves beyond vibes as quickly as possible. It doesn't have to be anything super robust. And it's not that different to ML powered software, right. This process. There are some details that are, but both workflows are grounded in test sets. Iteration and failure analysis. So\u2026 um what Do I want to say i've already shown you the app I built that I'd love you all to contribute to and we'll share a repo later with respect. To that. And we already had a look at the logs together um so What I'd like you to do is update the MVP from workshop two and start building out some evaluations. Use everything that I've given you today in order to do this. And I'm really excited to see in the homework channel What you will do with this type of thing and what happens in Builders Club with respect to that. The next workshop, I'm beyond excited for. It will be Stefan. It's my favorite talk in workshop in this course except for this one. No, I'm kidding. But I do love this workshop and we have A lot of guest speakers on Thursday, we've got Innes Montani from Spacey, Pavitra from Quansite. Vincent from Marimo and Catherine Jarmel from KJ Amistan talking a lot about all the work they do with LLMs from human in the loop development to building applications with open source tools. And a lot of Pavitra's work at QuanSight. She works with Travis Oliphant, who created NumPy back in the day, right. But there, you know, I've said we've had like the rug pulled out from under us with respect to open source tooling and they're working on a lot of interesting tooling for building AI powered applications using open source Vinsome's just a An absolute joy who has so many cool tricks and tips And\u2026 Catherine does a lot of work on adversarial threats, so very excited for that. And I'll be getting up at 4.30 a.m. Local time for Innices Talk. I'm very excited to do that, actually.", "metadata": {"speaker": "Unknown", "workshop_id": "WS3", "token_count": 1055, "position": 16, "original_chunk_id": "e9c0557a-5b5f-45dd-af0c-dde43e745e04", "timestamp": "Chunk 17", "source": "sentence_chunking"}, "id": "WS3_e9c0557a-5b5f-45dd-af0c-dde43e745e04", "relevance": 1.0}, {"text": "It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right. So I do want to just quickly go to the next notebook to notebook I have a brief summary of this before, but to give a high level summary. Of what we're doing here. I wanted to give you an introduction to actually summarization is something that isn't that we're still working on here, right. So think about how to evaluate factual consistency in in in summarization. So in this, what we do is we load and preprocess a data set containing source text. Correct summaries and incorrect summaries. So distractor summaries, right. Then we hand label, we annotate summaries for factual consistency using pass fail and then calculate metrics like accuracy, precision, and recall. We identify common failure modes in incorrect summaries to better understand the model behavior. Then we train an LLM as a judge to evaluate summaries based on the human annotations, providing scalability for larger data sets. And then we monitor the LLM judgments over time by validating against human labeled samples to ensure alignment and consistency Okay. So that's what we do here. If you're super excited to jump in sooner, but I'll put a video in this channel, in the channel Later today. And you can see like I use Gemini to do a whole bunch of summarization, okay. And then I export it to a spreadsheet and annotate it. And then we look at pass rates and fail rates and we can see like how different prompts perform based around my human annotation. Once we have that. We can define evaluation criteria Do we want consistency, relevance, clarity. Then we prepared a few shot examples where we have a few label examples to give an LLM as a judge. Then we write our few shot prompt saying, you are an expert evaluator of tech summaries. For each summary, evaluate its quality based on these three things. And Bob's your uncle, so to speak. We can generate LLM judgments, compare them with human annotations. And then we can automate the scalable evaluation and monitor and iterate and move from there. So I know I kind of, that was a whirlwind tour through that notebook but And I appreciate that that's a lot to take in. I do want to say everything in this notebook is what we've been talking about for an hour and a half now as well and foreshadowed last week also so All of these things, this process essentially is under the rubric of evaluation-driven development. So if there's one takeaway I want you to have. It's here. It's on one of my first slides. The one takeaway. Of course, once you have one takeaway, you can't find it, right. Yes. The one takeaway is that As you build out an MVP, build out an MVE. With any minimal viable product, build out a minimum viable evaluation harness that moves beyond vibes as quickly as possible. It doesn't have to be anything super robust. And it's not that different to ML powered software, right. This process. There are some details that are, but both workflows are grounded in test sets. Iteration and failure analysis. So\u2026 um what Do I want to say i've already shown you the app I built that I'd love you all to contribute to and we'll share a repo later with respect. To that. And we already had a look at the logs together um so What I'd like you to do is update the MVP from workshop two and start building out some evaluations. Use everything that I've given you today in order to do this. And I'm really excited to see in the homework channel What you will do with this type of thing and what happens in Builders Club with respect to that. The next workshop, I'm beyond excited for. It will be Stefan. It's my favorite talk in workshop in this course except for this one. No, I'm kidding. But I do love this workshop and we have A lot of guest speakers on Thursday, we've got Innes Montani from Spacey, Pavitra from Quansite. Vincent from Marimo and Catherine Jarmel from KJ Amistan talking a lot about all the work they do with LLMs from human in the loop development to building applications with open source tools. And a lot of Pavitra's work at QuanSight. She works with Travis Oliphant, who created NumPy back in the day, right. But there, you know, I've said we've had like the rug pulled out from under us with respect to open source tooling and they're working on a lot of interesting tooling for building AI powered applications using open source Vinsome's just a An absolute joy who has so many cool tricks and tips And\u2026 Catherine does a lot of work on adversarial threats, so very excited for that. And I'll be getting up at 4.30 a.m. Local time for Innices Talk. I'm very excited to do that, actually.", "metadata": {"original_chunk_id": "c3c1a303-20c6-4f43-b071-38ab1db3a9e8", "timestamp": "Chunk 17", "source": "sentence_chunking", "workshop_id": "WS3", "speaker": "Unknown", "position": 16, "token_count": 1055}, "id": "WS3_c3c1a303-20c6-4f43-b071-38ab1db3a9e8", "relevance": 1.0}]}}{"question": "Does anybody mention the Builders in Residence Program?", "response": "Yes, the Builders in Residence Program is mentioned multiple times throughout the workshop transcripts. It highlights the involvement of individuals who are part of this program, such as Nathan and William Horton, who are contributing their expertise and time to support participants. \n\nThe Builders Club sessions are specifically designed to provide tailored feedback and foster collaboration among participants. The program aims to create a supportive environment where attendees can engage with experienced builders, share ideas, and receive guidance on their projects.\n\nIf you're interested in participating or learning more about the Builders in Residence Program, consider attending the Builders Club sessions, where you can interact directly with these builders and gain insights into their experiences and knowledge.", "context_info": {"num_chunks": 10, "context_tokens": 10420, "completion_tokens": 130, "embedding_tokens": 15360, "workshops_used": ["WS1", "WS3", "WS5", "WS4"], "chunks": [{"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"timestamp": "Chunk 2", "speaker": "Unknown", "position": 1, "original_chunk_id": "5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "token_count": 1068, "workshop_id": "WS3", "source": "sentence_chunking"}, "id": "WS3_5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "relevance": 1.0}, {"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"original_chunk_id": "8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "position": 1, "speaker": "Unknown", "timestamp": "Chunk 2", "token_count": 1068, "workshop_id": "WS3", "source": "sentence_chunking"}, "id": "WS3_8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "relevance": 1.0}, {"text": "We've got 448 hugo bowne-anderson: a folder of lots of different Linkedin profiles that you can use for this as well, and thank you to Stefan for that. It's when I thought you had to do all this manually. 449 hugo bowne-anderson: And he he showed me how to do that. 450 hugo bowne-anderson: So incorporate some retrieval processes into your app. Okay. And I would encourage you to think about exploring advanced retrieval methods like semantic search or chunking strategies. And there are so many ideas that I thought to create a notebook around, different things to do, and different ways to 451 hugo bowne-anderson: think about this and so once again generating embeddings, thinking about different. And we haven't talked about chunking yet, but when you have very large things you want to do them in different sized chunks, and we can can talk about that moving forward. I would also encourage you to think about. 452 hugo bowne-anderson: Do you want your app to. 453 hugo bowne-anderson: you know, reach out to people based on it. Or do you want to match profiles to particular jobs. Do you have a job listing that you want to match profiles to. Or do you have profiles that you want to match a job listing to using all of these different tools. The other thing. 454 hugo bowne-anderson: at the risk of biting off more than anyone can chew, but it is highly relevant. 455 hugo bowne-anderson: I'll once again post this 456 hugo bowne-anderson: in. I'm very excited because this app that I built to query our workshop transcripts. You may recall last time I showed you the basic Q&A. Which 457 hugo bowne-anderson: essentially says you're a helpful assistant. Answer questions based only on the provided context. And I wasn't able yet to show you the simple rag version which is the one that I stood up on modal and served to you all. But I encourage those interested to have a look through this this code, and so I've got my embedding functions. I've got my system prompt. 458 hugo bowne-anderson: I'm doing a bit of counting tokens and figuring out cost and that type of stuff. Then a bit of chunking, working with metadata, extracting the workshop material, splitting into paragraphs, bit of preprocessing, creating the chunks, and so on. Then some vector storage stuff. So using chroma and vector dB, adding the chunks to the collection. 459 hugo bowne-anderson: then querying the collection, then retrieval functions. And I, if you're interested, please do look through this code very happy to chat about it very happy to see pull requests about it as well, and then some logging. So. But I just do want to say, in 781 lines of code, I was really able to to build something which, at least as a 1st approximation, stands up pretty well and 460 hugo bowne-anderson: gives me a bunch of logs and that type of stuff and allows you all to interact with it. So once again, I'll include this all in ideas for project and homework stuff. 461 hugo bowne-anderson: But it would be exciting if anyone wants to work on this app with me. That would be fun. The other thing, I I suppose, worth mentioning is, as you all know 462 hugo bowne-anderson: it. Currently, we're only working with workshop one from this cohort, and I want to start adding more workshops to it as well. So that's enough out of me. I'm very excited to 463 hugo bowne-anderson: introduce William, one of our builders in residence to the proverbial virtual stage, to talk about what is production. This, this is one of the things I love about this course as well. Is, you know, we have conversations and new material and new ways of speaking with each other. Come, come out of it. I met William earlier this year. When he took 464 hugo bowne-anderson: the 1st cohort of this course got along like a house on fire. I learned a lot from him, and you know all the stuff he's put into production. It included health I've been very excited to learn about, and everything he can say. 465 hugo bowne-anderson: I also just want to congratulate William on his new position at Maven. Health. Is that. Is that correct. William. 466 William Horton: Maven maven clinic yep. 467 hugo bowne-anderson: Maven Clinic. I'm I'm sorry. And I don't know if Nathan has conveyed this to you yet, but him and his wife have been used maven clinic for some time, and and love it so. 468 Nathan Danielsen: Yeah, I'm I'm just gonna chime in. We're super promoters at my work.", "metadata": {"original_chunk_id": "9f8ae0f7-6303-4dd8-be08-90ffec099e9d", "source": "sentence_chunking", "speaker": "Unknown", "position": 20, "workshop_id": "WS5", "token_count": 1064, "timestamp": "Chunk 21"}, "id": "WS5_9f8ae0f7-6303-4dd8-be08-90ffec099e9d", "relevance": 1.0}, {"text": "We've got 448 hugo bowne-anderson: a folder of lots of different Linkedin profiles that you can use for this as well, and thank you to Stefan for that. It's when I thought you had to do all this manually. 449 hugo bowne-anderson: And he he showed me how to do that. 450 hugo bowne-anderson: So incorporate some retrieval processes into your app. Okay. And I would encourage you to think about exploring advanced retrieval methods like semantic search or chunking strategies. And there are so many ideas that I thought to create a notebook around, different things to do, and different ways to 451 hugo bowne-anderson: think about this and so once again generating embeddings, thinking about different. And we haven't talked about chunking yet, but when you have very large things you want to do them in different sized chunks, and we can can talk about that moving forward. I would also encourage you to think about. 452 hugo bowne-anderson: Do you want your app to. 453 hugo bowne-anderson: you know, reach out to people based on it. Or do you want to match profiles to particular jobs. Do you have a job listing that you want to match profiles to. Or do you have profiles that you want to match a job listing to using all of these different tools. The other thing. 454 hugo bowne-anderson: at the risk of biting off more than anyone can chew, but it is highly relevant. 455 hugo bowne-anderson: I'll once again post this 456 hugo bowne-anderson: in. I'm very excited because this app that I built to query our workshop transcripts. You may recall last time I showed you the basic Q&A. Which 457 hugo bowne-anderson: essentially says you're a helpful assistant. Answer questions based only on the provided context. And I wasn't able yet to show you the simple rag version which is the one that I stood up on modal and served to you all. But I encourage those interested to have a look through this this code, and so I've got my embedding functions. I've got my system prompt. 458 hugo bowne-anderson: I'm doing a bit of counting tokens and figuring out cost and that type of stuff. Then a bit of chunking, working with metadata, extracting the workshop material, splitting into paragraphs, bit of preprocessing, creating the chunks, and so on. Then some vector storage stuff. So using chroma and vector dB, adding the chunks to the collection. 459 hugo bowne-anderson: then querying the collection, then retrieval functions. And I, if you're interested, please do look through this code very happy to chat about it very happy to see pull requests about it as well, and then some logging. So. But I just do want to say, in 781 lines of code, I was really able to to build something which, at least as a 1st approximation, stands up pretty well and 460 hugo bowne-anderson: gives me a bunch of logs and that type of stuff and allows you all to interact with it. So once again, I'll include this all in ideas for project and homework stuff. 461 hugo bowne-anderson: But it would be exciting if anyone wants to work on this app with me. That would be fun. The other thing, I I suppose, worth mentioning is, as you all know 462 hugo bowne-anderson: it. Currently, we're only working with workshop one from this cohort, and I want to start adding more workshops to it as well. So that's enough out of me. I'm very excited to 463 hugo bowne-anderson: introduce William, one of our builders in residence to the proverbial virtual stage, to talk about what is production. This, this is one of the things I love about this course as well. Is, you know, we have conversations and new material and new ways of speaking with each other. Come, come out of it. I met William earlier this year. When he took 464 hugo bowne-anderson: the 1st cohort of this course got along like a house on fire. I learned a lot from him, and you know all the stuff he's put into production. It included health I've been very excited to learn about, and everything he can say. 465 hugo bowne-anderson: I also just want to congratulate William on his new position at Maven. Health. Is that. Is that correct. William. 466 William Horton: Maven maven clinic yep. 467 hugo bowne-anderson: Maven Clinic. I'm I'm sorry. And I don't know if Nathan has conveyed this to you yet, but him and his wife have been used maven clinic for some time, and and love it so. 468 Nathan Danielsen: Yeah, I'm I'm just gonna chime in. We're super promoters at my work.", "metadata": {"position": 20, "speaker": "Unknown", "token_count": 1064, "source": "sentence_chunking", "timestamp": "Chunk 21", "workshop_id": "WS5", "original_chunk_id": "6e8cf94e-8275-443e-bd2d-34ba35a70773"}, "id": "WS5_6e8cf94e-8275-443e-bd2d-34ba35a70773", "relevance": 1.0}, {"text": "It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right. So I do want to just quickly go to the next notebook to notebook I have a brief summary of this before, but to give a high level summary. Of what we're doing here. I wanted to give you an introduction to actually summarization is something that isn't that we're still working on here, right. So think about how to evaluate factual consistency in in in summarization. So in this, what we do is we load and preprocess a data set containing source text. Correct summaries and incorrect summaries. So distractor summaries, right. Then we hand label, we annotate summaries for factual consistency using pass fail and then calculate metrics like accuracy, precision, and recall. We identify common failure modes in incorrect summaries to better understand the model behavior. Then we train an LLM as a judge to evaluate summaries based on the human annotations, providing scalability for larger data sets. And then we monitor the LLM judgments over time by validating against human labeled samples to ensure alignment and consistency Okay. So that's what we do here. If you're super excited to jump in sooner, but I'll put a video in this channel, in the channel Later today. And you can see like I use Gemini to do a whole bunch of summarization, okay. And then I export it to a spreadsheet and annotate it. And then we look at pass rates and fail rates and we can see like how different prompts perform based around my human annotation. Once we have that. We can define evaluation criteria Do we want consistency, relevance, clarity. Then we prepared a few shot examples where we have a few label examples to give an LLM as a judge. Then we write our few shot prompt saying, you are an expert evaluator of tech summaries. For each summary, evaluate its quality based on these three things. And Bob's your uncle, so to speak. We can generate LLM judgments, compare them with human annotations. And then we can automate the scalable evaluation and monitor and iterate and move from there. So I know I kind of, that was a whirlwind tour through that notebook but And I appreciate that that's a lot to take in. I do want to say everything in this notebook is what we've been talking about for an hour and a half now as well and foreshadowed last week also so All of these things, this process essentially is under the rubric of evaluation-driven development. So if there's one takeaway I want you to have. It's here. It's on one of my first slides. The one takeaway. Of course, once you have one takeaway, you can't find it, right. Yes. The one takeaway is that As you build out an MVP, build out an MVE. With any minimal viable product, build out a minimum viable evaluation harness that moves beyond vibes as quickly as possible. It doesn't have to be anything super robust. And it's not that different to ML powered software, right. This process. There are some details that are, but both workflows are grounded in test sets. Iteration and failure analysis. So\u2026 um what Do I want to say i've already shown you the app I built that I'd love you all to contribute to and we'll share a repo later with respect. To that. And we already had a look at the logs together um so What I'd like you to do is update the MVP from workshop two and start building out some evaluations. Use everything that I've given you today in order to do this. And I'm really excited to see in the homework channel What you will do with this type of thing and what happens in Builders Club with respect to that. The next workshop, I'm beyond excited for. It will be Stefan. It's my favorite talk in workshop in this course except for this one. No, I'm kidding. But I do love this workshop and we have A lot of guest speakers on Thursday, we've got Innes Montani from Spacey, Pavitra from Quansite. Vincent from Marimo and Catherine Jarmel from KJ Amistan talking a lot about all the work they do with LLMs from human in the loop development to building applications with open source tools. And a lot of Pavitra's work at QuanSight. She works with Travis Oliphant, who created NumPy back in the day, right. But there, you know, I've said we've had like the rug pulled out from under us with respect to open source tooling and they're working on a lot of interesting tooling for building AI powered applications using open source Vinsome's just a An absolute joy who has so many cool tricks and tips And\u2026 Catherine does a lot of work on adversarial threats, so very excited for that. And I'll be getting up at 4.30 a.m. Local time for Innices Talk. I'm very excited to do that, actually.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 17", "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "e9c0557a-5b5f-45dd-af0c-dde43e745e04", "position": 16, "token_count": 1055}, "id": "WS3_e9c0557a-5b5f-45dd-af0c-dde43e745e04", "relevance": 1.0}, {"text": "It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right. So I do want to just quickly go to the next notebook to notebook I have a brief summary of this before, but to give a high level summary. Of what we're doing here. I wanted to give you an introduction to actually summarization is something that isn't that we're still working on here, right. So think about how to evaluate factual consistency in in in summarization. So in this, what we do is we load and preprocess a data set containing source text. Correct summaries and incorrect summaries. So distractor summaries, right. Then we hand label, we annotate summaries for factual consistency using pass fail and then calculate metrics like accuracy, precision, and recall. We identify common failure modes in incorrect summaries to better understand the model behavior. Then we train an LLM as a judge to evaluate summaries based on the human annotations, providing scalability for larger data sets. And then we monitor the LLM judgments over time by validating against human labeled samples to ensure alignment and consistency Okay. So that's what we do here. If you're super excited to jump in sooner, but I'll put a video in this channel, in the channel Later today. And you can see like I use Gemini to do a whole bunch of summarization, okay. And then I export it to a spreadsheet and annotate it. And then we look at pass rates and fail rates and we can see like how different prompts perform based around my human annotation. Once we have that. We can define evaluation criteria Do we want consistency, relevance, clarity. Then we prepared a few shot examples where we have a few label examples to give an LLM as a judge. Then we write our few shot prompt saying, you are an expert evaluator of tech summaries. For each summary, evaluate its quality based on these three things. And Bob's your uncle, so to speak. We can generate LLM judgments, compare them with human annotations. And then we can automate the scalable evaluation and monitor and iterate and move from there. So I know I kind of, that was a whirlwind tour through that notebook but And I appreciate that that's a lot to take in. I do want to say everything in this notebook is what we've been talking about for an hour and a half now as well and foreshadowed last week also so All of these things, this process essentially is under the rubric of evaluation-driven development. So if there's one takeaway I want you to have. It's here. It's on one of my first slides. The one takeaway. Of course, once you have one takeaway, you can't find it, right. Yes. The one takeaway is that As you build out an MVP, build out an MVE. With any minimal viable product, build out a minimum viable evaluation harness that moves beyond vibes as quickly as possible. It doesn't have to be anything super robust. And it's not that different to ML powered software, right. This process. There are some details that are, but both workflows are grounded in test sets. Iteration and failure analysis. So\u2026 um what Do I want to say i've already shown you the app I built that I'd love you all to contribute to and we'll share a repo later with respect. To that. And we already had a look at the logs together um so What I'd like you to do is update the MVP from workshop two and start building out some evaluations. Use everything that I've given you today in order to do this. And I'm really excited to see in the homework channel What you will do with this type of thing and what happens in Builders Club with respect to that. The next workshop, I'm beyond excited for. It will be Stefan. It's my favorite talk in workshop in this course except for this one. No, I'm kidding. But I do love this workshop and we have A lot of guest speakers on Thursday, we've got Innes Montani from Spacey, Pavitra from Quansite. Vincent from Marimo and Catherine Jarmel from KJ Amistan talking a lot about all the work they do with LLMs from human in the loop development to building applications with open source tools. And a lot of Pavitra's work at QuanSight. She works with Travis Oliphant, who created NumPy back in the day, right. But there, you know, I've said we've had like the rug pulled out from under us with respect to open source tooling and they're working on a lot of interesting tooling for building AI powered applications using open source Vinsome's just a An absolute joy who has so many cool tricks and tips And\u2026 Catherine does a lot of work on adversarial threats, so very excited for that. And I'll be getting up at 4.30 a.m. Local time for Innices Talk. I'm very excited to do that, actually.", "metadata": {"workshop_id": "WS3", "speaker": "Unknown", "position": 16, "original_chunk_id": "c3c1a303-20c6-4f43-b071-38ab1db3a9e8", "timestamp": "Chunk 17", "source": "sentence_chunking", "token_count": 1055}, "id": "WS3_c3c1a303-20c6-4f43-b071-38ab1db3a9e8", "relevance": 1.0}, {"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"timestamp": "Chunk 18", "speaker": "Unknown", "workshop_id": "WS4", "original_chunk_id": "f9e33134-8ebe-4b68-90f8-9d442e7bb591", "token_count": 1055, "position": 17, "source": "sentence_chunking"}, "id": "WS4_f9e33134-8ebe-4b68-90f8-9d442e7bb591", "relevance": 1.0}, {"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 18", "position": 17, "original_chunk_id": "334c243a-3488-4cde-a7c9-f1f7345397fb", "token_count": 1055, "workshop_id": "WS4"}, "id": "WS4_334c243a-3488-4cde-a7c9-f1f7345397fb", "relevance": 1.0}, {"text": "And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say. Fine tuning isn't really useful these days, cause if I I would, that sounds like something I I would say. What I mean is that most of the time 213 hugo bowne-anderson: you need. You think you need to fine tune. You can do other things such as prompt your way out of it. 214 hugo bowne-anderson: There are cases, for example. 215 hugo bowne-anderson: well, most of the cases I see are actually for vision models and these types of things. But there are cases where you want it to be really good on domain, specific expertise or previous conversations, and then fine tuning would be useful. The other use case that I like, for fine tuning is migrating to smaller models. 216 hugo bowne-anderson: So if you build something and you're using like a state of the art, relatively expensive vendor based model, and you want to migrate to a smaller open weight. Model yourself, taking some of the conversations with Gpt. Whatever, and fine tuning on those can be incredibly useful. I also love that. It seems like that's in violation of Openai's terms and services, and I love that they would have 217 hugo bowne-anderson: the attitude to to be like this is our copyright, or something like that. I find it fascinating. But yeah, I just actually haven't haven't seen a lot of uses of fine tuning that haven't been achievable without others. I would. I would welcome 218 hugo bowne-anderson: William to add anything, he said, and he's he's linked to Emmanuel's wonderful talk. Why, fine tuning is dead. 219 William Horton: Yeah, I think that I think the main thing is that you can get pretty far without having to fine tune your model. And and so a lot of both people and and companies doing this work will start by using the strongest model that's available via Api. Before they go down that road. I I also would add to the list of things, Hugo said, where people still get some 220 William Horton: value out of fine tuning. I have also seen people fine tuning their own embeddings and re rankers for retrieval specifically, is a place where I think people still find a benefit, especially because those models tend to be smaller as well. So the inference cost is is not as high, and the training is not as extensive. That's another place where I think people still do it and get maybe more value than if you were trying to do fine tuning like for the whole Llm. 221 hugo bowne-anderson: Absolutely love it. And I also just noticed in the chat Vishut went. This is probably when I spoke about proof of concept purgatory. Vishrud has faced issues when he used Mvp. Without context. In North America it's used for most valuable player totally agree, and it shows how much 222 hugo bowne-anderson: how most people I speak with at least professionally work in tech and know what know that it means. That right. 223 hugo bowne-anderson: awesome. Well, I'm just. I can't see everything in discord at the moment, but so much fantastic conversation and 224 hugo bowne-anderson: questions as as well. So if anyone. 225 hugo bowne-anderson: I'm gonna open a window right now. But if anyone doesn't have access to github, could you 226 hugo bowne-anderson: just put your handle in the workshop. One channel and I will add you almost in real time. 227 hugo bowne-anderson: And the reason for that is with Github has changed certain things so that I actually can't assign anyone else to be admin currently as it's under my personal. 228 hugo bowne-anderson: Okay, we have Vinysh. Oh, oh, I love this and I do hope that 229 hugo bowne-anderson: you're all okay with us. Just taking 2\u00a0min to 230 hugo bowne-anderson: add a few more people. Oh, this is great. 231 hugo bowne-anderson: amazing. 232 Nathan Danielsen: Well, while people are doing that, yeah. Want to comment on the the rich conversations that's happening in discord right now. And from the 1st kind of like run of this course. This is what that the conversations are. One of the highlights as well. So just calling that out. That's a lot of good stuff. A lot of smart people sharing resources there. So check out the discord for the session. 233 hugo bowne-anderson: I really appreciate that. And in fact, I know it. I mean, we we started building this right to do a court.", "metadata": {"timestamp": "Chunk 13", "workshop_id": "WS1", "speaker": "Unknown", "position": 12, "source": "sentence_chunking", "original_chunk_id": "9a58ab61-5b20-4005-89ac-26935dae8118", "token_count": 1044}, "id": "WS1_9a58ab61-5b20-4005-89ac-26935dae8118", "relevance": 1.0}, {"text": "And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say. Fine tuning isn't really useful these days, cause if I I would, that sounds like something I I would say. What I mean is that most of the time 213 hugo bowne-anderson: you need. You think you need to fine tune. You can do other things such as prompt your way out of it. 214 hugo bowne-anderson: There are cases, for example. 215 hugo bowne-anderson: well, most of the cases I see are actually for vision models and these types of things. But there are cases where you want it to be really good on domain, specific expertise or previous conversations, and then fine tuning would be useful. The other use case that I like, for fine tuning is migrating to smaller models. 216 hugo bowne-anderson: So if you build something and you're using like a state of the art, relatively expensive vendor based model, and you want to migrate to a smaller open weight. Model yourself, taking some of the conversations with Gpt. Whatever, and fine tuning on those can be incredibly useful. I also love that. It seems like that's in violation of Openai's terms and services, and I love that they would have 217 hugo bowne-anderson: the attitude to to be like this is our copyright, or something like that. I find it fascinating. But yeah, I just actually haven't haven't seen a lot of uses of fine tuning that haven't been achievable without others. I would. I would welcome 218 hugo bowne-anderson: William to add anything, he said, and he's he's linked to Emmanuel's wonderful talk. Why, fine tuning is dead. 219 William Horton: Yeah, I think that I think the main thing is that you can get pretty far without having to fine tune your model. And and so a lot of both people and and companies doing this work will start by using the strongest model that's available via Api. Before they go down that road. I I also would add to the list of things, Hugo said, where people still get some 220 William Horton: value out of fine tuning. I have also seen people fine tuning their own embeddings and re rankers for retrieval specifically, is a place where I think people still find a benefit, especially because those models tend to be smaller as well. So the inference cost is is not as high, and the training is not as extensive. That's another place where I think people still do it and get maybe more value than if you were trying to do fine tuning like for the whole Llm. 221 hugo bowne-anderson: Absolutely love it. And I also just noticed in the chat Vishut went. This is probably when I spoke about proof of concept purgatory. Vishrud has faced issues when he used Mvp. Without context. In North America it's used for most valuable player totally agree, and it shows how much 222 hugo bowne-anderson: how most people I speak with at least professionally work in tech and know what know that it means. That right. 223 hugo bowne-anderson: awesome. Well, I'm just. I can't see everything in discord at the moment, but so much fantastic conversation and 224 hugo bowne-anderson: questions as as well. So if anyone. 225 hugo bowne-anderson: I'm gonna open a window right now. But if anyone doesn't have access to github, could you 226 hugo bowne-anderson: just put your handle in the workshop. One channel and I will add you almost in real time. 227 hugo bowne-anderson: And the reason for that is with Github has changed certain things so that I actually can't assign anyone else to be admin currently as it's under my personal. 228 hugo bowne-anderson: Okay, we have Vinysh. Oh, oh, I love this and I do hope that 229 hugo bowne-anderson: you're all okay with us. Just taking 2\u00a0min to 230 hugo bowne-anderson: add a few more people. Oh, this is great. 231 hugo bowne-anderson: amazing. 232 Nathan Danielsen: Well, while people are doing that, yeah. Want to comment on the the rich conversations that's happening in discord right now. And from the 1st kind of like run of this course. This is what that the conversations are. One of the highlights as well. So just calling that out. That's a lot of good stuff. A lot of smart people sharing resources there. So check out the discord for the session. 233 hugo bowne-anderson: I really appreciate that. And in fact, I know it. I mean, we we started building this right to do a court.", "metadata": {"position": 12, "token_count": 1044, "timestamp": "Chunk 13", "original_chunk_id": "b4168834-631b-4b73-b888-0cd8a51cec8e", "speaker": "Unknown", "workshop_id": "WS1", "source": "sentence_chunking"}, "id": "WS1_b4168834-631b-4b73-b888-0cd8a51cec8e", "relevance": 1.0}]}}{"question": "Does anyone talk about a builders in residence program here?", "response": "Yes, the transcript discusses a Builders in Residence program. This program was initiated in response to requests from participants in the first cohort of the course, who wanted more opportunities for building, office hours, and support. \n\n**Context and Action Items:**\n- The program includes experienced builders from the industry, such as Jeff Pidcock, William Horton, and Nathan Danielson, who are returning to assist current participants.\n- They will help facilitate sessions and provide guidance on projects, leveraging their expertise in various fields, including data science and machine learning.\n\n**Desire:**\nThe Builders in Residence program aims to enhance the learning experience by providing tailored support and fostering collaboration among participants.\n\n**Action:**\nIf you're interested in participating or have questions about the Builders in Residence program, consider reaching out in the Discord channel or attending the upcoming sessions where these builders will be present.", "context_info": {"num_chunks": 10, "context_tokens": 10418, "completion_tokens": 173, "embedding_tokens": 15360, "workshops_used": ["WS1", "WS3", "WS5", "WS4"], "chunks": [{"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"timestamp": "Chunk 2", "original_chunk_id": "5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "workshop_id": "WS3", "source": "sentence_chunking", "token_count": 1068, "position": 1, "speaker": "Unknown"}, "id": "WS3_5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "relevance": 1.0}, {"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"token_count": 1068, "workshop_id": "WS3", "position": 1, "timestamp": "Chunk 2", "speaker": "Unknown", "original_chunk_id": "8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "source": "sentence_chunking"}, "id": "WS3_8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "relevance": 1.0}, {"text": "We've got 448 hugo bowne-anderson: a folder of lots of different Linkedin profiles that you can use for this as well, and thank you to Stefan for that. It's when I thought you had to do all this manually. 449 hugo bowne-anderson: And he he showed me how to do that. 450 hugo bowne-anderson: So incorporate some retrieval processes into your app. Okay. And I would encourage you to think about exploring advanced retrieval methods like semantic search or chunking strategies. And there are so many ideas that I thought to create a notebook around, different things to do, and different ways to 451 hugo bowne-anderson: think about this and so once again generating embeddings, thinking about different. And we haven't talked about chunking yet, but when you have very large things you want to do them in different sized chunks, and we can can talk about that moving forward. I would also encourage you to think about. 452 hugo bowne-anderson: Do you want your app to. 453 hugo bowne-anderson: you know, reach out to people based on it. Or do you want to match profiles to particular jobs. Do you have a job listing that you want to match profiles to. Or do you have profiles that you want to match a job listing to using all of these different tools. The other thing. 454 hugo bowne-anderson: at the risk of biting off more than anyone can chew, but it is highly relevant. 455 hugo bowne-anderson: I'll once again post this 456 hugo bowne-anderson: in. I'm very excited because this app that I built to query our workshop transcripts. You may recall last time I showed you the basic Q&A. Which 457 hugo bowne-anderson: essentially says you're a helpful assistant. Answer questions based only on the provided context. And I wasn't able yet to show you the simple rag version which is the one that I stood up on modal and served to you all. But I encourage those interested to have a look through this this code, and so I've got my embedding functions. I've got my system prompt. 458 hugo bowne-anderson: I'm doing a bit of counting tokens and figuring out cost and that type of stuff. Then a bit of chunking, working with metadata, extracting the workshop material, splitting into paragraphs, bit of preprocessing, creating the chunks, and so on. Then some vector storage stuff. So using chroma and vector dB, adding the chunks to the collection. 459 hugo bowne-anderson: then querying the collection, then retrieval functions. And I, if you're interested, please do look through this code very happy to chat about it very happy to see pull requests about it as well, and then some logging. So. But I just do want to say, in 781 lines of code, I was really able to to build something which, at least as a 1st approximation, stands up pretty well and 460 hugo bowne-anderson: gives me a bunch of logs and that type of stuff and allows you all to interact with it. So once again, I'll include this all in ideas for project and homework stuff. 461 hugo bowne-anderson: But it would be exciting if anyone wants to work on this app with me. That would be fun. The other thing, I I suppose, worth mentioning is, as you all know 462 hugo bowne-anderson: it. Currently, we're only working with workshop one from this cohort, and I want to start adding more workshops to it as well. So that's enough out of me. I'm very excited to 463 hugo bowne-anderson: introduce William, one of our builders in residence to the proverbial virtual stage, to talk about what is production. This, this is one of the things I love about this course as well. Is, you know, we have conversations and new material and new ways of speaking with each other. Come, come out of it. I met William earlier this year. When he took 464 hugo bowne-anderson: the 1st cohort of this course got along like a house on fire. I learned a lot from him, and you know all the stuff he's put into production. It included health I've been very excited to learn about, and everything he can say. 465 hugo bowne-anderson: I also just want to congratulate William on his new position at Maven. Health. Is that. Is that correct. William. 466 William Horton: Maven maven clinic yep. 467 hugo bowne-anderson: Maven Clinic. I'm I'm sorry. And I don't know if Nathan has conveyed this to you yet, but him and his wife have been used maven clinic for some time, and and love it so. 468 Nathan Danielsen: Yeah, I'm I'm just gonna chime in. We're super promoters at my work.", "metadata": {"token_count": 1064, "speaker": "Unknown", "timestamp": "Chunk 21", "position": 20, "workshop_id": "WS5", "original_chunk_id": "9f8ae0f7-6303-4dd8-be08-90ffec099e9d", "source": "sentence_chunking"}, "id": "WS5_9f8ae0f7-6303-4dd8-be08-90ffec099e9d", "relevance": 1.0}, {"text": "We've got 448 hugo bowne-anderson: a folder of lots of different Linkedin profiles that you can use for this as well, and thank you to Stefan for that. It's when I thought you had to do all this manually. 449 hugo bowne-anderson: And he he showed me how to do that. 450 hugo bowne-anderson: So incorporate some retrieval processes into your app. Okay. And I would encourage you to think about exploring advanced retrieval methods like semantic search or chunking strategies. And there are so many ideas that I thought to create a notebook around, different things to do, and different ways to 451 hugo bowne-anderson: think about this and so once again generating embeddings, thinking about different. And we haven't talked about chunking yet, but when you have very large things you want to do them in different sized chunks, and we can can talk about that moving forward. I would also encourage you to think about. 452 hugo bowne-anderson: Do you want your app to. 453 hugo bowne-anderson: you know, reach out to people based on it. Or do you want to match profiles to particular jobs. Do you have a job listing that you want to match profiles to. Or do you have profiles that you want to match a job listing to using all of these different tools. The other thing. 454 hugo bowne-anderson: at the risk of biting off more than anyone can chew, but it is highly relevant. 455 hugo bowne-anderson: I'll once again post this 456 hugo bowne-anderson: in. I'm very excited because this app that I built to query our workshop transcripts. You may recall last time I showed you the basic Q&A. Which 457 hugo bowne-anderson: essentially says you're a helpful assistant. Answer questions based only on the provided context. And I wasn't able yet to show you the simple rag version which is the one that I stood up on modal and served to you all. But I encourage those interested to have a look through this this code, and so I've got my embedding functions. I've got my system prompt. 458 hugo bowne-anderson: I'm doing a bit of counting tokens and figuring out cost and that type of stuff. Then a bit of chunking, working with metadata, extracting the workshop material, splitting into paragraphs, bit of preprocessing, creating the chunks, and so on. Then some vector storage stuff. So using chroma and vector dB, adding the chunks to the collection. 459 hugo bowne-anderson: then querying the collection, then retrieval functions. And I, if you're interested, please do look through this code very happy to chat about it very happy to see pull requests about it as well, and then some logging. So. But I just do want to say, in 781 lines of code, I was really able to to build something which, at least as a 1st approximation, stands up pretty well and 460 hugo bowne-anderson: gives me a bunch of logs and that type of stuff and allows you all to interact with it. So once again, I'll include this all in ideas for project and homework stuff. 461 hugo bowne-anderson: But it would be exciting if anyone wants to work on this app with me. That would be fun. The other thing, I I suppose, worth mentioning is, as you all know 462 hugo bowne-anderson: it. Currently, we're only working with workshop one from this cohort, and I want to start adding more workshops to it as well. So that's enough out of me. I'm very excited to 463 hugo bowne-anderson: introduce William, one of our builders in residence to the proverbial virtual stage, to talk about what is production. This, this is one of the things I love about this course as well. Is, you know, we have conversations and new material and new ways of speaking with each other. Come, come out of it. I met William earlier this year. When he took 464 hugo bowne-anderson: the 1st cohort of this course got along like a house on fire. I learned a lot from him, and you know all the stuff he's put into production. It included health I've been very excited to learn about, and everything he can say. 465 hugo bowne-anderson: I also just want to congratulate William on his new position at Maven. Health. Is that. Is that correct. William. 466 William Horton: Maven maven clinic yep. 467 hugo bowne-anderson: Maven Clinic. I'm I'm sorry. And I don't know if Nathan has conveyed this to you yet, but him and his wife have been used maven clinic for some time, and and love it so. 468 Nathan Danielsen: Yeah, I'm I'm just gonna chime in. We're super promoters at my work.", "metadata": {"source": "sentence_chunking", "token_count": 1064, "position": 20, "original_chunk_id": "6e8cf94e-8275-443e-bd2d-34ba35a70773", "timestamp": "Chunk 21", "workshop_id": "WS5", "speaker": "Unknown"}, "id": "WS5_6e8cf94e-8275-443e-bd2d-34ba35a70773", "relevance": 1.0}, {"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"source": "sentence_chunking", "timestamp": "Chunk 18", "position": 17, "speaker": "Unknown", "original_chunk_id": "f9e33134-8ebe-4b68-90f8-9d442e7bb591", "workshop_id": "WS4", "token_count": 1055}, "id": "WS4_f9e33134-8ebe-4b68-90f8-9d442e7bb591", "relevance": 1.0}, {"text": "And then if someone comes to you, if you need to use a framework, hopefully giving you a few pointers as to how to push back or ask questions as to whether it's a good idea. And then this last slide is to how guardrails fit in and some kind of caveats and complications with using them, which is namely latency and then streaming. I guess I'll skip this part, I guess, in terms of time, put your questions in discord and we'll kind of get to them. Otherwise, Hugo. Any words to say for the upcoming workshops. Yeah, yeah. I've got a lot to say, actually. So next week, we're going to be jumping into Thinking through embeddings, RAG, and then a whole bunch of agentic stuff as well, which I'm super excited. Excited for. So that's a little precursor there. Could you go to the next slide, Stefan. Yep. So what I would love is Eddie, we're about to have a guest lecture, but as always, please give as much feedback as possible. I'm going to put this link in the discord now, if you can take a couple of minutes now while Oh, sorry, I just\u2026 I just put something else in the Discord. I don't know if anybody remembers the game Leisure Suit Larry. But I actually had, I was trying to create, anyway, I got it to break a bunch of guardrails In terms of getting celebrities and breaking copyright the other day as well. Yeah. Just be careful, if you're having a chat with one of these systems and it gets a bit iffy on its own guardrails, it will then be very conservative. So start a new conversation with it at that point. We would love feedback. Please give us as much feedback as possible. Once again, I hope you all are aware that we take your feedback very, very seriously. So if you have it. Give it. Next slide. Oh. No. Wait a second. Can you go to the next one. No, these are any slides you added. Unless you added No, no, no, no. No, there are several others. I'm looking at them right now, actually Okay, I just need to\u2026 Fresh. Ah. Yes, there we go. So you change them yeah Yes. This is important. We've got a bunch of very exciting guest talks next week. One of the ones I'm most excited about is actually William Horton, one of our builders in residence, come and talk about what is production. People talk about production or not a lot of the time. But it's not only a continuum, it's a dimensional unbounded space in a lot of respects. So we're going to jump into that with William. We've also got Constantine from Learn Prompting. So he runs DevRel there and we're getting you all your free access to learn prompting by end of week. On top of that, we've got Paige Bailey is coming from Google DeepMind to talk about building AI systems with Gemini. And in fact. You should all have received in the past hour or two or in the next hour or two will receive instructions for how to get your $300 credit coupon for Gemini APIs. And Paige is coming\u2026 essentially to talk about how to leverage that $300 to build cool stuff. Okay. So super excited about that. And then\u2026 Shredder from Replicate is actually going to have a 45 minute session on building multimodal apps from scratch on Replicate. Trying to figure out, we've got 70 people enrolled in the course. I'm trying to figure out how much more wiggle room they have for that. So I can't promise anything but know that I'm working hard to get more credits. But if I can't, I do sincerely apologize for that. This session will be incredibly useful. If you want to put 10 bucks of your own down on these credits as well, though. Next slide. We then then we then have This is actually going to be super cool. Greg Cicerelli, I've mentioned him. He used to be a VP of data science at GitHub and chief product at Plurasight He's going to come and give a short talk and demo on how to build with AI assistance. We've talked about vibe coding and He frames it as software composition. So stepping back and building systems with AI assistance. And he's one of the people who builds custom JSON viewers for a lot of stuff that he does. So those are the types of things we'll be doing there. And then Philip Carter from Huntingcomb will give a talk on building an MCP from the the trenches. And I think It's unclear what protocols will last. The test of time currently for, you know, LLMs and agents interacting with them each other and other systems I think MCP with the community behind it, at least in some form, is particularly interesting to think about at the moment, but I don't want to index on it On it too much. And then an old dear friend of mine and collaborator, Alan Nickel um will be talking about building reliable agents, thinking through state.", "metadata": {"original_chunk_id": "334c243a-3488-4cde-a7c9-f1f7345397fb", "speaker": "Unknown", "workshop_id": "WS4", "position": 17, "timestamp": "Chunk 18", "token_count": 1055, "source": "sentence_chunking"}, "id": "WS4_334c243a-3488-4cde-a7c9-f1f7345397fb", "relevance": 1.0}, {"text": "And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say. Fine tuning isn't really useful these days, cause if I I would, that sounds like something I I would say. What I mean is that most of the time 213 hugo bowne-anderson: you need. You think you need to fine tune. You can do other things such as prompt your way out of it. 214 hugo bowne-anderson: There are cases, for example. 215 hugo bowne-anderson: well, most of the cases I see are actually for vision models and these types of things. But there are cases where you want it to be really good on domain, specific expertise or previous conversations, and then fine tuning would be useful. The other use case that I like, for fine tuning is migrating to smaller models. 216 hugo bowne-anderson: So if you build something and you're using like a state of the art, relatively expensive vendor based model, and you want to migrate to a smaller open weight. Model yourself, taking some of the conversations with Gpt. Whatever, and fine tuning on those can be incredibly useful. I also love that. It seems like that's in violation of Openai's terms and services, and I love that they would have 217 hugo bowne-anderson: the attitude to to be like this is our copyright, or something like that. I find it fascinating. But yeah, I just actually haven't haven't seen a lot of uses of fine tuning that haven't been achievable without others. I would. I would welcome 218 hugo bowne-anderson: William to add anything, he said, and he's he's linked to Emmanuel's wonderful talk. Why, fine tuning is dead. 219 William Horton: Yeah, I think that I think the main thing is that you can get pretty far without having to fine tune your model. And and so a lot of both people and and companies doing this work will start by using the strongest model that's available via Api. Before they go down that road. I I also would add to the list of things, Hugo said, where people still get some 220 William Horton: value out of fine tuning. I have also seen people fine tuning their own embeddings and re rankers for retrieval specifically, is a place where I think people still find a benefit, especially because those models tend to be smaller as well. So the inference cost is is not as high, and the training is not as extensive. That's another place where I think people still do it and get maybe more value than if you were trying to do fine tuning like for the whole Llm. 221 hugo bowne-anderson: Absolutely love it. And I also just noticed in the chat Vishut went. This is probably when I spoke about proof of concept purgatory. Vishrud has faced issues when he used Mvp. Without context. In North America it's used for most valuable player totally agree, and it shows how much 222 hugo bowne-anderson: how most people I speak with at least professionally work in tech and know what know that it means. That right. 223 hugo bowne-anderson: awesome. Well, I'm just. I can't see everything in discord at the moment, but so much fantastic conversation and 224 hugo bowne-anderson: questions as as well. So if anyone. 225 hugo bowne-anderson: I'm gonna open a window right now. But if anyone doesn't have access to github, could you 226 hugo bowne-anderson: just put your handle in the workshop. One channel and I will add you almost in real time. 227 hugo bowne-anderson: And the reason for that is with Github has changed certain things so that I actually can't assign anyone else to be admin currently as it's under my personal. 228 hugo bowne-anderson: Okay, we have Vinysh. Oh, oh, I love this and I do hope that 229 hugo bowne-anderson: you're all okay with us. Just taking 2\u00a0min to 230 hugo bowne-anderson: add a few more people. Oh, this is great. 231 hugo bowne-anderson: amazing. 232 Nathan Danielsen: Well, while people are doing that, yeah. Want to comment on the the rich conversations that's happening in discord right now. And from the 1st kind of like run of this course. This is what that the conversations are. One of the highlights as well. So just calling that out. That's a lot of good stuff. A lot of smart people sharing resources there. So check out the discord for the session. 233 hugo bowne-anderson: I really appreciate that. And in fact, I know it. I mean, we we started building this right to do a court.", "metadata": {"token_count": 1044, "speaker": "Unknown", "original_chunk_id": "9a58ab61-5b20-4005-89ac-26935dae8118", "position": 12, "workshop_id": "WS1", "source": "sentence_chunking", "timestamp": "Chunk 13"}, "id": "WS1_9a58ab61-5b20-4005-89ac-26935dae8118", "relevance": 1.0}, {"text": "And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say. Fine tuning isn't really useful these days, cause if I I would, that sounds like something I I would say. What I mean is that most of the time 213 hugo bowne-anderson: you need. You think you need to fine tune. You can do other things such as prompt your way out of it. 214 hugo bowne-anderson: There are cases, for example. 215 hugo bowne-anderson: well, most of the cases I see are actually for vision models and these types of things. But there are cases where you want it to be really good on domain, specific expertise or previous conversations, and then fine tuning would be useful. The other use case that I like, for fine tuning is migrating to smaller models. 216 hugo bowne-anderson: So if you build something and you're using like a state of the art, relatively expensive vendor based model, and you want to migrate to a smaller open weight. Model yourself, taking some of the conversations with Gpt. Whatever, and fine tuning on those can be incredibly useful. I also love that. It seems like that's in violation of Openai's terms and services, and I love that they would have 217 hugo bowne-anderson: the attitude to to be like this is our copyright, or something like that. I find it fascinating. But yeah, I just actually haven't haven't seen a lot of uses of fine tuning that haven't been achievable without others. I would. I would welcome 218 hugo bowne-anderson: William to add anything, he said, and he's he's linked to Emmanuel's wonderful talk. Why, fine tuning is dead. 219 William Horton: Yeah, I think that I think the main thing is that you can get pretty far without having to fine tune your model. And and so a lot of both people and and companies doing this work will start by using the strongest model that's available via Api. Before they go down that road. I I also would add to the list of things, Hugo said, where people still get some 220 William Horton: value out of fine tuning. I have also seen people fine tuning their own embeddings and re rankers for retrieval specifically, is a place where I think people still find a benefit, especially because those models tend to be smaller as well. So the inference cost is is not as high, and the training is not as extensive. That's another place where I think people still do it and get maybe more value than if you were trying to do fine tuning like for the whole Llm. 221 hugo bowne-anderson: Absolutely love it. And I also just noticed in the chat Vishut went. This is probably when I spoke about proof of concept purgatory. Vishrud has faced issues when he used Mvp. Without context. In North America it's used for most valuable player totally agree, and it shows how much 222 hugo bowne-anderson: how most people I speak with at least professionally work in tech and know what know that it means. That right. 223 hugo bowne-anderson: awesome. Well, I'm just. I can't see everything in discord at the moment, but so much fantastic conversation and 224 hugo bowne-anderson: questions as as well. So if anyone. 225 hugo bowne-anderson: I'm gonna open a window right now. But if anyone doesn't have access to github, could you 226 hugo bowne-anderson: just put your handle in the workshop. One channel and I will add you almost in real time. 227 hugo bowne-anderson: And the reason for that is with Github has changed certain things so that I actually can't assign anyone else to be admin currently as it's under my personal. 228 hugo bowne-anderson: Okay, we have Vinysh. Oh, oh, I love this and I do hope that 229 hugo bowne-anderson: you're all okay with us. Just taking 2\u00a0min to 230 hugo bowne-anderson: add a few more people. Oh, this is great. 231 hugo bowne-anderson: amazing. 232 Nathan Danielsen: Well, while people are doing that, yeah. Want to comment on the the rich conversations that's happening in discord right now. And from the 1st kind of like run of this course. This is what that the conversations are. One of the highlights as well. So just calling that out. That's a lot of good stuff. A lot of smart people sharing resources there. So check out the discord for the session. 233 hugo bowne-anderson: I really appreciate that. And in fact, I know it. I mean, we we started building this right to do a court.", "metadata": {"timestamp": "Chunk 13", "speaker": "Unknown", "position": 12, "source": "sentence_chunking", "original_chunk_id": "b4168834-631b-4b73-b888-0cd8a51cec8e", "workshop_id": "WS1", "token_count": 1044}, "id": "WS1_b4168834-631b-4b73-b888-0cd8a51cec8e", "relevance": 1.0}, {"text": "I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm. Powered apps and all the things he's just mentioned, but plus plus essentially. 17 Stefan Krawczyk: Cool. 18 Stefan Krawczyk: Yeah, so hopefully, I'll be able to reveal more in the course. But I have a few things with kind of, you know, legal, social and a few other things that need to clear before I officially change my Linkedin status. So I appreciate you guys keeping that under apps until I can do that myself. Thanks. 19 hugo bowne-anderson: And, as we all know, Linkedin is the only reality. So we'll wait for that to happen. Thanks so much, Stefan. I also am super excited to introduce you to Jeff Pidcock, Nathan Danielson, and William Horton, who 20 hugo bowne-anderson: joined for cohort, one as participants students in the course, and they're very serious builders in industry, and I was so blown away I was blown away by the caliber of participants generally, but we invited them back to to kick off our builders in residence program one of the requests from people in cohort. One was just more time building more office hours more, being able to 21 hugo bowne-anderson: answer questions and help people build. So I chatted with Jeff, William and and Nathan about this, and they were so excited to to come back and work with you all on everything you're you're trying to build. So I'd love for them each to say a thing or 2, and we've got them geographically distributed. So for context, this course is really timed for the Us. And Apac. Selfishly, because I'm in Sydney. 22 hugo bowne-anderson: and so Jeff is in Sydney, or just south of Sydney, plus Williams on the East coast in DC. And and Nathan's in Los Angeles. So we've got them geographically distributed so they can organize pods to to work with you all on on things. So, Jeff, maybe you could say a few words about yourself. 23 gp: So Hugo and Hello, everybody. So, as Hugo mentioned, I'm in Sydney Plus. So Sydney's a big place, and I'm in Wollongong, which is south of Sydney, and I'm pretty excited to come on back my background is more as a data scientist than a software engineer. So I really appreciated the opportunity to learn particularly from Stefan in in the 1st round and skill up with Tessie. And I think what I bring to this course is just a bit of bandwidth and capacity to get people helping each other. 24 gp: So you're gonna see for yourself. It's gonna be a lot of material that's thrown at you very quickly. So I think the best thing I can do to help you is to give you some space to explore that, and also some accountability to explore that as well. So I'm looking forward to work with Hugo to propose some sessions maybe some in person stuff in Sydney, which is a beautiful city. And yeah, I'll pass this over to William next to introduce himself. 25 William Horton: Alright, thank you. So I'm William. As as Hugo said, I live in Washington, DC, and I come more from the software engineering side. For the past 4 years I've been working as a machine learning engineer at a company called included Health and for the past 2 have really been focused on building out our Gen. AI platform. 26 William Horton: So if you're particularly interested in Gen. AI for healthcare applications like understanding, explaining health benefits automated scribing for doctors. These are some of the things that I've been working on or supporting. So those are topics that I got excited about. 27 William Horton: I think that's all I can say for now, and I'll pass it on to Nathan. 28 Nathan Danielsen: Everybody, Nathan Danielson, here in Los Angeles, California, Les. 29 Nathan Danielsen: which I mean technically in Pasadena, which is the plus part not really la. But the 1st time I've heard that. So I'm going to adopt that bring it over to to la, so yeah, I've been in the kind of data building space for a while. I started as a software engineer. And then about 5, 6 years ago, kind of moved over into the data science. Ml, range. I've kind of been all over the place. But I'm currently at Carvana right now. Just started a new role. 30 Nathan Danielsen: Focusing on Gen. AI for content generation building building out various domain data pipelines for automotive data, which is very fun.", "metadata": {"timestamp": "Chunk 2", "token_count": 1057, "workshop_id": "WS1", "source": "sentence_chunking", "speaker": "Unknown", "position": 1, "original_chunk_id": "8a960075-f39b-4765-bda2-3c53f464b6a3"}, "id": "WS1_8a960075-f39b-4765-bda2-3c53f464b6a3", "relevance": 1.0}, {"text": "I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm. Powered apps and all the things he's just mentioned, but plus plus essentially. 17 Stefan Krawczyk: Cool. 18 Stefan Krawczyk: Yeah, so hopefully, I'll be able to reveal more in the course. But I have a few things with kind of, you know, legal, social and a few other things that need to clear before I officially change my Linkedin status. So I appreciate you guys keeping that under apps until I can do that myself. Thanks. 19 hugo bowne-anderson: And, as we all know, Linkedin is the only reality. So we'll wait for that to happen. Thanks so much, Stefan. I also am super excited to introduce you to Jeff Pidcock, Nathan Danielson, and William Horton, who 20 hugo bowne-anderson: joined for cohort, one as participants students in the course, and they're very serious builders in industry, and I was so blown away I was blown away by the caliber of participants generally, but we invited them back to to kick off our builders in residence program one of the requests from people in cohort. One was just more time building more office hours more, being able to 21 hugo bowne-anderson: answer questions and help people build. So I chatted with Jeff, William and and Nathan about this, and they were so excited to to come back and work with you all on everything you're you're trying to build. So I'd love for them each to say a thing or 2, and we've got them geographically distributed. So for context, this course is really timed for the Us. And Apac. Selfishly, because I'm in Sydney. 22 hugo bowne-anderson: and so Jeff is in Sydney, or just south of Sydney, plus Williams on the East coast in DC. And and Nathan's in Los Angeles. So we've got them geographically distributed so they can organize pods to to work with you all on on things. So, Jeff, maybe you could say a few words about yourself. 23 gp: So Hugo and Hello, everybody. So, as Hugo mentioned, I'm in Sydney Plus. So Sydney's a big place, and I'm in Wollongong, which is south of Sydney, and I'm pretty excited to come on back my background is more as a data scientist than a software engineer. So I really appreciated the opportunity to learn particularly from Stefan in in the 1st round and skill up with Tessie. And I think what I bring to this course is just a bit of bandwidth and capacity to get people helping each other. 24 gp: So you're gonna see for yourself. It's gonna be a lot of material that's thrown at you very quickly. So I think the best thing I can do to help you is to give you some space to explore that, and also some accountability to explore that as well. So I'm looking forward to work with Hugo to propose some sessions maybe some in person stuff in Sydney, which is a beautiful city. And yeah, I'll pass this over to William next to introduce himself. 25 William Horton: Alright, thank you. So I'm William. As as Hugo said, I live in Washington, DC, and I come more from the software engineering side. For the past 4 years I've been working as a machine learning engineer at a company called included Health and for the past 2 have really been focused on building out our Gen. AI platform. 26 William Horton: So if you're particularly interested in Gen. AI for healthcare applications like understanding, explaining health benefits automated scribing for doctors. These are some of the things that I've been working on or supporting. So those are topics that I got excited about. 27 William Horton: I think that's all I can say for now, and I'll pass it on to Nathan. 28 Nathan Danielsen: Everybody, Nathan Danielson, here in Los Angeles, California, Les. 29 Nathan Danielsen: which I mean technically in Pasadena, which is the plus part not really la. But the 1st time I've heard that. So I'm going to adopt that bring it over to to la, so yeah, I've been in the kind of data building space for a while. I started as a software engineer. And then about 5, 6 years ago, kind of moved over into the data science. Ml, range. I've kind of been all over the place. But I'm currently at Carvana right now. Just started a new role. 30 Nathan Danielsen: Focusing on Gen. AI for content generation building building out various domain data pipelines for automotive data, which is very fun.", "metadata": {"timestamp": "Chunk 2", "token_count": 1057, "source": "sentence_chunking", "original_chunk_id": "420c0c9e-14ca-4882-9181-4f0efc3f1c9e", "position": 1, "workshop_id": "WS1", "speaker": "Unknown"}, "id": "WS1_420c0c9e-14ca-4882-9181-4f0efc3f1c9e", "relevance": 1.0}]}}{"question": "How long is the course?", "response": "The course duration is not explicitly mentioned in the provided transcript sections. However, it is indicated that there will be multiple workshops, including a specific mention of Workshop 4 occurring in a couple of days after the first session. \n\nTo get a clearer understanding of the course length, you might want to check the course schedule in the course wiki that was mentioned, as it will provide detailed information about the timeline and structure of the workshops. Additionally, engaging in the Discord community could help clarify any questions regarding the course duration.", "context_info": {"num_chunks": 10, "context_tokens": 8151, "completion_tokens": 103, "embedding_tokens": 15360, "workshops_used": ["WS1", "WS3"], "chunks": [{"text": "What I can tell you is if I actually mean this very seriously If after this course you want to take like evals to the next level and everything we've discussed today um I'm going to take Hamill's course as a student. Hamill and Schreier and Hamill's impressive. I personally think Shreya is even more impressive, but I don't know her as well. I know her pretty well. But I think I'm a bit desensitized to Hamill. I've also he very kindly gave me a code which Look, it's not a cheap course. I want to be the first to say that. But he gave me a code which gives everyone 100 bucks off The code looks like it's go hugoor. Or go home. Um but um if If you're interested in doing a lot more around evaluation and kind of seeing um kind of what state of the art is looking like now. And Schreier builds tools around this, but she researches it like in industry academic collaborations uh at uc berkeley. And she's someone who went back to school to do a PhD UC Berkeley after working at Google Brain and Meta and Facebook and all of these things. So they're super sharp people. And I'll actually share a link to a lot of their resources as well. But I'm having a chat about what I want to get out of this course as well if you're interested. As I said, we are at time, so I will\u2026 record a video of me going through the final eval's notebook What I would love, once again, I'll paste it in the discord channel um if you're able to give feedback seriously if you can even take three minutes now if you haven't already to do so that will help us Just make this course as cool as possible for you all um but I think that's probably it for today. Thank you all for sticking just past the hour. And I'll see you on Discord and we'll see you for Workshop four in a couple of days to get into the wonderful world of observability and debugging in development and production as well. And thank you for all the great engagement and great questions. All right. Thanks, everyone. Thank you, Hik.", "metadata": {"token_count": 455, "source": "sentence_chunking", "timestamp": "Chunk 24", "original_chunk_id": "7b0c99fd-2e0c-4133-9792-4c6ba853b6f6", "workshop_id": "WS3", "speaker": "Unknown", "position": 23}, "id": "WS3_7b0c99fd-2e0c-4133-9792-4c6ba853b6f6", "relevance": 1.0}, {"text": "What I can tell you is if I actually mean this very seriously If after this course you want to take like evals to the next level and everything we've discussed today um I'm going to take Hamill's course as a student. Hamill and Schreier and Hamill's impressive. I personally think Shreya is even more impressive, but I don't know her as well. I know her pretty well. But I think I'm a bit desensitized to Hamill. I've also he very kindly gave me a code which Look, it's not a cheap course. I want to be the first to say that. But he gave me a code which gives everyone 100 bucks off The code looks like it's go hugoor. Or go home. Um but um if If you're interested in doing a lot more around evaluation and kind of seeing um kind of what state of the art is looking like now. And Schreier builds tools around this, but she researches it like in industry academic collaborations uh at uc berkeley. And she's someone who went back to school to do a PhD UC Berkeley after working at Google Brain and Meta and Facebook and all of these things. So they're super sharp people. And I'll actually share a link to a lot of their resources as well. But I'm having a chat about what I want to get out of this course as well if you're interested. As I said, we are at time, so I will\u2026 record a video of me going through the final eval's notebook What I would love, once again, I'll paste it in the discord channel um if you're able to give feedback seriously if you can even take three minutes now if you haven't already to do so that will help us Just make this course as cool as possible for you all um but I think that's probably it for today. Thank you all for sticking just past the hour. And I'll see you on Discord and we'll see you for Workshop four in a couple of days to get into the wonderful world of observability and debugging in development and production as well. And thank you for all the great engagement and great questions. All right. Thanks, everyone. Thank you, Hik.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 24", "token_count": 455, "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "f0cb5dd5-8cee-4238-b32a-7254d77b663f", "position": 23}, "id": "WS3_f0cb5dd5-8cee-4238-b32a-7254d77b663f", "relevance": 1.0}, {"text": "And how we really need to think about building software that's powered by machine learning and Llms which share a lot of similarities, as Stefan pointed out. But instead of you know, writing some specs. 47 hugo bowne-anderson: building, then deploying, and then maybe working on v. 1.2 in a month, or something like that. What we really need to do, due to the non-deterministic nature, the flip-floppy nature and the nature of Llm. Powered software that it brings in a lot of data from the real world. And we don't know how it will behave. 48 hugo bowne-anderson: We need to build quickly and then deploy and then monitor and evaluate and iterate rapidly on this cycle. So that's what this course is all about, and we'll see on the building side. Prompt engineering will be important. Embeddings will be important. Fine tuning business logic. When deploying. We want to think through unit tests and versioning and continuous integration. And those things are some of the very exciting things that Stefan will be talking about in his workshop next week. 49 hugo bowne-anderson: Want to think about tracing and general observability when monitoring and when evaluating. We want to think about not only monitoring how good individual Llm. Calls are, but how they relate to the business metrics that you're you're trying to meet as well. 50 hugo bowne-anderson: So what this course is. It's a practical 1st principles. Approach to building Llm. Powered apps focusing on workflows, iteration and hands-on development. Okay, it's also a space to build production grade systems that scale beyond proof of concepts and demos. It's a deep dive into the non-deterministic nature of these systems, observability, testing, and evaluation, which are the backbones of AI systems. There is a world out there 51 hugo bowne-anderson: called Linkedin, among other things which wants you to believe that the backbone of robust AI systems is the latest and hottest models, multi-agentic frameworks, infinite rag systems conversations about is rag dead because of 10 trillion token, context windows. And this type of stuff 52 hugo bowne-anderson: all of that stuff aside. I mean, there are places for that. That is not what this course is about. Of course we're going to introduce you to a lot of those tools. But this is about the workflow and the non-determinism, the observability, testing and evaluation. It's also an opportunity to learn from practitioners not only the ones who are here and will be here the entire time, but we have a lot of guest guest lectures from people. 53 hugo bowne-anderson: from people I admire deeply. I mean, we invited them because we think they're they're at the top top of the game. People like Hamil Hussein from parlance labs, Eric, Ma. From Moderna, Ravin Kumar from Google Deepmind, who I ravin's talk last time on because he he was on the team that built notebook. Lm. And mariner at Google and Deepmind. Just getting insight into that Ines Montani from from Spacey, who's an old friend, and 54 hugo bowne-anderson: her work's just just so so wonderful. And that's kind of a short list of of a lot of the wonderful guest lectures and talks that we have. So 55 hugo bowne-anderson: I hinted at this. But what this course isn't, it is not a deep dive into every Llm. Tool or trend. Of course we'll be doing. We'll be doing a bunch on agents actually, in the second half of the course, and rag and embeddings and more. But the focus is on workflows, not tools, I honestly think, and I'm happy to chat about those and message about those to be honest. But you will have enough resources on that. What we're teaching is what we've really noticed is a huge gap 56 hugo bowne-anderson: in education around this stuff. This is not a plug and play AI recipe book. They don't even exist. If someone tries to tell you that or sell you that I'd love to hear it. What you'll learn is how to iterate, debug and adapt not just follow step by step. Templates. Okay, once again, this is not some, you know. Hyper, scaled, load balance, multi agent, guide. This course is about getting you moving, not solving every scaling challenge, but really approaching the workflow 57 hugo bowne-anderson: of how to get things in production and then keep iterating on them. 58 hugo bowne-anderson: So I'm gonna put this. This is incredibly important. This is an onboarding form that actually 25 of you have completed already. And I'm so happy with with that number so far.", "metadata": {"workshop_id": "WS1", "speaker": "Unknown", "position": 3, "original_chunk_id": "fa9ad653-c805-4e70-ab36-c785417b4b44", "timestamp": "Chunk 4", "token_count": 1040, "source": "sentence_chunking"}, "id": "WS1_fa9ad653-c805-4e70-ab36-c785417b4b44", "relevance": 1.0}, {"text": "And how we really need to think about building software that's powered by machine learning and Llms which share a lot of similarities, as Stefan pointed out. But instead of you know, writing some specs. 47 hugo bowne-anderson: building, then deploying, and then maybe working on v. 1.2 in a month, or something like that. What we really need to do, due to the non-deterministic nature, the flip-floppy nature and the nature of Llm. Powered software that it brings in a lot of data from the real world. And we don't know how it will behave. 48 hugo bowne-anderson: We need to build quickly and then deploy and then monitor and evaluate and iterate rapidly on this cycle. So that's what this course is all about, and we'll see on the building side. Prompt engineering will be important. Embeddings will be important. Fine tuning business logic. When deploying. We want to think through unit tests and versioning and continuous integration. And those things are some of the very exciting things that Stefan will be talking about in his workshop next week. 49 hugo bowne-anderson: Want to think about tracing and general observability when monitoring and when evaluating. We want to think about not only monitoring how good individual Llm. Calls are, but how they relate to the business metrics that you're you're trying to meet as well. 50 hugo bowne-anderson: So what this course is. It's a practical 1st principles. Approach to building Llm. Powered apps focusing on workflows, iteration and hands-on development. Okay, it's also a space to build production grade systems that scale beyond proof of concepts and demos. It's a deep dive into the non-deterministic nature of these systems, observability, testing, and evaluation, which are the backbones of AI systems. There is a world out there 51 hugo bowne-anderson: called Linkedin, among other things which wants you to believe that the backbone of robust AI systems is the latest and hottest models, multi-agentic frameworks, infinite rag systems conversations about is rag dead because of 10 trillion token, context windows. And this type of stuff 52 hugo bowne-anderson: all of that stuff aside. I mean, there are places for that. That is not what this course is about. Of course we're going to introduce you to a lot of those tools. But this is about the workflow and the non-determinism, the observability, testing and evaluation. It's also an opportunity to learn from practitioners not only the ones who are here and will be here the entire time, but we have a lot of guest guest lectures from people. 53 hugo bowne-anderson: from people I admire deeply. I mean, we invited them because we think they're they're at the top top of the game. People like Hamil Hussein from parlance labs, Eric, Ma. From Moderna, Ravin Kumar from Google Deepmind, who I ravin's talk last time on because he he was on the team that built notebook. Lm. And mariner at Google and Deepmind. Just getting insight into that Ines Montani from from Spacey, who's an old friend, and 54 hugo bowne-anderson: her work's just just so so wonderful. And that's kind of a short list of of a lot of the wonderful guest lectures and talks that we have. So 55 hugo bowne-anderson: I hinted at this. But what this course isn't, it is not a deep dive into every Llm. Tool or trend. Of course we'll be doing. We'll be doing a bunch on agents actually, in the second half of the course, and rag and embeddings and more. But the focus is on workflows, not tools, I honestly think, and I'm happy to chat about those and message about those to be honest. But you will have enough resources on that. What we're teaching is what we've really noticed is a huge gap 56 hugo bowne-anderson: in education around this stuff. This is not a plug and play AI recipe book. They don't even exist. If someone tries to tell you that or sell you that I'd love to hear it. What you'll learn is how to iterate, debug and adapt not just follow step by step. Templates. Okay, once again, this is not some, you know. Hyper, scaled, load balance, multi agent, guide. This course is about getting you moving, not solving every scaling challenge, but really approaching the workflow 57 hugo bowne-anderson: of how to get things in production and then keep iterating on them. 58 hugo bowne-anderson: So I'm gonna put this. This is incredibly important. This is an onboarding form that actually 25 of you have completed already. And I'm so happy with with that number so far.", "metadata": {"source": "sentence_chunking", "position": 3, "workshop_id": "WS1", "speaker": "Unknown", "timestamp": "Chunk 4", "original_chunk_id": "daf7c4dc-e364-4546-853e-48f32a7e6cd2", "token_count": 1040}, "id": "WS1_daf7c4dc-e364-4546-853e-48f32a7e6cd2", "relevance": 1.0}, {"text": "As well learn prompting. They published the prompt report which was really the 1st comprehensive 78 hugo bowne-anderson: report on prompting techniques. They did it with Openai, with a bunch of with 20 different organizations, and they'll be coming to give a talk about prompt engineering in the software development lifecycle and prompt injection attacks. But they're giving. They've got a wonderful educational platform. They'll be giving 3 months of plus access and then prodigy. I don't know if you know prodigy, but it's human in the loop annotation, fine-tuning and Nlp. Workflow building from Innis, Montani and Matthew Honnibal, who built Spacey 79 hugo bowne-anderson: and and maintain it. So that's all to say we have instructions for getting most of them, I think, replicate. For example. Yes, I need. There are a couple which I still need to just follow up on several things, but we'll sort all of those out in the near future, but most of them have instructions on how to do. Get them. So, please. 80 hugo bowne-anderson: Pretty, please do that. 81 hugo bowne-anderson: and I just encourage you all to 82 hugo bowne-anderson: use. I mean, use these tools as much as you can while building during the course, because we're here to support and talk about them and and build with you. I do want to say I do appreciate. Everyone likely has more than full time, jobs and families, and all of these things or subsets of these. So I do understand, particularly in our line of work. Time is an incredibly precious 83 hugo bowne-anderson: resource. So 84 hugo bowne-anderson: all that's to say is, if you don't have much time to build outside, that's totally cool. If you, if all you do is attend these workshops and nothing else. You'll get a huge amount out of the course if you keep building also, and keep in the discords, and then join the office. Hours and pods will set up, you'll get even even more so. I like to think the world's your oyster in proverbial oyster in a lot of ways. So just getting started in terms of 85 hugo bowne-anderson: set up. 86 hugo bowne-anderson: We use several tools, and my goal is to use as few tools as possible. I will rarely adopt a new tool 87 hugo bowne-anderson: if I don't have to. So in the past we used in the first.st So in the 1st cohort, we use Google drive. This is for slides supporting material. This type of stuff we use github and codespaces for code and also codespaces, feel free to clone the repository and do what you want with it, but we want to make sure that you can get set up and up and running as quickly as possible, and I'll introduce you to code spaces later. It's kind of 88 hugo bowne-anderson: Google colab on steroids or Vs code superpowered in the browser. We use discord for the Async Chat announcements, and the Q. And a 89 hugo bowne-anderson: we use zoom for these workshops, and that's what we did in cohort one. Those were the 4 tools we used now. I didn't even quite need to put notion here to. To be honest, because you don't get to. You don't need to see the notion at all. 90 hugo bowne-anderson: But what I decided to build this time was a course, Wiki, which I would very much love you all to keep private. That's really for this course, as with everything in this course I do think. 91 hugo bowne-anderson: You all are paying like a handsome sum of money to to learn and get access to all of this. So please respect that everyone else in the room is is doing the same as well, and and so don't share share too much with with friends, and we'll put out a lot of free content as as well. Having said that this is the course wiki which we'll keep updated. So I will actually spend a couple of minutes 92 hugo bowne-anderson: going through this. So we have. Yep stuff about us when the sessions are little welcome here. This was the setup before week one and anything you haven't done there feel free to do afterwards. There's the course schedule 93 hugo bowne-anderson: and so you click on that, and it goes to another page, and you'll see. Week one workshop one. We're in that right now. Okay, and you'll see there's something called a workshop. One page coming soon. And so that's empty. But what I'm going to do is put the slides there, put the recording there.", "metadata": {"workshop_id": "WS1", "token_count": 1052, "position": 5, "source": "sentence_chunking", "original_chunk_id": "4636c618-f768-4f46-a0a6-6fe122036f92", "timestamp": "Chunk 6", "speaker": "Unknown"}, "id": "WS1_4636c618-f768-4f46-a0a6-6fe122036f92", "relevance": 1.0}, {"text": "As well learn prompting. They published the prompt report which was really the 1st comprehensive 78 hugo bowne-anderson: report on prompting techniques. They did it with Openai, with a bunch of with 20 different organizations, and they'll be coming to give a talk about prompt engineering in the software development lifecycle and prompt injection attacks. But they're giving. They've got a wonderful educational platform. They'll be giving 3 months of plus access and then prodigy. I don't know if you know prodigy, but it's human in the loop annotation, fine-tuning and Nlp. Workflow building from Innis, Montani and Matthew Honnibal, who built Spacey 79 hugo bowne-anderson: and and maintain it. So that's all to say we have instructions for getting most of them, I think, replicate. For example. Yes, I need. There are a couple which I still need to just follow up on several things, but we'll sort all of those out in the near future, but most of them have instructions on how to do. Get them. So, please. 80 hugo bowne-anderson: Pretty, please do that. 81 hugo bowne-anderson: and I just encourage you all to 82 hugo bowne-anderson: use. I mean, use these tools as much as you can while building during the course, because we're here to support and talk about them and and build with you. I do want to say I do appreciate. Everyone likely has more than full time, jobs and families, and all of these things or subsets of these. So I do understand, particularly in our line of work. Time is an incredibly precious 83 hugo bowne-anderson: resource. So 84 hugo bowne-anderson: all that's to say is, if you don't have much time to build outside, that's totally cool. If you, if all you do is attend these workshops and nothing else. You'll get a huge amount out of the course if you keep building also, and keep in the discords, and then join the office. Hours and pods will set up, you'll get even even more so. I like to think the world's your oyster in proverbial oyster in a lot of ways. So just getting started in terms of 85 hugo bowne-anderson: set up. 86 hugo bowne-anderson: We use several tools, and my goal is to use as few tools as possible. I will rarely adopt a new tool 87 hugo bowne-anderson: if I don't have to. So in the past we used in the first.st So in the 1st cohort, we use Google drive. This is for slides supporting material. This type of stuff we use github and codespaces for code and also codespaces, feel free to clone the repository and do what you want with it, but we want to make sure that you can get set up and up and running as quickly as possible, and I'll introduce you to code spaces later. It's kind of 88 hugo bowne-anderson: Google colab on steroids or Vs code superpowered in the browser. We use discord for the Async Chat announcements, and the Q. And a 89 hugo bowne-anderson: we use zoom for these workshops, and that's what we did in cohort one. Those were the 4 tools we used now. I didn't even quite need to put notion here to. To be honest, because you don't get to. You don't need to see the notion at all. 90 hugo bowne-anderson: But what I decided to build this time was a course, Wiki, which I would very much love you all to keep private. That's really for this course, as with everything in this course I do think. 91 hugo bowne-anderson: You all are paying like a handsome sum of money to to learn and get access to all of this. So please respect that everyone else in the room is is doing the same as well, and and so don't share share too much with with friends, and we'll put out a lot of free content as as well. Having said that this is the course wiki which we'll keep updated. So I will actually spend a couple of minutes 92 hugo bowne-anderson: going through this. So we have. Yep stuff about us when the sessions are little welcome here. This was the setup before week one and anything you haven't done there feel free to do afterwards. There's the course schedule 93 hugo bowne-anderson: and so you click on that, and it goes to another page, and you'll see. Week one workshop one. We're in that right now. Okay, and you'll see there's something called a workshop. One page coming soon. And so that's empty. But what I'm going to do is put the slides there, put the recording there.", "metadata": {"timestamp": "Chunk 6", "speaker": "Unknown", "source": "sentence_chunking", "position": 5, "original_chunk_id": "68f221e0-c4c3-4ad0-a098-296aa3fc237e", "token_count": 1052, "workshop_id": "WS1"}, "id": "WS1_68f221e0-c4c3-4ad0-a098-296aa3fc237e", "relevance": 1.0}, {"text": "Well, thank you so much. It's 587 hugo bowne-anderson: we're we're at time. It's it's time to wrap up, but really appreciate your time and hope you travel safe, and don't have to give away too many, too many credits at the at the airport or not. The amount of credits, just the amount of time. Hope you get the chill as well. 588 Philip Kiely: It'll be great. Yeah, definitely hit me. Send me an email. Hit me up for credits. And if you have any remaining questions for my talk. I do have 589 Philip Kiely: a link to the discord. Drop them in the discord, and I'll swing by tomorrow and answer them 590 Philip Kiely: fantastic. And are you at liberty to share your slides in the discord as well cause people love. 591 Philip Kiely: Yeah, I'll drop by and and put those out as well. 592 hugo bowne-anderson: Fantastic. Well, safe travels, my man, and be in touch soon. 593 Philip Kiely: All right. Sounds good. 594 hugo bowne-anderson: Thank you. And everyone. 595 Philip Kiely: Thank you so much for having me. Thanks everyone for your time today. 596 hugo bowne-anderson: Such a pleasure. Thanks, Philip. 597 hugo bowne-anderson: and everyone else. It's it's time to time to wrap up. I just want to give Philip one more round of applause. Great 598 hugo bowne-anderson: It is time to wrap up. We do have 2. As as I mentioned before, we have an just an onboarding form. A lot of you have completed it, but if you're if you haven't, and able to, this will allow us, it'll let us know where you are. What you're interested in, and it will allow us to set up the office hours that that we're going to with our wonderful builders in residence as well. Once again I will share with you 599 hugo bowne-anderson: the feedback form. A bunch of you have already provided a lot of valuable feedback in there, which we'll use to iterate quickly on how we, how we teach and how we approach this course. I'm going to share that in the discord chat also. 600 hugo bowne-anderson: So it's there. But that's it for today, and we'll see you on discord, and we'll see you in 46\u00a0h for the next workshop as well. So thanks everyone, and thanks for a great 1st session..", "metadata": {"speaker": "Unknown", "workshop_id": "WS1", "source": "sentence_chunking", "original_chunk_id": "f92d865a-8de5-4d41-a951-03c819928104", "token_count": 567, "timestamp": "Chunk 30", "position": 29}, "id": "WS1_f92d865a-8de5-4d41-a951-03c819928104", "relevance": 1.0}, {"text": "Well, thank you so much. It's 587 hugo bowne-anderson: we're we're at time. It's it's time to wrap up, but really appreciate your time and hope you travel safe, and don't have to give away too many, too many credits at the at the airport or not. The amount of credits, just the amount of time. Hope you get the chill as well. 588 Philip Kiely: It'll be great. Yeah, definitely hit me. Send me an email. Hit me up for credits. And if you have any remaining questions for my talk. I do have 589 Philip Kiely: a link to the discord. Drop them in the discord, and I'll swing by tomorrow and answer them 590 Philip Kiely: fantastic. And are you at liberty to share your slides in the discord as well cause people love. 591 Philip Kiely: Yeah, I'll drop by and and put those out as well. 592 hugo bowne-anderson: Fantastic. Well, safe travels, my man, and be in touch soon. 593 Philip Kiely: All right. Sounds good. 594 hugo bowne-anderson: Thank you. And everyone. 595 Philip Kiely: Thank you so much for having me. Thanks everyone for your time today. 596 hugo bowne-anderson: Such a pleasure. Thanks, Philip. 597 hugo bowne-anderson: and everyone else. It's it's time to time to wrap up. I just want to give Philip one more round of applause. Great 598 hugo bowne-anderson: It is time to wrap up. We do have 2. As as I mentioned before, we have an just an onboarding form. A lot of you have completed it, but if you're if you haven't, and able to, this will allow us, it'll let us know where you are. What you're interested in, and it will allow us to set up the office hours that that we're going to with our wonderful builders in residence as well. Once again I will share with you 599 hugo bowne-anderson: the feedback form. A bunch of you have already provided a lot of valuable feedback in there, which we'll use to iterate quickly on how we, how we teach and how we approach this course. I'm going to share that in the discord chat also. 600 hugo bowne-anderson: So it's there. But that's it for today, and we'll see you on discord, and we'll see you in 46\u00a0h for the next workshop as well. So thanks everyone, and thanks for a great 1st session..", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "5bf695d0-0fdd-4d21-bfc8-178a3ee97b85", "timestamp": "Chunk 30", "token_count": 567, "position": 29, "workshop_id": "WS1"}, "id": "WS1_5bf695d0-0fdd-4d21-bfc8-178a3ee97b85", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: All right. Welcome everyone. Hugo Bowen Anderson, here so excited to be kicking off workshop. One of building Llm. Powered applications for data, scientists, software engineers, and whoever else wants to get on this rocket ship together, love that. I can see a bunch of friendly faces already. We'll get to this. But 2 hugo bowne-anderson: if you're able to turn your camera on, I, personally would really appreciate it, and I can tell you it makes me a better teacher as well to see a bunch of faces, and to see all the, all the engagement as well, and only if you're able to, of course, and generally microphones off when not speaking as well. 3 hugo bowne-anderson: I'm I'm so excited to have you all here already, seeing what you all are up to in the discord in the forms you filled in really appreciate you filling in the onboarding form as well. We've got a bunch of really serious builders who seem a lot of fun also so really excited to get to know you all 4 hugo bowne-anderson: as well. So my name is Hugo Band Anderson. I've worked in data, science and machine learning, and what now we call AI for far out 15 years now. So I feel a bit old to to be honest. I've worked in building, in education, in consulting and and devrel. Also a lot of work in devrel for open, open source stuff. 5 hugo bowne-anderson: so like my, I think my, my. 6 hugo bowne-anderson: my background really has been in doing scientific research using the Pi data stack that then turned into software stuff. So that's really where I come from. In the past couple of years I've been building a lot with generative AI and helping a lot of other people to build also. 7 hugo bowne-anderson: But that really brings up my my background is in scientific research, in doing stuff, in notebooks, in doing my best to productionize stuff without throwing it over the wall to software engineers, which is one of the reasons I teamed up with with Stefan, who is my co-instructor and colleague and friend here, so I'll just hand it over to Stefan to say a few words. 8 Stefan Krawczyk: Cool. Hi, everyone welcome so name Stefan Craftyk. I'm based in San Francisco. But by my judging by my accent, if you can tell, I'm from New Zealand, not Australia. So 9 Stefan Krawczyk: in terms of yeah, like my background effectively for the last 10 years has been around Mlops. So if you heard the term machine learning operations. That's kind of where I kind of cut a lot of my teeth in terms of helping data scientists and machine learning. Engineers in particular go from prototype to productionization. More recently, in the last few years I've been the CEO of a company called Dagworks, where we've been driving to open source projects called Hamilton and Burr, and the thesis around there is that the same 10 Stefan Krawczyk: problems that you had with Mlops are the same ones you have with Gen. AI, specifically, you need to understand and observe a lot around your data. And once you know more about your data, you can actually improve your whatever apps you're kind of building 11 Stefan Krawczyk: news news for you guys. So like, I didn't introduce myself to the CEO, because, actually, the company's actually been absorbed by a large public company, so 12 Stefan Krawczyk: can't talk about it. But all I can say is, I'm helping them with their kind of agent efforts. And so in particular, yeah, how to get to production. So I will have I'll be on discord mostly, but otherwise I'll have a few sessions throughout this course that will talk a little bit more about some of the production stuff. And so just to let you know this is, you know, this firsthand stuff. 13 Stefan Krawczyk: anything relevant I left out of my intro Hugo, or. 14 hugo bowne-anderson: No, I'll just say on under friend Da. Stefan has told me where he's where he's he's working, and I hope we get to announce it throughout the course. Because he's a really incredible place as far as I'm I'm concerned, supporting cutting edge at agentic work. So it's really very, very exciting. I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm.", "metadata": {"token_count": 1044, "speaker": "Unknown", "position": 0, "workshop_id": "WS1", "original_chunk_id": "d9f254cb-bd4d-431a-a716-d942440a1823", "source": "sentence_chunking", "timestamp": "Chunk 1"}, "id": "WS1_d9f254cb-bd4d-431a-a716-d942440a1823", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: All right. Welcome everyone. Hugo Bowen Anderson, here so excited to be kicking off workshop. One of building Llm. Powered applications for data, scientists, software engineers, and whoever else wants to get on this rocket ship together, love that. I can see a bunch of friendly faces already. We'll get to this. But 2 hugo bowne-anderson: if you're able to turn your camera on, I, personally would really appreciate it, and I can tell you it makes me a better teacher as well to see a bunch of faces, and to see all the, all the engagement as well, and only if you're able to, of course, and generally microphones off when not speaking as well. 3 hugo bowne-anderson: I'm I'm so excited to have you all here already, seeing what you all are up to in the discord in the forms you filled in really appreciate you filling in the onboarding form as well. We've got a bunch of really serious builders who seem a lot of fun also so really excited to get to know you all 4 hugo bowne-anderson: as well. So my name is Hugo Band Anderson. I've worked in data, science and machine learning, and what now we call AI for far out 15 years now. So I feel a bit old to to be honest. I've worked in building, in education, in consulting and and devrel. Also a lot of work in devrel for open, open source stuff. 5 hugo bowne-anderson: so like my, I think my, my. 6 hugo bowne-anderson: my background really has been in doing scientific research using the Pi data stack that then turned into software stuff. So that's really where I come from. In the past couple of years I've been building a lot with generative AI and helping a lot of other people to build also. 7 hugo bowne-anderson: But that really brings up my my background is in scientific research, in doing stuff, in notebooks, in doing my best to productionize stuff without throwing it over the wall to software engineers, which is one of the reasons I teamed up with with Stefan, who is my co-instructor and colleague and friend here, so I'll just hand it over to Stefan to say a few words. 8 Stefan Krawczyk: Cool. Hi, everyone welcome so name Stefan Craftyk. I'm based in San Francisco. But by my judging by my accent, if you can tell, I'm from New Zealand, not Australia. So 9 Stefan Krawczyk: in terms of yeah, like my background effectively for the last 10 years has been around Mlops. So if you heard the term machine learning operations. That's kind of where I kind of cut a lot of my teeth in terms of helping data scientists and machine learning. Engineers in particular go from prototype to productionization. More recently, in the last few years I've been the CEO of a company called Dagworks, where we've been driving to open source projects called Hamilton and Burr, and the thesis around there is that the same 10 Stefan Krawczyk: problems that you had with Mlops are the same ones you have with Gen. AI, specifically, you need to understand and observe a lot around your data. And once you know more about your data, you can actually improve your whatever apps you're kind of building 11 Stefan Krawczyk: news news for you guys. So like, I didn't introduce myself to the CEO, because, actually, the company's actually been absorbed by a large public company, so 12 Stefan Krawczyk: can't talk about it. But all I can say is, I'm helping them with their kind of agent efforts. And so in particular, yeah, how to get to production. So I will have I'll be on discord mostly, but otherwise I'll have a few sessions throughout this course that will talk a little bit more about some of the production stuff. And so just to let you know this is, you know, this firsthand stuff. 13 Stefan Krawczyk: anything relevant I left out of my intro Hugo, or. 14 hugo bowne-anderson: No, I'll just say on under friend Da. Stefan has told me where he's where he's he's working, and I hope we get to announce it throughout the course. Because he's a really incredible place as far as I'm I'm concerned, supporting cutting edge at agentic work. So it's really very, very exciting. I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm.", "metadata": {"speaker": "Unknown", "position": 0, "workshop_id": "WS1", "original_chunk_id": "07f7373a-e1e8-44ed-9d01-87e9150e7bbf", "timestamp": "Chunk 1", "token_count": 1044, "source": "sentence_chunking"}, "id": "WS1_07f7373a-e1e8-44ed-9d01-87e9150e7bbf", "relevance": 1.0}]}}{"question": "How many attended this workshop? I believe Hugo mentioned that at the beginning of the workshop. ", "response": "Hugo did not specify the exact number of attendees at the beginning of the workshop. He mentioned seeing \"a bunch of friendly faces\" but did not provide a specific count. If you need to know the number of attendees, it might be best to check any follow-up communications or reports from the workshop organizers.", "context_info": {"num_chunks": 10, "context_tokens": 7805, "completion_tokens": 62, "embedding_tokens": 15360, "workshops_used": ["WS6", "WS2", "WS1"], "chunks": [{"text": "1 hugo bowne-anderson: Hey, everyone. Welcome to the next session, which is. 2 hugo bowne-anderson: I mean, I I think it's gonna be a lot of fun. It's it's hard to live up to the the energy of of and and 3 hugo bowne-anderson: fun of of Greg, to to be honest. But let me ask are you able to. Whoa, I'm trying to close this little zoom thing. 4 hugo bowne-anderson: Are you all able to see Google slides. 5 hugo bowne-anderson: Fantastic and so this may seem slightly odd, but it will become clear why, for this port for this workshop we're going to have all conversation in the workshop. 6 channel and then, when we have our next special guest, we're going to go back to special guests on on discord. So I appreciate your 6 hugo bowne-anderson: flexibility with with such systems. 7 hugo bowne-anderson: but let me just make sure I've got everything working 8 hugo bowne-anderson: fantastic. So I'm actually incredibly excited about today's workshop. I get excited about all of them. But we're starting to do more with tool use and and moving towards agentic agent workflows, and we'll see what that means soon. I just did want a bit of housekeeping, so learn prompting. You should all, everyone who filled out the form and has given me their email addresses. You should be good good to go, and you can see Constantine had a great time. 9 hugo bowne-anderson: and so we've given full access to all the courses. If you did not already have a learn prompting account, you should have received a welcome email hugging face. We're nearly there. You can see just because, as I've said, this is the 1st time. We're provisioning so many credits from so many different providers for the course. So a lot of it is manual. So I just want to say, we really appreciate your patience with all of this, and 10 hugo bowne-anderson: at hugging faces, doing the work to, to manually do it to every for everyone in the organization. That I that I set up and you've joined. So we're nearly there. 11 hugo bowne-anderson: Appreciate your patience on that couple of other things. 12 hugo bowne-anderson: At the end of this session. I'm going to circulate 3 forms. Okay. Now, I know that may seem a bit too much. If you take a few minutes for each. That would be super helpful. The the 1st one will be, what do you want us to cover next week. Okay, there's a wide array of things we could. We could focus on, such as you know, go deeper into evaluations, and Llm. As a judge, human alignment with Llm. As a judge we could also go down the fine tuning path or more productionization stories. 13 hugo bowne-anderson: we could start looking at more frameworks and traces and spans and you know, prompt versioning. So there are so many things we could cover. I want to get signal from you. All I have said this time do not give, and I think I can figure out how to make the form. Not do this now, but please do not give the same rank to multiple topics. Last time 14 hugo bowne-anderson: a certain proportion of people, said We, we want you to cover all of them, and that gives me no signal on how to make a decision, and I do appreciate the sentiment. But this is in order for me to get information to make decisions around. Then 15 hugo bowne-anderson: on the last day we'll be having part of the workshop will be a demo day where for those people who'd like to demo stuff that they've built. 16 hugo bowne-anderson: I and we all would love to see it. 17 hugo bowne-anderson: I'll circulate that form, and we're just gauging interest. What. We want essentially, to know how many people are interested. I've got a few questions around what topic you'd like to cover, or if there any links like discord threads. And look what I built that we can check out once we've gauged interest. We'll organize timings. A likely format will be 5\u00a0min lightning talks and no live Q. And A. And all discussion can happen on on discord. That's because 18 hugo bowne-anderson: even if 20 people are presenting right. 5\u00a0min time, times 20, it adds up right. And last time we did 19 hugo bowne-anderson: remember, we had like a 4\u00a0h Demo day, which was, which was a lot of fun.", "metadata": {"original_chunk_id": "9419bc3f-1ecf-4e29-8529-4bcba8d4a0bd", "source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 1", "position": 0, "token_count": 1063, "workshop_id": "WS6"}, "id": "WS6_9419bc3f-1ecf-4e29-8529-4bcba8d4a0bd", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: Hey, everyone. Welcome to the next session, which is. 2 hugo bowne-anderson: I mean, I I think it's gonna be a lot of fun. It's it's hard to live up to the the energy of of and and 3 hugo bowne-anderson: fun of of Greg, to to be honest. But let me ask are you able to. Whoa, I'm trying to close this little zoom thing. 4 hugo bowne-anderson: Are you all able to see Google slides. 5 hugo bowne-anderson: Fantastic and so this may seem slightly odd, but it will become clear why, for this port for this workshop we're going to have all conversation in the workshop. 6 channel and then, when we have our next special guest, we're going to go back to special guests on on discord. So I appreciate your 6 hugo bowne-anderson: flexibility with with such systems. 7 hugo bowne-anderson: but let me just make sure I've got everything working 8 hugo bowne-anderson: fantastic. So I'm actually incredibly excited about today's workshop. I get excited about all of them. But we're starting to do more with tool use and and moving towards agentic agent workflows, and we'll see what that means soon. I just did want a bit of housekeeping, so learn prompting. You should all, everyone who filled out the form and has given me their email addresses. You should be good good to go, and you can see Constantine had a great time. 9 hugo bowne-anderson: and so we've given full access to all the courses. If you did not already have a learn prompting account, you should have received a welcome email hugging face. We're nearly there. You can see just because, as I've said, this is the 1st time. We're provisioning so many credits from so many different providers for the course. So a lot of it is manual. So I just want to say, we really appreciate your patience with all of this, and 10 hugo bowne-anderson: at hugging faces, doing the work to, to manually do it to every for everyone in the organization. That I that I set up and you've joined. So we're nearly there. 11 hugo bowne-anderson: Appreciate your patience on that couple of other things. 12 hugo bowne-anderson: At the end of this session. I'm going to circulate 3 forms. Okay. Now, I know that may seem a bit too much. If you take a few minutes for each. That would be super helpful. The the 1st one will be, what do you want us to cover next week. Okay, there's a wide array of things we could. We could focus on, such as you know, go deeper into evaluations, and Llm. As a judge, human alignment with Llm. As a judge we could also go down the fine tuning path or more productionization stories. 13 hugo bowne-anderson: we could start looking at more frameworks and traces and spans and you know, prompt versioning. So there are so many things we could cover. I want to get signal from you. All I have said this time do not give, and I think I can figure out how to make the form. Not do this now, but please do not give the same rank to multiple topics. Last time 14 hugo bowne-anderson: a certain proportion of people, said We, we want you to cover all of them, and that gives me no signal on how to make a decision, and I do appreciate the sentiment. But this is in order for me to get information to make decisions around. Then 15 hugo bowne-anderson: on the last day we'll be having part of the workshop will be a demo day where for those people who'd like to demo stuff that they've built. 16 hugo bowne-anderson: I and we all would love to see it. 17 hugo bowne-anderson: I'll circulate that form, and we're just gauging interest. What. We want essentially, to know how many people are interested. I've got a few questions around what topic you'd like to cover, or if there any links like discord threads. And look what I built that we can check out once we've gauged interest. We'll organize timings. A likely format will be 5\u00a0min lightning talks and no live Q. And A. And all discussion can happen on on discord. That's because 18 hugo bowne-anderson: even if 20 people are presenting right. 5\u00a0min time, times 20, it adds up right. And last time we did 19 hugo bowne-anderson: remember, we had like a 4\u00a0h Demo day, which was, which was a lot of fun.", "metadata": {"token_count": 1063, "original_chunk_id": "49ccb875-0af0-4e42-bdd0-9804128b5cac", "timestamp": "Chunk 1", "speaker": "Unknown", "position": 0, "workshop_id": "WS6", "source": "sentence_chunking"}, "id": "WS6_49ccb875-0af0-4e42-bdd0-9804128b5cac", "relevance": 1.0}, {"text": "So I mean, I had him on a podcast to chat about it as well. This is the one page where the embed just didn't work, for 405 hugo bowne-anderson: whatever reason. 406 hugo bowne-anderson: And also, by the way, oh, yeah, I used sorry I used the new Gpt to create funny covers for everyone. So this is unlisted. So I would appreciate it. If you don't share these videos. These are really, for course, attendees also. 407 hugo bowne-anderson: and as part of the value. But he gave a wonderful talk about what we all, as Llm. Developers need to know about gpus and hardware, so I do encourage you all time permitting to check that out, so he'll be giving a guest talk, and then 408 hugo bowne-anderson: in next week it'll all be about iteration, evaluation, and observability. So we'll be jumping right into systematic evaluation of Llm outputs qualitative and quantitative outputs how to define metrics for success such as relevance, coherence, user satisfaction setting up feedback loops, observability basics, debugging common Llm issues and then scaling observability and tools for production. Monitoring 409 hugo bowne-anderson: Stefan. I don't know if you want to add any flavor with respect to what you will talk about next week. Something I really did appreciate, do appreciate about your work is really thinking about the testing loops in Devon prod. So maybe you want to say a word about that. 410 Stefan Krawczyk: I mean, yeah, I mean the so testing and evaluation. So one of the things I guess we'll dig into over the I guess the the coming workshop is to you know what's the difference. 411 Stefan Krawczyk: in terms of mechanics. There really isn't any just kind of what what you do and what you want to do with it. And so 412 Stefan Krawczyk: in terms of the 1st principles, right. We're really trying to gonna be teaching you, you know, not trying to teach you a particular tool so, and how to do it with that tool, but really the principle behind it. So you can kind of 413 Stefan Krawczyk: take it, and project it to where, however, your place of work kind of does things right. So so that's probably I want to say at a high level, the things to kind of think about is that, yeah, we we're not necessarily going to be very tool specific. We're going to be 1st principle based. And that way it should hopefully be easier for you to then project and like, bring in to wherever you're working, since everybody does software development, something differently. And so which case the emphasis, I think on 1st principles also. Helps here. But 414 Stefan Krawczyk: yeah, any any more. There was that. 415 Stefan Krawczyk: That's I mean, Hugo, cool. 416 hugo bowne-anderson: I love that workshop last time so much. I'm just excited excited for it. 417 hugo bowne-anderson: awesome. I appreciate that. So Philip. I wonder, Philip, are you on the call yet. By any chance. 418 hugo bowne-anderson: Okay, you should be here in a few minutes. But Philip Philip Keeley, who's devrel at base 10 really, really fascinating dude does a lot of technical documentation and technical writing as well as published around that. He'll be giving a talk in a few minutes on moving Llm. Apps from prototype to production with with base 10. Please do not forget to claim your $1,000 free free credit, which I'm so grateful for that I've been able to provide. And you just have a look at all the types of things they have 419 hugo bowne-anderson: on their platform that you can deploy with a couple of clicks. To be honest, I really would love, and I, 420 hugo bowne-anderson: because we have a few minutes now. 421 hugo bowne-anderson: I would actually really love. I have a feedback form 422 hugo bowne-anderson: for each workshop. And so what you'll see here. 423 hugo bowne-anderson: you know you put your email in. How did you rate the pace. What did you like most. What could we improve. Was there anything unclear or confusing. If so, what any other feedback or suggestions. I'm going to put that in the discord right now, and if you wouldn't mind just taking a few minutes now to complete it, there's far more chance of you actually completing it now than there is afterwards. I just statistically. So yeah, let's just take.", "metadata": {"token_count": 1032, "original_chunk_id": "3c286924-e45d-4a6d-9325-d3553c292e18", "workshop_id": "WS1", "source": "sentence_chunking", "speaker": "Unknown", "position": 20, "timestamp": "Chunk 21"}, "id": "WS1_3c286924-e45d-4a6d-9325-d3553c292e18", "relevance": 1.0}, {"text": "So I mean, I had him on a podcast to chat about it as well. This is the one page where the embed just didn't work, for 405 hugo bowne-anderson: whatever reason. 406 hugo bowne-anderson: And also, by the way, oh, yeah, I used sorry I used the new Gpt to create funny covers for everyone. So this is unlisted. So I would appreciate it. If you don't share these videos. These are really, for course, attendees also. 407 hugo bowne-anderson: and as part of the value. But he gave a wonderful talk about what we all, as Llm. Developers need to know about gpus and hardware, so I do encourage you all time permitting to check that out, so he'll be giving a guest talk, and then 408 hugo bowne-anderson: in next week it'll all be about iteration, evaluation, and observability. So we'll be jumping right into systematic evaluation of Llm outputs qualitative and quantitative outputs how to define metrics for success such as relevance, coherence, user satisfaction setting up feedback loops, observability basics, debugging common Llm issues and then scaling observability and tools for production. Monitoring 409 hugo bowne-anderson: Stefan. I don't know if you want to add any flavor with respect to what you will talk about next week. Something I really did appreciate, do appreciate about your work is really thinking about the testing loops in Devon prod. So maybe you want to say a word about that. 410 Stefan Krawczyk: I mean, yeah, I mean the so testing and evaluation. So one of the things I guess we'll dig into over the I guess the the coming workshop is to you know what's the difference. 411 Stefan Krawczyk: in terms of mechanics. There really isn't any just kind of what what you do and what you want to do with it. And so 412 Stefan Krawczyk: in terms of the 1st principles, right. We're really trying to gonna be teaching you, you know, not trying to teach you a particular tool so, and how to do it with that tool, but really the principle behind it. So you can kind of 413 Stefan Krawczyk: take it, and project it to where, however, your place of work kind of does things right. So so that's probably I want to say at a high level, the things to kind of think about is that, yeah, we we're not necessarily going to be very tool specific. We're going to be 1st principle based. And that way it should hopefully be easier for you to then project and like, bring in to wherever you're working, since everybody does software development, something differently. And so which case the emphasis, I think on 1st principles also. Helps here. But 414 Stefan Krawczyk: yeah, any any more. There was that. 415 Stefan Krawczyk: That's I mean, Hugo, cool. 416 hugo bowne-anderson: I love that workshop last time so much. I'm just excited excited for it. 417 hugo bowne-anderson: awesome. I appreciate that. So Philip. I wonder, Philip, are you on the call yet. By any chance. 418 hugo bowne-anderson: Okay, you should be here in a few minutes. But Philip Philip Keeley, who's devrel at base 10 really, really fascinating dude does a lot of technical documentation and technical writing as well as published around that. He'll be giving a talk in a few minutes on moving Llm. Apps from prototype to production with with base 10. Please do not forget to claim your $1,000 free free credit, which I'm so grateful for that I've been able to provide. And you just have a look at all the types of things they have 419 hugo bowne-anderson: on their platform that you can deploy with a couple of clicks. To be honest, I really would love, and I, 420 hugo bowne-anderson: because we have a few minutes now. 421 hugo bowne-anderson: I would actually really love. I have a feedback form 422 hugo bowne-anderson: for each workshop. And so what you'll see here. 423 hugo bowne-anderson: you know you put your email in. How did you rate the pace. What did you like most. What could we improve. Was there anything unclear or confusing. If so, what any other feedback or suggestions. I'm going to put that in the discord right now, and if you wouldn't mind just taking a few minutes now to complete it, there's far more chance of you actually completing it now than there is afterwards. I just statistically. So yeah, let's just take.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "29ec1037-b8bb-4524-ac3e-e309e3276efe", "timestamp": "Chunk 21", "workshop_id": "WS1", "token_count": 1032, "position": 20, "speaker": "Unknown"}, "id": "WS1_29ec1037-b8bb-4524-ac3e-e309e3276efe", "relevance": 1.0}, {"text": "Well, thank you so much. It's 587 hugo bowne-anderson: we're we're at time. It's it's time to wrap up, but really appreciate your time and hope you travel safe, and don't have to give away too many, too many credits at the at the airport or not. The amount of credits, just the amount of time. Hope you get the chill as well. 588 Philip Kiely: It'll be great. Yeah, definitely hit me. Send me an email. Hit me up for credits. And if you have any remaining questions for my talk. I do have 589 Philip Kiely: a link to the discord. Drop them in the discord, and I'll swing by tomorrow and answer them 590 Philip Kiely: fantastic. And are you at liberty to share your slides in the discord as well cause people love. 591 Philip Kiely: Yeah, I'll drop by and and put those out as well. 592 hugo bowne-anderson: Fantastic. Well, safe travels, my man, and be in touch soon. 593 Philip Kiely: All right. Sounds good. 594 hugo bowne-anderson: Thank you. And everyone. 595 Philip Kiely: Thank you so much for having me. Thanks everyone for your time today. 596 hugo bowne-anderson: Such a pleasure. Thanks, Philip. 597 hugo bowne-anderson: and everyone else. It's it's time to time to wrap up. I just want to give Philip one more round of applause. Great 598 hugo bowne-anderson: It is time to wrap up. We do have 2. As as I mentioned before, we have an just an onboarding form. A lot of you have completed it, but if you're if you haven't, and able to, this will allow us, it'll let us know where you are. What you're interested in, and it will allow us to set up the office hours that that we're going to with our wonderful builders in residence as well. Once again I will share with you 599 hugo bowne-anderson: the feedback form. A bunch of you have already provided a lot of valuable feedback in there, which we'll use to iterate quickly on how we, how we teach and how we approach this course. I'm going to share that in the discord chat also. 600 hugo bowne-anderson: So it's there. But that's it for today, and we'll see you on discord, and we'll see you in 46\u00a0h for the next workshop as well. So thanks everyone, and thanks for a great 1st session..", "metadata": {"token_count": 567, "original_chunk_id": "f92d865a-8de5-4d41-a951-03c819928104", "source": "sentence_chunking", "position": 29, "timestamp": "Chunk 30", "speaker": "Unknown", "workshop_id": "WS1"}, "id": "WS1_f92d865a-8de5-4d41-a951-03c819928104", "relevance": 1.0}, {"text": "Well, thank you so much. It's 587 hugo bowne-anderson: we're we're at time. It's it's time to wrap up, but really appreciate your time and hope you travel safe, and don't have to give away too many, too many credits at the at the airport or not. The amount of credits, just the amount of time. Hope you get the chill as well. 588 Philip Kiely: It'll be great. Yeah, definitely hit me. Send me an email. Hit me up for credits. And if you have any remaining questions for my talk. I do have 589 Philip Kiely: a link to the discord. Drop them in the discord, and I'll swing by tomorrow and answer them 590 Philip Kiely: fantastic. And are you at liberty to share your slides in the discord as well cause people love. 591 Philip Kiely: Yeah, I'll drop by and and put those out as well. 592 hugo bowne-anderson: Fantastic. Well, safe travels, my man, and be in touch soon. 593 Philip Kiely: All right. Sounds good. 594 hugo bowne-anderson: Thank you. And everyone. 595 Philip Kiely: Thank you so much for having me. Thanks everyone for your time today. 596 hugo bowne-anderson: Such a pleasure. Thanks, Philip. 597 hugo bowne-anderson: and everyone else. It's it's time to time to wrap up. I just want to give Philip one more round of applause. Great 598 hugo bowne-anderson: It is time to wrap up. We do have 2. As as I mentioned before, we have an just an onboarding form. A lot of you have completed it, but if you're if you haven't, and able to, this will allow us, it'll let us know where you are. What you're interested in, and it will allow us to set up the office hours that that we're going to with our wonderful builders in residence as well. Once again I will share with you 599 hugo bowne-anderson: the feedback form. A bunch of you have already provided a lot of valuable feedback in there, which we'll use to iterate quickly on how we, how we teach and how we approach this course. I'm going to share that in the discord chat also. 600 hugo bowne-anderson: So it's there. But that's it for today, and we'll see you on discord, and we'll see you in 46\u00a0h for the next workshop as well. So thanks everyone, and thanks for a great 1st session..", "metadata": {"timestamp": "Chunk 30", "token_count": 567, "workshop_id": "WS1", "original_chunk_id": "5bf695d0-0fdd-4d21-bfc8-178a3ee97b85", "position": 29, "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS1_5bf695d0-0fdd-4d21-bfc8-178a3ee97b85", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: All right. Welcome everyone. Hugo Bowen Anderson, here so excited to be kicking off workshop. One of building Llm. Powered applications for data, scientists, software engineers, and whoever else wants to get on this rocket ship together, love that. I can see a bunch of friendly faces already. We'll get to this. But 2 hugo bowne-anderson: if you're able to turn your camera on, I, personally would really appreciate it, and I can tell you it makes me a better teacher as well to see a bunch of faces, and to see all the, all the engagement as well, and only if you're able to, of course, and generally microphones off when not speaking as well. 3 hugo bowne-anderson: I'm I'm so excited to have you all here already, seeing what you all are up to in the discord in the forms you filled in really appreciate you filling in the onboarding form as well. We've got a bunch of really serious builders who seem a lot of fun also so really excited to get to know you all 4 hugo bowne-anderson: as well. So my name is Hugo Band Anderson. I've worked in data, science and machine learning, and what now we call AI for far out 15 years now. So I feel a bit old to to be honest. I've worked in building, in education, in consulting and and devrel. Also a lot of work in devrel for open, open source stuff. 5 hugo bowne-anderson: so like my, I think my, my. 6 hugo bowne-anderson: my background really has been in doing scientific research using the Pi data stack that then turned into software stuff. So that's really where I come from. In the past couple of years I've been building a lot with generative AI and helping a lot of other people to build also. 7 hugo bowne-anderson: But that really brings up my my background is in scientific research, in doing stuff, in notebooks, in doing my best to productionize stuff without throwing it over the wall to software engineers, which is one of the reasons I teamed up with with Stefan, who is my co-instructor and colleague and friend here, so I'll just hand it over to Stefan to say a few words. 8 Stefan Krawczyk: Cool. Hi, everyone welcome so name Stefan Craftyk. I'm based in San Francisco. But by my judging by my accent, if you can tell, I'm from New Zealand, not Australia. So 9 Stefan Krawczyk: in terms of yeah, like my background effectively for the last 10 years has been around Mlops. So if you heard the term machine learning operations. That's kind of where I kind of cut a lot of my teeth in terms of helping data scientists and machine learning. Engineers in particular go from prototype to productionization. More recently, in the last few years I've been the CEO of a company called Dagworks, where we've been driving to open source projects called Hamilton and Burr, and the thesis around there is that the same 10 Stefan Krawczyk: problems that you had with Mlops are the same ones you have with Gen. AI, specifically, you need to understand and observe a lot around your data. And once you know more about your data, you can actually improve your whatever apps you're kind of building 11 Stefan Krawczyk: news news for you guys. So like, I didn't introduce myself to the CEO, because, actually, the company's actually been absorbed by a large public company, so 12 Stefan Krawczyk: can't talk about it. But all I can say is, I'm helping them with their kind of agent efforts. And so in particular, yeah, how to get to production. So I will have I'll be on discord mostly, but otherwise I'll have a few sessions throughout this course that will talk a little bit more about some of the production stuff. And so just to let you know this is, you know, this firsthand stuff. 13 Stefan Krawczyk: anything relevant I left out of my intro Hugo, or. 14 hugo bowne-anderson: No, I'll just say on under friend Da. Stefan has told me where he's where he's he's working, and I hope we get to announce it throughout the course. Because he's a really incredible place as far as I'm I'm concerned, supporting cutting edge at agentic work. So it's really very, very exciting. I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm.", "metadata": {"original_chunk_id": "d9f254cb-bd4d-431a-a716-d942440a1823", "source": "sentence_chunking", "token_count": 1044, "workshop_id": "WS1", "speaker": "Unknown", "timestamp": "Chunk 1", "position": 0}, "id": "WS1_d9f254cb-bd4d-431a-a716-d942440a1823", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: All right. Welcome everyone. Hugo Bowen Anderson, here so excited to be kicking off workshop. One of building Llm. Powered applications for data, scientists, software engineers, and whoever else wants to get on this rocket ship together, love that. I can see a bunch of friendly faces already. We'll get to this. But 2 hugo bowne-anderson: if you're able to turn your camera on, I, personally would really appreciate it, and I can tell you it makes me a better teacher as well to see a bunch of faces, and to see all the, all the engagement as well, and only if you're able to, of course, and generally microphones off when not speaking as well. 3 hugo bowne-anderson: I'm I'm so excited to have you all here already, seeing what you all are up to in the discord in the forms you filled in really appreciate you filling in the onboarding form as well. We've got a bunch of really serious builders who seem a lot of fun also so really excited to get to know you all 4 hugo bowne-anderson: as well. So my name is Hugo Band Anderson. I've worked in data, science and machine learning, and what now we call AI for far out 15 years now. So I feel a bit old to to be honest. I've worked in building, in education, in consulting and and devrel. Also a lot of work in devrel for open, open source stuff. 5 hugo bowne-anderson: so like my, I think my, my. 6 hugo bowne-anderson: my background really has been in doing scientific research using the Pi data stack that then turned into software stuff. So that's really where I come from. In the past couple of years I've been building a lot with generative AI and helping a lot of other people to build also. 7 hugo bowne-anderson: But that really brings up my my background is in scientific research, in doing stuff, in notebooks, in doing my best to productionize stuff without throwing it over the wall to software engineers, which is one of the reasons I teamed up with with Stefan, who is my co-instructor and colleague and friend here, so I'll just hand it over to Stefan to say a few words. 8 Stefan Krawczyk: Cool. Hi, everyone welcome so name Stefan Craftyk. I'm based in San Francisco. But by my judging by my accent, if you can tell, I'm from New Zealand, not Australia. So 9 Stefan Krawczyk: in terms of yeah, like my background effectively for the last 10 years has been around Mlops. So if you heard the term machine learning operations. That's kind of where I kind of cut a lot of my teeth in terms of helping data scientists and machine learning. Engineers in particular go from prototype to productionization. More recently, in the last few years I've been the CEO of a company called Dagworks, where we've been driving to open source projects called Hamilton and Burr, and the thesis around there is that the same 10 Stefan Krawczyk: problems that you had with Mlops are the same ones you have with Gen. AI, specifically, you need to understand and observe a lot around your data. And once you know more about your data, you can actually improve your whatever apps you're kind of building 11 Stefan Krawczyk: news news for you guys. So like, I didn't introduce myself to the CEO, because, actually, the company's actually been absorbed by a large public company, so 12 Stefan Krawczyk: can't talk about it. But all I can say is, I'm helping them with their kind of agent efforts. And so in particular, yeah, how to get to production. So I will have I'll be on discord mostly, but otherwise I'll have a few sessions throughout this course that will talk a little bit more about some of the production stuff. And so just to let you know this is, you know, this firsthand stuff. 13 Stefan Krawczyk: anything relevant I left out of my intro Hugo, or. 14 hugo bowne-anderson: No, I'll just say on under friend Da. Stefan has told me where he's where he's he's working, and I hope we get to announce it throughout the course. Because he's a really incredible place as far as I'm I'm concerned, supporting cutting edge at agentic work. So it's really very, very exciting. I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 1", "token_count": 1044, "source": "sentence_chunking", "workshop_id": "WS1", "position": 0, "original_chunk_id": "07f7373a-e1e8-44ed-9d01-87e9150e7bbf"}, "id": "WS1_07f7373a-e1e8-44ed-9d01-87e9150e7bbf", "relevance": 1.0}, {"text": "Awesome. Well, thanks so much. For your wisdom and time and generosity, Charles, and all the credits y'all are giving us as well and I'm so people seem really excited to try out the platform and all the wonderful examples. So I can't wait to see what everyone builds. Yeah. Also, I will say there was a question about like whether you can run modal in the browser. We do have a little playground. That I shared a link to. The other one is like, you can also drop, like, I\u2026 All you need is Python and Modal. So you can like drop into like collab and then just like pip install modal and then start like running commands from there. So those are the two sort of like super low friction ways to get started and try stuff out without needing to like set up your own local virtual environment Yeah, thanks for having me, Hugo. Amazing. Well, thanks, Charles, once again. Absolutely. And everyone, thank you for sticking around a bit longer past the end of workshop time and we'll see you on Discord and we'll announce all the office hours and build a session soon. And see you in the next workshop next week. Thanks, everyone. Thank you. Bye.", "metadata": {"timestamp": "Chunk 14", "position": 13, "original_chunk_id": "a5aa0063-b44d-47dc-a6b8-ffd872542892", "speaker": "Unknown", "token_count": 252, "source": "sentence_chunking", "workshop_id": "WS2"}, "id": "WS2_a5aa0063-b44d-47dc-a6b8-ffd872542892", "relevance": 1.0}, {"text": "Awesome. Well, thanks so much. For your wisdom and time and generosity, Charles, and all the credits y'all are giving us as well and I'm so people seem really excited to try out the platform and all the wonderful examples. So I can't wait to see what everyone builds. Yeah. Also, I will say there was a question about like whether you can run modal in the browser. We do have a little playground. That I shared a link to. The other one is like, you can also drop, like, I\u2026 All you need is Python and Modal. So you can like drop into like collab and then just like pip install modal and then start like running commands from there. So those are the two sort of like super low friction ways to get started and try stuff out without needing to like set up your own local virtual environment Yeah, thanks for having me, Hugo. Amazing. Well, thanks, Charles, once again. Absolutely. And everyone, thank you for sticking around a bit longer past the end of workshop time and we'll see you on Discord and we'll announce all the office hours and build a session soon. And see you in the next workshop next week. Thanks, everyone. Thank you. Bye.", "metadata": {"timestamp": "Chunk 14", "token_count": 252, "original_chunk_id": "3982551c-1870-4812-94b9-3f67a51a077c", "position": 13, "source": "sentence_chunking", "workshop_id": "WS2", "speaker": "Unknown"}, "id": "WS2_3982551c-1870-4812-94b9-3f67a51a077c", "relevance": 1.0}]}}{"question": "How many speakers this workshop has?", "response": "The workshop features two main speakers: Hugo Bowne-Anderson and Stefan Krawczyk. Hugo is the primary instructor, while Stefan is a co-instructor who will also lead a workshop next week. Additionally, there is a guest speaker, Philip Keeley, who gives a talk during the session. \n\nIn summary, there are three speakers involved in this workshop: \n1. Hugo Bowne-Anderson\n2. Stefan Krawczyk\n3. Philip Keeley (guest speaker) \n\nIf you have any further questions about the workshop or its content, feel free to ask!", "context_info": {"num_chunks": 10, "context_tokens": 9357, "completion_tokens": 119, "embedding_tokens": 15360, "workshops_used": ["WS6", "WS3", "WS1"], "chunks": [{"text": "1 hugo bowne-anderson: Hey, everyone. Welcome to the next session, which is. 2 hugo bowne-anderson: I mean, I I think it's gonna be a lot of fun. It's it's hard to live up to the the energy of of and and 3 hugo bowne-anderson: fun of of Greg, to to be honest. But let me ask are you able to. Whoa, I'm trying to close this little zoom thing. 4 hugo bowne-anderson: Are you all able to see Google slides. 5 hugo bowne-anderson: Fantastic and so this may seem slightly odd, but it will become clear why, for this port for this workshop we're going to have all conversation in the workshop. 6 channel and then, when we have our next special guest, we're going to go back to special guests on on discord. So I appreciate your 6 hugo bowne-anderson: flexibility with with such systems. 7 hugo bowne-anderson: but let me just make sure I've got everything working 8 hugo bowne-anderson: fantastic. So I'm actually incredibly excited about today's workshop. I get excited about all of them. But we're starting to do more with tool use and and moving towards agentic agent workflows, and we'll see what that means soon. I just did want a bit of housekeeping, so learn prompting. You should all, everyone who filled out the form and has given me their email addresses. You should be good good to go, and you can see Constantine had a great time. 9 hugo bowne-anderson: and so we've given full access to all the courses. If you did not already have a learn prompting account, you should have received a welcome email hugging face. We're nearly there. You can see just because, as I've said, this is the 1st time. We're provisioning so many credits from so many different providers for the course. So a lot of it is manual. So I just want to say, we really appreciate your patience with all of this, and 10 hugo bowne-anderson: at hugging faces, doing the work to, to manually do it to every for everyone in the organization. That I that I set up and you've joined. So we're nearly there. 11 hugo bowne-anderson: Appreciate your patience on that couple of other things. 12 hugo bowne-anderson: At the end of this session. I'm going to circulate 3 forms. Okay. Now, I know that may seem a bit too much. If you take a few minutes for each. That would be super helpful. The the 1st one will be, what do you want us to cover next week. Okay, there's a wide array of things we could. We could focus on, such as you know, go deeper into evaluations, and Llm. As a judge, human alignment with Llm. As a judge we could also go down the fine tuning path or more productionization stories. 13 hugo bowne-anderson: we could start looking at more frameworks and traces and spans and you know, prompt versioning. So there are so many things we could cover. I want to get signal from you. All I have said this time do not give, and I think I can figure out how to make the form. Not do this now, but please do not give the same rank to multiple topics. Last time 14 hugo bowne-anderson: a certain proportion of people, said We, we want you to cover all of them, and that gives me no signal on how to make a decision, and I do appreciate the sentiment. But this is in order for me to get information to make decisions around. Then 15 hugo bowne-anderson: on the last day we'll be having part of the workshop will be a demo day where for those people who'd like to demo stuff that they've built. 16 hugo bowne-anderson: I and we all would love to see it. 17 hugo bowne-anderson: I'll circulate that form, and we're just gauging interest. What. We want essentially, to know how many people are interested. I've got a few questions around what topic you'd like to cover, or if there any links like discord threads. And look what I built that we can check out once we've gauged interest. We'll organize timings. A likely format will be 5\u00a0min lightning talks and no live Q. And A. And all discussion can happen on on discord. That's because 18 hugo bowne-anderson: even if 20 people are presenting right. 5\u00a0min time, times 20, it adds up right. And last time we did 19 hugo bowne-anderson: remember, we had like a 4\u00a0h Demo day, which was, which was a lot of fun.", "metadata": {"timestamp": "Chunk 1", "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS6", "original_chunk_id": "9419bc3f-1ecf-4e29-8529-4bcba8d4a0bd", "position": 0, "token_count": 1063}, "id": "WS6_9419bc3f-1ecf-4e29-8529-4bcba8d4a0bd", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: Hey, everyone. Welcome to the next session, which is. 2 hugo bowne-anderson: I mean, I I think it's gonna be a lot of fun. It's it's hard to live up to the the energy of of and and 3 hugo bowne-anderson: fun of of Greg, to to be honest. But let me ask are you able to. Whoa, I'm trying to close this little zoom thing. 4 hugo bowne-anderson: Are you all able to see Google slides. 5 hugo bowne-anderson: Fantastic and so this may seem slightly odd, but it will become clear why, for this port for this workshop we're going to have all conversation in the workshop. 6 channel and then, when we have our next special guest, we're going to go back to special guests on on discord. So I appreciate your 6 hugo bowne-anderson: flexibility with with such systems. 7 hugo bowne-anderson: but let me just make sure I've got everything working 8 hugo bowne-anderson: fantastic. So I'm actually incredibly excited about today's workshop. I get excited about all of them. But we're starting to do more with tool use and and moving towards agentic agent workflows, and we'll see what that means soon. I just did want a bit of housekeeping, so learn prompting. You should all, everyone who filled out the form and has given me their email addresses. You should be good good to go, and you can see Constantine had a great time. 9 hugo bowne-anderson: and so we've given full access to all the courses. If you did not already have a learn prompting account, you should have received a welcome email hugging face. We're nearly there. You can see just because, as I've said, this is the 1st time. We're provisioning so many credits from so many different providers for the course. So a lot of it is manual. So I just want to say, we really appreciate your patience with all of this, and 10 hugo bowne-anderson: at hugging faces, doing the work to, to manually do it to every for everyone in the organization. That I that I set up and you've joined. So we're nearly there. 11 hugo bowne-anderson: Appreciate your patience on that couple of other things. 12 hugo bowne-anderson: At the end of this session. I'm going to circulate 3 forms. Okay. Now, I know that may seem a bit too much. If you take a few minutes for each. That would be super helpful. The the 1st one will be, what do you want us to cover next week. Okay, there's a wide array of things we could. We could focus on, such as you know, go deeper into evaluations, and Llm. As a judge, human alignment with Llm. As a judge we could also go down the fine tuning path or more productionization stories. 13 hugo bowne-anderson: we could start looking at more frameworks and traces and spans and you know, prompt versioning. So there are so many things we could cover. I want to get signal from you. All I have said this time do not give, and I think I can figure out how to make the form. Not do this now, but please do not give the same rank to multiple topics. Last time 14 hugo bowne-anderson: a certain proportion of people, said We, we want you to cover all of them, and that gives me no signal on how to make a decision, and I do appreciate the sentiment. But this is in order for me to get information to make decisions around. Then 15 hugo bowne-anderson: on the last day we'll be having part of the workshop will be a demo day where for those people who'd like to demo stuff that they've built. 16 hugo bowne-anderson: I and we all would love to see it. 17 hugo bowne-anderson: I'll circulate that form, and we're just gauging interest. What. We want essentially, to know how many people are interested. I've got a few questions around what topic you'd like to cover, or if there any links like discord threads. And look what I built that we can check out once we've gauged interest. We'll organize timings. A likely format will be 5\u00a0min lightning talks and no live Q. And A. And all discussion can happen on on discord. That's because 18 hugo bowne-anderson: even if 20 people are presenting right. 5\u00a0min time, times 20, it adds up right. And last time we did 19 hugo bowne-anderson: remember, we had like a 4\u00a0h Demo day, which was, which was a lot of fun.", "metadata": {"position": 0, "token_count": 1063, "original_chunk_id": "49ccb875-0af0-4e42-bdd0-9804128b5cac", "workshop_id": "WS6", "speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 1"}, "id": "WS6_49ccb875-0af0-4e42-bdd0-9804128b5cac", "relevance": 1.0}, {"text": "Well, thank you so much. It's 587 hugo bowne-anderson: we're we're at time. It's it's time to wrap up, but really appreciate your time and hope you travel safe, and don't have to give away too many, too many credits at the at the airport or not. The amount of credits, just the amount of time. Hope you get the chill as well. 588 Philip Kiely: It'll be great. Yeah, definitely hit me. Send me an email. Hit me up for credits. And if you have any remaining questions for my talk. I do have 589 Philip Kiely: a link to the discord. Drop them in the discord, and I'll swing by tomorrow and answer them 590 Philip Kiely: fantastic. And are you at liberty to share your slides in the discord as well cause people love. 591 Philip Kiely: Yeah, I'll drop by and and put those out as well. 592 hugo bowne-anderson: Fantastic. Well, safe travels, my man, and be in touch soon. 593 Philip Kiely: All right. Sounds good. 594 hugo bowne-anderson: Thank you. And everyone. 595 Philip Kiely: Thank you so much for having me. Thanks everyone for your time today. 596 hugo bowne-anderson: Such a pleasure. Thanks, Philip. 597 hugo bowne-anderson: and everyone else. It's it's time to time to wrap up. I just want to give Philip one more round of applause. Great 598 hugo bowne-anderson: It is time to wrap up. We do have 2. As as I mentioned before, we have an just an onboarding form. A lot of you have completed it, but if you're if you haven't, and able to, this will allow us, it'll let us know where you are. What you're interested in, and it will allow us to set up the office hours that that we're going to with our wonderful builders in residence as well. Once again I will share with you 599 hugo bowne-anderson: the feedback form. A bunch of you have already provided a lot of valuable feedback in there, which we'll use to iterate quickly on how we, how we teach and how we approach this course. I'm going to share that in the discord chat also. 600 hugo bowne-anderson: So it's there. But that's it for today, and we'll see you on discord, and we'll see you in 46\u00a0h for the next workshop as well. So thanks everyone, and thanks for a great 1st session..", "metadata": {"speaker": "Unknown", "token_count": 567, "timestamp": "Chunk 30", "original_chunk_id": "f92d865a-8de5-4d41-a951-03c819928104", "position": 29, "workshop_id": "WS1", "source": "sentence_chunking"}, "id": "WS1_f92d865a-8de5-4d41-a951-03c819928104", "relevance": 1.0}, {"text": "Well, thank you so much. It's 587 hugo bowne-anderson: we're we're at time. It's it's time to wrap up, but really appreciate your time and hope you travel safe, and don't have to give away too many, too many credits at the at the airport or not. The amount of credits, just the amount of time. Hope you get the chill as well. 588 Philip Kiely: It'll be great. Yeah, definitely hit me. Send me an email. Hit me up for credits. And if you have any remaining questions for my talk. I do have 589 Philip Kiely: a link to the discord. Drop them in the discord, and I'll swing by tomorrow and answer them 590 Philip Kiely: fantastic. And are you at liberty to share your slides in the discord as well cause people love. 591 Philip Kiely: Yeah, I'll drop by and and put those out as well. 592 hugo bowne-anderson: Fantastic. Well, safe travels, my man, and be in touch soon. 593 Philip Kiely: All right. Sounds good. 594 hugo bowne-anderson: Thank you. And everyone. 595 Philip Kiely: Thank you so much for having me. Thanks everyone for your time today. 596 hugo bowne-anderson: Such a pleasure. Thanks, Philip. 597 hugo bowne-anderson: and everyone else. It's it's time to time to wrap up. I just want to give Philip one more round of applause. Great 598 hugo bowne-anderson: It is time to wrap up. We do have 2. As as I mentioned before, we have an just an onboarding form. A lot of you have completed it, but if you're if you haven't, and able to, this will allow us, it'll let us know where you are. What you're interested in, and it will allow us to set up the office hours that that we're going to with our wonderful builders in residence as well. Once again I will share with you 599 hugo bowne-anderson: the feedback form. A bunch of you have already provided a lot of valuable feedback in there, which we'll use to iterate quickly on how we, how we teach and how we approach this course. I'm going to share that in the discord chat also. 600 hugo bowne-anderson: So it's there. But that's it for today, and we'll see you on discord, and we'll see you in 46\u00a0h for the next workshop as well. So thanks everyone, and thanks for a great 1st session..", "metadata": {"position": 29, "timestamp": "Chunk 30", "speaker": "Unknown", "original_chunk_id": "5bf695d0-0fdd-4d21-bfc8-178a3ee97b85", "source": "sentence_chunking", "workshop_id": "WS1", "token_count": 567}, "id": "WS1_5bf695d0-0fdd-4d21-bfc8-178a3ee97b85", "relevance": 1.0}, {"text": "So I mean, I had him on a podcast to chat about it as well. This is the one page where the embed just didn't work, for 405 hugo bowne-anderson: whatever reason. 406 hugo bowne-anderson: And also, by the way, oh, yeah, I used sorry I used the new Gpt to create funny covers for everyone. So this is unlisted. So I would appreciate it. If you don't share these videos. These are really, for course, attendees also. 407 hugo bowne-anderson: and as part of the value. But he gave a wonderful talk about what we all, as Llm. Developers need to know about gpus and hardware, so I do encourage you all time permitting to check that out, so he'll be giving a guest talk, and then 408 hugo bowne-anderson: in next week it'll all be about iteration, evaluation, and observability. So we'll be jumping right into systematic evaluation of Llm outputs qualitative and quantitative outputs how to define metrics for success such as relevance, coherence, user satisfaction setting up feedback loops, observability basics, debugging common Llm issues and then scaling observability and tools for production. Monitoring 409 hugo bowne-anderson: Stefan. I don't know if you want to add any flavor with respect to what you will talk about next week. Something I really did appreciate, do appreciate about your work is really thinking about the testing loops in Devon prod. So maybe you want to say a word about that. 410 Stefan Krawczyk: I mean, yeah, I mean the so testing and evaluation. So one of the things I guess we'll dig into over the I guess the the coming workshop is to you know what's the difference. 411 Stefan Krawczyk: in terms of mechanics. There really isn't any just kind of what what you do and what you want to do with it. And so 412 Stefan Krawczyk: in terms of the 1st principles, right. We're really trying to gonna be teaching you, you know, not trying to teach you a particular tool so, and how to do it with that tool, but really the principle behind it. So you can kind of 413 Stefan Krawczyk: take it, and project it to where, however, your place of work kind of does things right. So so that's probably I want to say at a high level, the things to kind of think about is that, yeah, we we're not necessarily going to be very tool specific. We're going to be 1st principle based. And that way it should hopefully be easier for you to then project and like, bring in to wherever you're working, since everybody does software development, something differently. And so which case the emphasis, I think on 1st principles also. Helps here. But 414 Stefan Krawczyk: yeah, any any more. There was that. 415 Stefan Krawczyk: That's I mean, Hugo, cool. 416 hugo bowne-anderson: I love that workshop last time so much. I'm just excited excited for it. 417 hugo bowne-anderson: awesome. I appreciate that. So Philip. I wonder, Philip, are you on the call yet. By any chance. 418 hugo bowne-anderson: Okay, you should be here in a few minutes. But Philip Philip Keeley, who's devrel at base 10 really, really fascinating dude does a lot of technical documentation and technical writing as well as published around that. He'll be giving a talk in a few minutes on moving Llm. Apps from prototype to production with with base 10. Please do not forget to claim your $1,000 free free credit, which I'm so grateful for that I've been able to provide. And you just have a look at all the types of things they have 419 hugo bowne-anderson: on their platform that you can deploy with a couple of clicks. To be honest, I really would love, and I, 420 hugo bowne-anderson: because we have a few minutes now. 421 hugo bowne-anderson: I would actually really love. I have a feedback form 422 hugo bowne-anderson: for each workshop. And so what you'll see here. 423 hugo bowne-anderson: you know you put your email in. How did you rate the pace. What did you like most. What could we improve. Was there anything unclear or confusing. If so, what any other feedback or suggestions. I'm going to put that in the discord right now, and if you wouldn't mind just taking a few minutes now to complete it, there's far more chance of you actually completing it now than there is afterwards. I just statistically. So yeah, let's just take.", "metadata": {"position": 20, "workshop_id": "WS1", "timestamp": "Chunk 21", "source": "sentence_chunking", "token_count": 1032, "speaker": "Unknown", "original_chunk_id": "3c286924-e45d-4a6d-9325-d3553c292e18"}, "id": "WS1_3c286924-e45d-4a6d-9325-d3553c292e18", "relevance": 1.0}, {"text": "So I mean, I had him on a podcast to chat about it as well. This is the one page where the embed just didn't work, for 405 hugo bowne-anderson: whatever reason. 406 hugo bowne-anderson: And also, by the way, oh, yeah, I used sorry I used the new Gpt to create funny covers for everyone. So this is unlisted. So I would appreciate it. If you don't share these videos. These are really, for course, attendees also. 407 hugo bowne-anderson: and as part of the value. But he gave a wonderful talk about what we all, as Llm. Developers need to know about gpus and hardware, so I do encourage you all time permitting to check that out, so he'll be giving a guest talk, and then 408 hugo bowne-anderson: in next week it'll all be about iteration, evaluation, and observability. So we'll be jumping right into systematic evaluation of Llm outputs qualitative and quantitative outputs how to define metrics for success such as relevance, coherence, user satisfaction setting up feedback loops, observability basics, debugging common Llm issues and then scaling observability and tools for production. Monitoring 409 hugo bowne-anderson: Stefan. I don't know if you want to add any flavor with respect to what you will talk about next week. Something I really did appreciate, do appreciate about your work is really thinking about the testing loops in Devon prod. So maybe you want to say a word about that. 410 Stefan Krawczyk: I mean, yeah, I mean the so testing and evaluation. So one of the things I guess we'll dig into over the I guess the the coming workshop is to you know what's the difference. 411 Stefan Krawczyk: in terms of mechanics. There really isn't any just kind of what what you do and what you want to do with it. And so 412 Stefan Krawczyk: in terms of the 1st principles, right. We're really trying to gonna be teaching you, you know, not trying to teach you a particular tool so, and how to do it with that tool, but really the principle behind it. So you can kind of 413 Stefan Krawczyk: take it, and project it to where, however, your place of work kind of does things right. So so that's probably I want to say at a high level, the things to kind of think about is that, yeah, we we're not necessarily going to be very tool specific. We're going to be 1st principle based. And that way it should hopefully be easier for you to then project and like, bring in to wherever you're working, since everybody does software development, something differently. And so which case the emphasis, I think on 1st principles also. Helps here. But 414 Stefan Krawczyk: yeah, any any more. There was that. 415 Stefan Krawczyk: That's I mean, Hugo, cool. 416 hugo bowne-anderson: I love that workshop last time so much. I'm just excited excited for it. 417 hugo bowne-anderson: awesome. I appreciate that. So Philip. I wonder, Philip, are you on the call yet. By any chance. 418 hugo bowne-anderson: Okay, you should be here in a few minutes. But Philip Philip Keeley, who's devrel at base 10 really, really fascinating dude does a lot of technical documentation and technical writing as well as published around that. He'll be giving a talk in a few minutes on moving Llm. Apps from prototype to production with with base 10. Please do not forget to claim your $1,000 free free credit, which I'm so grateful for that I've been able to provide. And you just have a look at all the types of things they have 419 hugo bowne-anderson: on their platform that you can deploy with a couple of clicks. To be honest, I really would love, and I, 420 hugo bowne-anderson: because we have a few minutes now. 421 hugo bowne-anderson: I would actually really love. I have a feedback form 422 hugo bowne-anderson: for each workshop. And so what you'll see here. 423 hugo bowne-anderson: you know you put your email in. How did you rate the pace. What did you like most. What could we improve. Was there anything unclear or confusing. If so, what any other feedback or suggestions. I'm going to put that in the discord right now, and if you wouldn't mind just taking a few minutes now to complete it, there's far more chance of you actually completing it now than there is afterwards. I just statistically. So yeah, let's just take.", "metadata": {"position": 20, "workshop_id": "WS1", "source": "sentence_chunking", "original_chunk_id": "29ec1037-b8bb-4524-ac3e-e309e3276efe", "speaker": "Unknown", "token_count": 1032, "timestamp": "Chunk 21"}, "id": "WS1_29ec1037-b8bb-4524-ac3e-e309e3276efe", "relevance": 1.0}, {"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"position": 1, "source": "sentence_chunking", "original_chunk_id": "5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "timestamp": "Chunk 2", "workshop_id": "WS3", "token_count": 1068, "speaker": "Unknown"}, "id": "WS3_5a7eef47-b7e8-4056-9e9b-fe5d8b18d1f2", "relevance": 1.0}, {"text": "I try to find a middle ground and we all try to find a middle ground. And what we can't occupy in that middle ground is solved on Discord and in the Builders Club. So the Discord and the Builders Club are opportunities for you to get really, really tailored, personalized feedback. I need to, in delivering a workshop for 60 of us in the end. I need to find something in the middle and it can't be so personalized. But the rest we really want to be. So definitely keep me in check with respect to pace. And please do keep giving feedback. But I want to give you a bit more clarity around that. Great feedback on posting the recordings on YouTube would make it easier to watch them later. We'll definitely do that. We can't necessarily do that in real time. And what I'm actually going to do is I'm going to chop up the videos to put them on YouTube so that, for example. We would have the workshop and then Charles Fry's guest talk. After that, a different YouTube videos. So we're going to put a bit of production quality into it, not too much But that's why we don't have them up yet. So in the meantime, please do rewatch them on Zoom. Feel free to download them from Zoom as well and you can do What you'd like with them. Wonderful feedback here about any more recommended resources or links for deep dives on any specific pieces. I do appreciate that in Discord, a lot of links are shared. What I'm going to do in the course wiki on the website is start compiling a list of resources. And maybe I'll even, we've got a channel in the Discord called Resources that's outside this cock, but maybe we'll create a course cohort specific one. An April specific one okay Quick update on credits. Everyone. Should feel liberated to get there. But if you haven't got your base 10 modal or prodigy from spacey people credits yet um the ball is in your court, okay. All the instructions on how to do that are in there. Please ask the help channel if you can't figure it out. All the other ones should have sorted out by end of week. There were questions around how long they last and when we want you to claim them. We want you to claim all of them by the end of the course. Nearly all of them last forever, but I'm clarifying that with some vendors, but I know nearly all of them do now. And I'll report back on that soon. But yeah, we're getting the others soon. And just to be clear. I'd love to have the credit process totally automated, but it does involve you completing forms, me sending the spreadsheets to people at companies. You know, people at Google don't necessarily such bandwidth. So they batch these things. So I just want to say I really appreciate your patience on all of this and i do understand the thirst and hunger for getting your sweet, sweet compute credits. Finally, Builders Clubs this week. So we've got US Eastern. At 7 p.m. On April 17. Us Pacific\u2026 I think, Nathan, you're going to do it around the same time as Eastern now, is that correct. No, so I don't want to\u2026 US Eastern, I kind of want to join that. So we're going to do pacific time Builders Club on Friday. And then I'll set another poll. We can do it 4 p.m. Or 5 p.m. On Friday. Because I want to join on the Eastern and hang out with everybody there too. Okay. Amazing. So Pacific Time Builders Club is on Friday. And I think what we're going to do all our builders in residence clearly incredibly generous with their time and expertise. Nathan would never say this. He did put a poll out and people responded with a certain time And so he did that time and no one turned up. So we're trying to figure out ways around that. So one idea we're having is maybe 24 hours before they'll post in the channel, give me a thumb up if you're going to join. And if people do so, then it will be hosted. But any ideas for, you know, this is the first time we're doing something, this much fun. And we want to give freedom. We don't necessarily want to send calendar invites to everyone and have commitments and that type of stuff. But it'd be nice to get a litmus test. For who's going to turn up where. So all that having been said, I do want to give a quick recap of where we're up to now. So in workshop one, we introduced the generative AI software development lifecycle. We built an MVP. With Gradio, Lama Index, OpenAI, Gemini APIs, SQLite, and Dataset, right. We discussed escaping proof of concept purgatory and laid the foundation for building robust LLM powered applications. Then in workshop two, we deployed the MVP on Modal. Don't forget to claim your credits if you haven't already, we explored prompt engineering, API tuning, and multi-vendor APIs, Gemini, OpenAI, Anthropy. We didn't actually do Grok.", "metadata": {"token_count": 1068, "source": "sentence_chunking", "workshop_id": "WS3", "timestamp": "Chunk 2", "original_chunk_id": "8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "position": 1, "speaker": "Unknown"}, "id": "WS3_8c975d9c-938b-4914-91c2-a9fec5fd9fb0", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: All right. Welcome everyone. Hugo Bowen Anderson, here so excited to be kicking off workshop. One of building Llm. Powered applications for data, scientists, software engineers, and whoever else wants to get on this rocket ship together, love that. I can see a bunch of friendly faces already. We'll get to this. But 2 hugo bowne-anderson: if you're able to turn your camera on, I, personally would really appreciate it, and I can tell you it makes me a better teacher as well to see a bunch of faces, and to see all the, all the engagement as well, and only if you're able to, of course, and generally microphones off when not speaking as well. 3 hugo bowne-anderson: I'm I'm so excited to have you all here already, seeing what you all are up to in the discord in the forms you filled in really appreciate you filling in the onboarding form as well. We've got a bunch of really serious builders who seem a lot of fun also so really excited to get to know you all 4 hugo bowne-anderson: as well. So my name is Hugo Band Anderson. I've worked in data, science and machine learning, and what now we call AI for far out 15 years now. So I feel a bit old to to be honest. I've worked in building, in education, in consulting and and devrel. Also a lot of work in devrel for open, open source stuff. 5 hugo bowne-anderson: so like my, I think my, my. 6 hugo bowne-anderson: my background really has been in doing scientific research using the Pi data stack that then turned into software stuff. So that's really where I come from. In the past couple of years I've been building a lot with generative AI and helping a lot of other people to build also. 7 hugo bowne-anderson: But that really brings up my my background is in scientific research, in doing stuff, in notebooks, in doing my best to productionize stuff without throwing it over the wall to software engineers, which is one of the reasons I teamed up with with Stefan, who is my co-instructor and colleague and friend here, so I'll just hand it over to Stefan to say a few words. 8 Stefan Krawczyk: Cool. Hi, everyone welcome so name Stefan Craftyk. I'm based in San Francisco. But by my judging by my accent, if you can tell, I'm from New Zealand, not Australia. So 9 Stefan Krawczyk: in terms of yeah, like my background effectively for the last 10 years has been around Mlops. So if you heard the term machine learning operations. That's kind of where I kind of cut a lot of my teeth in terms of helping data scientists and machine learning. Engineers in particular go from prototype to productionization. More recently, in the last few years I've been the CEO of a company called Dagworks, where we've been driving to open source projects called Hamilton and Burr, and the thesis around there is that the same 10 Stefan Krawczyk: problems that you had with Mlops are the same ones you have with Gen. AI, specifically, you need to understand and observe a lot around your data. And once you know more about your data, you can actually improve your whatever apps you're kind of building 11 Stefan Krawczyk: news news for you guys. So like, I didn't introduce myself to the CEO, because, actually, the company's actually been absorbed by a large public company, so 12 Stefan Krawczyk: can't talk about it. But all I can say is, I'm helping them with their kind of agent efforts. And so in particular, yeah, how to get to production. So I will have I'll be on discord mostly, but otherwise I'll have a few sessions throughout this course that will talk a little bit more about some of the production stuff. And so just to let you know this is, you know, this firsthand stuff. 13 Stefan Krawczyk: anything relevant I left out of my intro Hugo, or. 14 hugo bowne-anderson: No, I'll just say on under friend Da. Stefan has told me where he's where he's he's working, and I hope we get to announce it throughout the course. Because he's a really incredible place as far as I'm I'm concerned, supporting cutting edge at agentic work. So it's really very, very exciting. I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm.", "metadata": {"timestamp": "Chunk 1", "position": 0, "speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS1", "original_chunk_id": "d9f254cb-bd4d-431a-a716-d942440a1823", "token_count": 1044}, "id": "WS1_d9f254cb-bd4d-431a-a716-d942440a1823", "relevance": 1.0}, {"text": "1 hugo bowne-anderson: All right. Welcome everyone. Hugo Bowen Anderson, here so excited to be kicking off workshop. One of building Llm. Powered applications for data, scientists, software engineers, and whoever else wants to get on this rocket ship together, love that. I can see a bunch of friendly faces already. We'll get to this. But 2 hugo bowne-anderson: if you're able to turn your camera on, I, personally would really appreciate it, and I can tell you it makes me a better teacher as well to see a bunch of faces, and to see all the, all the engagement as well, and only if you're able to, of course, and generally microphones off when not speaking as well. 3 hugo bowne-anderson: I'm I'm so excited to have you all here already, seeing what you all are up to in the discord in the forms you filled in really appreciate you filling in the onboarding form as well. We've got a bunch of really serious builders who seem a lot of fun also so really excited to get to know you all 4 hugo bowne-anderson: as well. So my name is Hugo Band Anderson. I've worked in data, science and machine learning, and what now we call AI for far out 15 years now. So I feel a bit old to to be honest. I've worked in building, in education, in consulting and and devrel. Also a lot of work in devrel for open, open source stuff. 5 hugo bowne-anderson: so like my, I think my, my. 6 hugo bowne-anderson: my background really has been in doing scientific research using the Pi data stack that then turned into software stuff. So that's really where I come from. In the past couple of years I've been building a lot with generative AI and helping a lot of other people to build also. 7 hugo bowne-anderson: But that really brings up my my background is in scientific research, in doing stuff, in notebooks, in doing my best to productionize stuff without throwing it over the wall to software engineers, which is one of the reasons I teamed up with with Stefan, who is my co-instructor and colleague and friend here, so I'll just hand it over to Stefan to say a few words. 8 Stefan Krawczyk: Cool. Hi, everyone welcome so name Stefan Craftyk. I'm based in San Francisco. But by my judging by my accent, if you can tell, I'm from New Zealand, not Australia. So 9 Stefan Krawczyk: in terms of yeah, like my background effectively for the last 10 years has been around Mlops. So if you heard the term machine learning operations. That's kind of where I kind of cut a lot of my teeth in terms of helping data scientists and machine learning. Engineers in particular go from prototype to productionization. More recently, in the last few years I've been the CEO of a company called Dagworks, where we've been driving to open source projects called Hamilton and Burr, and the thesis around there is that the same 10 Stefan Krawczyk: problems that you had with Mlops are the same ones you have with Gen. AI, specifically, you need to understand and observe a lot around your data. And once you know more about your data, you can actually improve your whatever apps you're kind of building 11 Stefan Krawczyk: news news for you guys. So like, I didn't introduce myself to the CEO, because, actually, the company's actually been absorbed by a large public company, so 12 Stefan Krawczyk: can't talk about it. But all I can say is, I'm helping them with their kind of agent efforts. And so in particular, yeah, how to get to production. So I will have I'll be on discord mostly, but otherwise I'll have a few sessions throughout this course that will talk a little bit more about some of the production stuff. And so just to let you know this is, you know, this firsthand stuff. 13 Stefan Krawczyk: anything relevant I left out of my intro Hugo, or. 14 hugo bowne-anderson: No, I'll just say on under friend Da. Stefan has told me where he's where he's he's working, and I hope we get to announce it throughout the course. Because he's a really incredible place as far as I'm I'm concerned, supporting cutting edge at agentic work. So it's really very, very exciting. I will add that 15 hugo bowne-anderson: Stephan will be. He'll be giving a full workshop next week, and it's 1 of my favorite workshops in in this course, actually, because I get to learn so much from it about 16 hugo bowne-anderson: testing and running tests, both in Dev and and production for Llm.", "metadata": {"position": 0, "source": "sentence_chunking", "token_count": 1044, "workshop_id": "WS1", "speaker": "Unknown", "timestamp": "Chunk 1", "original_chunk_id": "07f7373a-e1e8-44ed-9d01-87e9150e7bbf"}, "id": "WS1_07f7373a-e1e8-44ed-9d01-87e9150e7bbf", "relevance": 1.0}]}}{"question": "I mean synthetic data generated questions", "response": "The underlying question seems to be about how to generate synthetic data, specifically questions, for evaluating models or applications.\n\nTo generate synthetic questions effectively, you can follow these steps:\n\n1. **Define Personas and Scenarios**: Start by identifying the target users (personas) and the scenarios they might encounter. This helps tailor the questions to be relevant and realistic.\n\n2. **Use a Language Model**: Implement a language model (like OpenAI's) to generate questions based on the defined personas and scenarios. For example, you can create a prompt that instructs the model to generate questions related to a workshop or specific topic.\n\n3. **Iterate and Refine**: Run the generation process multiple times to produce a variety of questions. This can be done in a loop to create a larger dataset of synthetic questions.\n\n4. **Hand Label and Analyze**: Once you have the generated questions, you can hand label them for quality and relevance. This involves assessing whether the questions make sense and align with the intended use case.\n\n5. **Log and Classify**: Store the generated questions along with their labels in a structured format (like a spreadsheet or JSON). Classify any issues or failure modes encountered during the generation process.\n\n6. **Use Feedback for Improvement**: Analyze the labeled data to identify patterns or common issues. Use this feedback to refine your prompts and improve the quality of future question generations.\n\nBy following these steps, you can create a robust process for generating synthetic questions that can be used for model evaluation and testing.\n\nIn summary, leverage language models to generate questions based on defined personas and scenarios, iterate on the process, and analyze the results to improve quality. This approach not only enhances the evaluation process but also provides a scalable way to create relevant data for testing.", "context_info": {"num_chunks": 10, "context_tokens": 10524, "completion_tokens": 359, "embedding_tokens": 15360, "workshops_used": ["WS6", "WS3", "WS4"], "chunks": [{"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"speaker": "Unknown", "workshop_id": "WS3", "timestamp": "Chunk 9", "position": 8, "token_count": 1067, "source": "sentence_chunking", "original_chunk_id": "6b9e78cb-cf86-45a1-a843-8a3c423b0a63"}, "id": "WS3_6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"speaker": "Unknown", "token_count": 1067, "position": 8, "timestamp": "Chunk 9", "source": "sentence_chunking", "workshop_id": "WS3", "original_chunk_id": "9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39"}, "id": "WS3_9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"token_count": 1062, "source": "sentence_chunking", "speaker": "Unknown", "position": 18, "original_chunk_id": "0023cc64-b3ca-486f-9dd8-dbbc987bf306", "workshop_id": "WS6", "timestamp": "Chunk 19"}, "id": "WS6_0023cc64-b3ca-486f-9dd8-dbbc987bf306", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"source": "sentence_chunking", "workshop_id": "WS6", "timestamp": "Chunk 19", "position": 18, "original_chunk_id": "981ee013-55da-46a9-a83e-dd1b1322ac44", "speaker": "Unknown", "token_count": 1062}, "id": "WS6_981ee013-55da-46a9-a83e-dd1b1322ac44", "relevance": 1.0}, {"text": "So maybe it's something on our prompt iteration we need to kind of dig into, right. And so How would I do the prompt iteration. Well, this is where you know i we could set up some expectations, right. We can then go back to the logic file. We could do our prompt engineering here and then rerun this test. And so this is where we can back and forth do it. And then once it's done right we actually have some some record of expectation that we have encoded. That we can then systematize. So rather than it being in a notebook, this is then potentially more easy to productionize, so to speak. To cheat a little, I have two functions. I have an already iterated on function that largely actually I want to thank Hugo for. And so generally, I think what happens, it's either your prompts become more specific Or you try to shorten what they do. And so in this case here, I've included a lot more context to what to kind of extract. And so I can\u2026 run this test again right with this say new prompt iteration And we should hopefully see that things pass. Cool. All right. It did. Let's see how stable this is. Right. Cool. Awesome. So this table. Now, so that shows you just basic one iteration, right. Now, obviously, so how do we incorporate some more data. And so this is where PyTest parameterize could be used to come in and so So here I have it in line, but this this is where it could it could come from a data set or a file or something that you're curating but effectively here, I now have my expectations and, you know, ones for kind of hugo's profile. And so I can\u2026 now write a more generic function that just iterates over different data, right. Using partis might prioritize. To get around the side, this is where we are using the PyTestHarvest plugin. What it does is it gives us a little fixture, what's called a fixture. So I'm not going to dive into that, but fixtures are a thing in PyTest that you can kind of use to do a lot of customization. What it means is it can give us back an object where we don't actually have to do asserts here, but we can capture information we need in a key value manner. And then at a later point, like assess everything altogether. And so how this works is that these functions are going to be So this function is going to access once for a feature of mine and Hugo's profiles. We're going to capture the inputs, outputs. Each kind of field match. And then if you only assert we're going to have is like, we want to make sure that we always get the name correct, right. So this is a choice we can make. Do we want to, you can use the search to fail hard or you can capture things in a software and then process them later to then decide whether to fail an aggregate or not and so with podcast harvest, this is where this test prep results function comes in. It gives us a, we can ask for a pandas data frame that has uh all this information. That was captured in these tests that were using the results back. And so\u2026 Let me just walk through it without scrolling too quickly. So\u2026 This will give us a data frame. We can then compute or add columns to this data frame. So if you come from the data science side, like this is where you know you can start to do some of those things you do with a data frame. We can, you know, we can compute field accuracy based on the different fields right This is where we can kind of, you know. Print what we want We can compute different measures, sorry, we compute different metrics from the things that we have measured And this is where like in aggregate, right, we can then choose to you know assert like fail like hey Fail the pipeline or fail the test completely or not, or just, you know, log some warnings This is where we could also in this function you know upload the data to Google Sheets and you know kick off some other process but Just to give you a mental picture of kind of how this works. Yep. Hey, Stefan, just one look. We've had a couple of related questions in the chat, which is\u2026 The answer is implicit in everything you're talking about, but it's around, but I think it's worthwhile being explicit about it. How do we use testing, deterministic tests, when the stochastic, when the nature of LLMs and this type of software is stochastic. Yep. And part of the answer that William spoke to in the thread is around running it several times and getting a distribution of results, right. But if you could speak to that explicitly, and I know you intend on doing so but Yep. Yeah, yeah. I mean, that's a good question. So this is where one of the things is like, should things be 100%.", "metadata": {"source": "sentence_chunking", "token_count": 1030, "speaker": "Unknown", "position": 6, "original_chunk_id": "c130ba54-7ba7-41e7-971f-8646ac141a8f", "timestamp": "Chunk 7", "workshop_id": "WS4"}, "id": "WS4_c130ba54-7ba7-41e7-971f-8646ac141a8f", "relevance": 1.0}, {"text": "So maybe it's something on our prompt iteration we need to kind of dig into, right. And so How would I do the prompt iteration. Well, this is where you know i we could set up some expectations, right. We can then go back to the logic file. We could do our prompt engineering here and then rerun this test. And so this is where we can back and forth do it. And then once it's done right we actually have some some record of expectation that we have encoded. That we can then systematize. So rather than it being in a notebook, this is then potentially more easy to productionize, so to speak. To cheat a little, I have two functions. I have an already iterated on function that largely actually I want to thank Hugo for. And so generally, I think what happens, it's either your prompts become more specific Or you try to shorten what they do. And so in this case here, I've included a lot more context to what to kind of extract. And so I can\u2026 run this test again right with this say new prompt iteration And we should hopefully see that things pass. Cool. All right. It did. Let's see how stable this is. Right. Cool. Awesome. So this table. Now, so that shows you just basic one iteration, right. Now, obviously, so how do we incorporate some more data. And so this is where PyTest parameterize could be used to come in and so So here I have it in line, but this this is where it could it could come from a data set or a file or something that you're curating but effectively here, I now have my expectations and, you know, ones for kind of hugo's profile. And so I can\u2026 now write a more generic function that just iterates over different data, right. Using partis might prioritize. To get around the side, this is where we are using the PyTestHarvest plugin. What it does is it gives us a little fixture, what's called a fixture. So I'm not going to dive into that, but fixtures are a thing in PyTest that you can kind of use to do a lot of customization. What it means is it can give us back an object where we don't actually have to do asserts here, but we can capture information we need in a key value manner. And then at a later point, like assess everything altogether. And so how this works is that these functions are going to be So this function is going to access once for a feature of mine and Hugo's profiles. We're going to capture the inputs, outputs. Each kind of field match. And then if you only assert we're going to have is like, we want to make sure that we always get the name correct, right. So this is a choice we can make. Do we want to, you can use the search to fail hard or you can capture things in a software and then process them later to then decide whether to fail an aggregate or not and so with podcast harvest, this is where this test prep results function comes in. It gives us a, we can ask for a pandas data frame that has uh all this information. That was captured in these tests that were using the results back. And so\u2026 Let me just walk through it without scrolling too quickly. So\u2026 This will give us a data frame. We can then compute or add columns to this data frame. So if you come from the data science side, like this is where you know you can start to do some of those things you do with a data frame. We can, you know, we can compute field accuracy based on the different fields right This is where we can kind of, you know. Print what we want We can compute different measures, sorry, we compute different metrics from the things that we have measured And this is where like in aggregate, right, we can then choose to you know assert like fail like hey Fail the pipeline or fail the test completely or not, or just, you know, log some warnings This is where we could also in this function you know upload the data to Google Sheets and you know kick off some other process but Just to give you a mental picture of kind of how this works. Yep. Hey, Stefan, just one look. We've had a couple of related questions in the chat, which is\u2026 The answer is implicit in everything you're talking about, but it's around, but I think it's worthwhile being explicit about it. How do we use testing, deterministic tests, when the stochastic, when the nature of LLMs and this type of software is stochastic. Yep. And part of the answer that William spoke to in the thread is around running it several times and getting a distribution of results, right. But if you could speak to that explicitly, and I know you intend on doing so but Yep. Yeah, yeah. I mean, that's a good question. So this is where one of the things is like, should things be 100%.", "metadata": {"source": "sentence_chunking", "timestamp": "Chunk 7", "position": 6, "speaker": "Unknown", "token_count": 1030, "workshop_id": "WS4", "original_chunk_id": "92c92730-93f3-4119-80eb-0fa518d7c391"}, "id": "WS4_92c92730-93f3-4119-80eb-0fa518d7c391", "relevance": 1.0}, {"text": "I got it to help me. Leverage the tools you have. And then what do you do. This is embedded in a for loop. I literally just had my completion endpoint and I'm giving it a system prompt and then I'm saying the content is generate realistic user questions about a workshop based on persona scenario. Okay. And I do that in a for loop and I get out all these questions. This is, I love this. I love that I've been able to synthetically generate 20, 50, however many questions, right. So we can see This one is, can you provide insights. Of course, it says insights because it's an LLM into None of you use the word insights, just to be clear. So we've already seen some data driven right in terms of inputs. Can you provide insights into how to generate synthetic data for training LLMs. What are the best practices for evaluating the performance of LM applications in a production environment. Could you like you can see, it's generated some reasonably sounding questions based on the types of things we may ask. It may not be quite the type of language we use, but LLMs are actually incredibly good at like not being language specific, although in their responses, they will often mimic you. Which is important to which is important To point out, what metrics should I focus on when evaluating the performance. Great question. I mean, that's what this is all about. And I didn't even ask it to give this question So once we have all of these questions. What do we do, right. And I showed you that we can hand label them in a spreadsheet. Now, I did something cheekier than that. And what I did was I actually used Some form of vibe coding. And I think not everyone has heard that term. Greg Cicerelli from\u2026 I think I mentioned he used to run product, BCPO product at Pluraside and then director of data science at GitHub. He's going to come and give a talk on vibe coding. Next week. And the premise is using LLMs as AI assistants where you don't necessarily quite understand all the code that it's giving you. And the reason I mention that now is because I vibe coded using cursor in agent mode. Not YOLO mode. There's a YOLO mode. I very much\u2026 advise against that if you're giving it your API keys as well. But I used a cursor Claude 3.7 Sonnet max in agent mode. And I got it to just build a JSON viewer for me where I could give pass and fails to responses. To the synthetically generated responses, right. And then give a reason as well. Now, I just side note, this is a wonderful use case for vibe code. And people are like, oh, vibe coding sucks like no one's going to blah, blah, blah, blah, blah, right. To be clear, on one side, there are grifters saying SaaS is dead because of vibe coding and on the other, there's like old men screaming at clouds. But this is an example where I didn't understand all the like like JavaScript, React, HTML stuff, this was doing, but I could validate that it worked for my use cases and gave me what I wanted. It worked, right. And I was able to hand label these rapidly. Once again, don't jump for like big tools. I mean, like Phoenix Arise can be used for this type of stuff. Brain trust. All of these things are great. But when you're doing an MVP, an MVE, start off small. Only go to larger tools that that have a higher abstraction level and more complexity when you need to. I ran the queries, I hand labeled the responses. What do we do then. We log the traces in a spreadsheet, right. Or a JSON viewer like I did there. We accept or reject a response. We add columns for reason for accept, reject. We classify the failure mode. Was it a hallucination. Was it a retrieval issue. Was it a structured output issue. Was it a format issue. Then we do a pivot table. So we get failure modes ranked by frequency. And we'll see how this plays out in a second. And then we get actionable fixes. It's like, oh. It's hallucinating too much. Now I need to iterate on my prompt or perhaps ground it in my information retrieval system more, which we'll get to next week or do some fine tuning. These types of things, right. Doesn't matter what the actions are yet. It's the diagnosis and then we start to prescribe. The action. So this is an example, a toy example where we have you know we have The input is summarize the doc. The output is dogs are cool Who can argue with that. But the ground truth was it was a company summary report. So that's a fail. What's the failure mode. It's hallucination. Input, get CEO name. It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval.", "metadata": {"original_chunk_id": "9c6ed167-c2ab-4855-a8f9-17d50e27a01d", "workshop_id": "WS3", "position": 9, "speaker": "Unknown", "timestamp": "Chunk 10", "source": "sentence_chunking", "token_count": 1067}, "id": "WS3_9c6ed167-c2ab-4855-a8f9-17d50e27a01d", "relevance": 1.0}, {"text": "I got it to help me. Leverage the tools you have. And then what do you do. This is embedded in a for loop. I literally just had my completion endpoint and I'm giving it a system prompt and then I'm saying the content is generate realistic user questions about a workshop based on persona scenario. Okay. And I do that in a for loop and I get out all these questions. This is, I love this. I love that I've been able to synthetically generate 20, 50, however many questions, right. So we can see This one is, can you provide insights. Of course, it says insights because it's an LLM into None of you use the word insights, just to be clear. So we've already seen some data driven right in terms of inputs. Can you provide insights into how to generate synthetic data for training LLMs. What are the best practices for evaluating the performance of LM applications in a production environment. Could you like you can see, it's generated some reasonably sounding questions based on the types of things we may ask. It may not be quite the type of language we use, but LLMs are actually incredibly good at like not being language specific, although in their responses, they will often mimic you. Which is important to which is important To point out, what metrics should I focus on when evaluating the performance. Great question. I mean, that's what this is all about. And I didn't even ask it to give this question So once we have all of these questions. What do we do, right. And I showed you that we can hand label them in a spreadsheet. Now, I did something cheekier than that. And what I did was I actually used Some form of vibe coding. And I think not everyone has heard that term. Greg Cicerelli from\u2026 I think I mentioned he used to run product, BCPO product at Pluraside and then director of data science at GitHub. He's going to come and give a talk on vibe coding. Next week. And the premise is using LLMs as AI assistants where you don't necessarily quite understand all the code that it's giving you. And the reason I mention that now is because I vibe coded using cursor in agent mode. Not YOLO mode. There's a YOLO mode. I very much\u2026 advise against that if you're giving it your API keys as well. But I used a cursor Claude 3.7 Sonnet max in agent mode. And I got it to just build a JSON viewer for me where I could give pass and fails to responses. To the synthetically generated responses, right. And then give a reason as well. Now, I just side note, this is a wonderful use case for vibe code. And people are like, oh, vibe coding sucks like no one's going to blah, blah, blah, blah, blah, right. To be clear, on one side, there are grifters saying SaaS is dead because of vibe coding and on the other, there's like old men screaming at clouds. But this is an example where I didn't understand all the like like JavaScript, React, HTML stuff, this was doing, but I could validate that it worked for my use cases and gave me what I wanted. It worked, right. And I was able to hand label these rapidly. Once again, don't jump for like big tools. I mean, like Phoenix Arise can be used for this type of stuff. Brain trust. All of these things are great. But when you're doing an MVP, an MVE, start off small. Only go to larger tools that that have a higher abstraction level and more complexity when you need to. I ran the queries, I hand labeled the responses. What do we do then. We log the traces in a spreadsheet, right. Or a JSON viewer like I did there. We accept or reject a response. We add columns for reason for accept, reject. We classify the failure mode. Was it a hallucination. Was it a retrieval issue. Was it a structured output issue. Was it a format issue. Then we do a pivot table. So we get failure modes ranked by frequency. And we'll see how this plays out in a second. And then we get actionable fixes. It's like, oh. It's hallucinating too much. Now I need to iterate on my prompt or perhaps ground it in my information retrieval system more, which we'll get to next week or do some fine tuning. These types of things, right. Doesn't matter what the actions are yet. It's the diagnosis and then we start to prescribe. The action. So this is an example, a toy example where we have you know we have The input is summarize the doc. The output is dogs are cool Who can argue with that. But the ground truth was it was a company summary report. So that's a fail. What's the failure mode. It's hallucination. Input, get CEO name. It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval.", "metadata": {"position": 9, "workshop_id": "WS3", "timestamp": "Chunk 10", "original_chunk_id": "f89c3a9b-2a07-4e5b-87e3-5ee75c5bd11b", "source": "sentence_chunking", "speaker": "Unknown", "token_count": 1067}, "id": "WS3_f89c3a9b-2a07-4e5b-87e3-5ee75c5bd11b", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"source": "sentence_chunking", "token_count": 1063, "speaker": "Unknown", "workshop_id": "WS3", "position": 4, "original_chunk_id": "f4c88392-cba7-4006-910b-c8336207f0c1", "timestamp": "Chunk 5"}, "id": "WS3_f4c88392-cba7-4006-910b-c8336207f0c1", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "token_count": 1063, "workshop_id": "WS3", "timestamp": "Chunk 5", "position": 4, "original_chunk_id": "38ad3440-7a66-40d4-86c8-bbec8e1ee7e8"}, "id": "WS3_38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "relevance": 1.0}]}}{"question": "I mean synthetically generated questions", "response": "The underlying question seems to be about how to effectively generate synthetic questions for a workshop or application based on user personas and scenarios.\n\nTo generate synthetic questions, you can follow these steps:\n\n1. **Define User Personas**: Identify the target users for your application, such as data scientists or ML engineers. Create detailed descriptions of their goals and technical levels.\n\n2. **Create Scenarios**: Develop scenarios that reflect the types of interactions these personas might have with your application. For example, consider different types of questions they might ask, such as general, technical, or factual inquiries.\n\n3. **Use a Language Model**: Implement a language model (like OpenAI's) to generate questions. You can set up a loop where you provide the model with a prompt that includes the persona and scenario, asking it to generate realistic user questions.\n\n4. **Hand Label and Analyze**: Once you have a set of generated questions, you can hand label them for quality and relevance. This involves assessing whether the questions align with the personas and scenarios you defined.\n\n5. **Iterate and Improve**: Use the feedback from the labeling process to refine your question generation process. You can adjust the prompts or the personas to improve the quality of the generated questions.\n\nBy following these steps, you can create a robust set of synthetic questions that are tailored to your specific use case, enhancing the relevance and effectiveness of your application.\n\nIn summary, leverage user personas and scenarios to guide the synthetic question generation process using a language model, and continuously refine your approach based on feedback.", "context_info": {"num_chunks": 10, "context_tokens": 10485, "completion_tokens": 312, "embedding_tokens": 15360, "workshops_used": ["WS6", "WS3", "WS5", "WS4"], "chunks": [{"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "position": 8, "token_count": 1067, "workshop_id": "WS3", "speaker": "Unknown", "timestamp": "Chunk 9"}, "id": "WS3_6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"position": 8, "token_count": 1067, "workshop_id": "WS3", "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "timestamp": "Chunk 9"}, "id": "WS3_9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"timestamp": "Chunk 19", "position": 18, "speaker": "Unknown", "original_chunk_id": "0023cc64-b3ca-486f-9dd8-dbbc987bf306", "source": "sentence_chunking", "workshop_id": "WS6", "token_count": 1062}, "id": "WS6_0023cc64-b3ca-486f-9dd8-dbbc987bf306", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"position": 18, "source": "sentence_chunking", "original_chunk_id": "981ee013-55da-46a9-a83e-dd1b1322ac44", "token_count": 1062, "workshop_id": "WS6", "speaker": "Unknown", "timestamp": "Chunk 19"}, "id": "WS6_981ee013-55da-46a9-a83e-dd1b1322ac44", "relevance": 1.0}, {"text": "I got it to help me. Leverage the tools you have. And then what do you do. This is embedded in a for loop. I literally just had my completion endpoint and I'm giving it a system prompt and then I'm saying the content is generate realistic user questions about a workshop based on persona scenario. Okay. And I do that in a for loop and I get out all these questions. This is, I love this. I love that I've been able to synthetically generate 20, 50, however many questions, right. So we can see This one is, can you provide insights. Of course, it says insights because it's an LLM into None of you use the word insights, just to be clear. So we've already seen some data driven right in terms of inputs. Can you provide insights into how to generate synthetic data for training LLMs. What are the best practices for evaluating the performance of LM applications in a production environment. Could you like you can see, it's generated some reasonably sounding questions based on the types of things we may ask. It may not be quite the type of language we use, but LLMs are actually incredibly good at like not being language specific, although in their responses, they will often mimic you. Which is important to which is important To point out, what metrics should I focus on when evaluating the performance. Great question. I mean, that's what this is all about. And I didn't even ask it to give this question So once we have all of these questions. What do we do, right. And I showed you that we can hand label them in a spreadsheet. Now, I did something cheekier than that. And what I did was I actually used Some form of vibe coding. And I think not everyone has heard that term. Greg Cicerelli from\u2026 I think I mentioned he used to run product, BCPO product at Pluraside and then director of data science at GitHub. He's going to come and give a talk on vibe coding. Next week. And the premise is using LLMs as AI assistants where you don't necessarily quite understand all the code that it's giving you. And the reason I mention that now is because I vibe coded using cursor in agent mode. Not YOLO mode. There's a YOLO mode. I very much\u2026 advise against that if you're giving it your API keys as well. But I used a cursor Claude 3.7 Sonnet max in agent mode. And I got it to just build a JSON viewer for me where I could give pass and fails to responses. To the synthetically generated responses, right. And then give a reason as well. Now, I just side note, this is a wonderful use case for vibe code. And people are like, oh, vibe coding sucks like no one's going to blah, blah, blah, blah, blah, right. To be clear, on one side, there are grifters saying SaaS is dead because of vibe coding and on the other, there's like old men screaming at clouds. But this is an example where I didn't understand all the like like JavaScript, React, HTML stuff, this was doing, but I could validate that it worked for my use cases and gave me what I wanted. It worked, right. And I was able to hand label these rapidly. Once again, don't jump for like big tools. I mean, like Phoenix Arise can be used for this type of stuff. Brain trust. All of these things are great. But when you're doing an MVP, an MVE, start off small. Only go to larger tools that that have a higher abstraction level and more complexity when you need to. I ran the queries, I hand labeled the responses. What do we do then. We log the traces in a spreadsheet, right. Or a JSON viewer like I did there. We accept or reject a response. We add columns for reason for accept, reject. We classify the failure mode. Was it a hallucination. Was it a retrieval issue. Was it a structured output issue. Was it a format issue. Then we do a pivot table. So we get failure modes ranked by frequency. And we'll see how this plays out in a second. And then we get actionable fixes. It's like, oh. It's hallucinating too much. Now I need to iterate on my prompt or perhaps ground it in my information retrieval system more, which we'll get to next week or do some fine tuning. These types of things, right. Doesn't matter what the actions are yet. It's the diagnosis and then we start to prescribe. The action. So this is an example, a toy example where we have you know we have The input is summarize the doc. The output is dogs are cool Who can argue with that. But the ground truth was it was a company summary report. So that's a fail. What's the failure mode. It's hallucination. Input, get CEO name. It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval.", "metadata": {"source": "sentence_chunking", "token_count": 1067, "position": 9, "speaker": "Unknown", "original_chunk_id": "9c6ed167-c2ab-4855-a8f9-17d50e27a01d", "workshop_id": "WS3", "timestamp": "Chunk 10"}, "id": "WS3_9c6ed167-c2ab-4855-a8f9-17d50e27a01d", "relevance": 1.0}, {"text": "I got it to help me. Leverage the tools you have. And then what do you do. This is embedded in a for loop. I literally just had my completion endpoint and I'm giving it a system prompt and then I'm saying the content is generate realistic user questions about a workshop based on persona scenario. Okay. And I do that in a for loop and I get out all these questions. This is, I love this. I love that I've been able to synthetically generate 20, 50, however many questions, right. So we can see This one is, can you provide insights. Of course, it says insights because it's an LLM into None of you use the word insights, just to be clear. So we've already seen some data driven right in terms of inputs. Can you provide insights into how to generate synthetic data for training LLMs. What are the best practices for evaluating the performance of LM applications in a production environment. Could you like you can see, it's generated some reasonably sounding questions based on the types of things we may ask. It may not be quite the type of language we use, but LLMs are actually incredibly good at like not being language specific, although in their responses, they will often mimic you. Which is important to which is important To point out, what metrics should I focus on when evaluating the performance. Great question. I mean, that's what this is all about. And I didn't even ask it to give this question So once we have all of these questions. What do we do, right. And I showed you that we can hand label them in a spreadsheet. Now, I did something cheekier than that. And what I did was I actually used Some form of vibe coding. And I think not everyone has heard that term. Greg Cicerelli from\u2026 I think I mentioned he used to run product, BCPO product at Pluraside and then director of data science at GitHub. He's going to come and give a talk on vibe coding. Next week. And the premise is using LLMs as AI assistants where you don't necessarily quite understand all the code that it's giving you. And the reason I mention that now is because I vibe coded using cursor in agent mode. Not YOLO mode. There's a YOLO mode. I very much\u2026 advise against that if you're giving it your API keys as well. But I used a cursor Claude 3.7 Sonnet max in agent mode. And I got it to just build a JSON viewer for me where I could give pass and fails to responses. To the synthetically generated responses, right. And then give a reason as well. Now, I just side note, this is a wonderful use case for vibe code. And people are like, oh, vibe coding sucks like no one's going to blah, blah, blah, blah, blah, right. To be clear, on one side, there are grifters saying SaaS is dead because of vibe coding and on the other, there's like old men screaming at clouds. But this is an example where I didn't understand all the like like JavaScript, React, HTML stuff, this was doing, but I could validate that it worked for my use cases and gave me what I wanted. It worked, right. And I was able to hand label these rapidly. Once again, don't jump for like big tools. I mean, like Phoenix Arise can be used for this type of stuff. Brain trust. All of these things are great. But when you're doing an MVP, an MVE, start off small. Only go to larger tools that that have a higher abstraction level and more complexity when you need to. I ran the queries, I hand labeled the responses. What do we do then. We log the traces in a spreadsheet, right. Or a JSON viewer like I did there. We accept or reject a response. We add columns for reason for accept, reject. We classify the failure mode. Was it a hallucination. Was it a retrieval issue. Was it a structured output issue. Was it a format issue. Then we do a pivot table. So we get failure modes ranked by frequency. And we'll see how this plays out in a second. And then we get actionable fixes. It's like, oh. It's hallucinating too much. Now I need to iterate on my prompt or perhaps ground it in my information retrieval system more, which we'll get to next week or do some fine tuning. These types of things, right. Doesn't matter what the actions are yet. It's the diagnosis and then we start to prescribe. The action. So this is an example, a toy example where we have you know we have The input is summarize the doc. The output is dogs are cool Who can argue with that. But the ground truth was it was a company summary report. So that's a fail. What's the failure mode. It's hallucination. Input, get CEO name. It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval.", "metadata": {"position": 9, "token_count": 1067, "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "f89c3a9b-2a07-4e5b-87e3-5ee75c5bd11b", "timestamp": "Chunk 10", "workshop_id": "WS3"}, "id": "WS3_f89c3a9b-2a07-4e5b-87e3-5ee75c5bd11b", "relevance": 1.0}, {"text": "What the language model does is it calls a relational database for the existing string for that user And so the existing string for that user, it's just like all the kind of accumulated information up to that point in time. And so this language model call And yeah, actually, I kind of talk about it more here, you know, just kind of like getting concrete about like, hey, what does the actual prompt look like um right so you um in kind of constructing this prompt and doing the update We're reading. Reading the existing memory And so here's the existing memory piece. Here it gets read from the relational database. Here's the new information that was just passed by the agent. I think it's always good practice. This isn't the subject of the talk, but just in my experience with prompting, meta context is always very important like giving given the language model some understanding of kind of like what are the broader objectives you know what company do you work for and kind of Are you trying to maximize revenue or maximize happiness or what have you, those type of things. And then the instructions, which here involve just incorporating the new information and kind of integrating it into the existing string And then providing that update So yeah, so basically how this goes is okay so we're generating this update prompt, basically the prompt that gets passed to the language model along with the new information. And then this language takes the new information and it just updates the string. So it's, you know, basically couldn't get more simple than that. I'll pause there in case it actually is the case that it could get more simple and it's been hard to follow what I've said. So I'll just pause for a couple It all makes perfect sense to me. We have a question from Luca that I'm not sure I understand. Yeah. And the question is, do we get the final response only from the LLM as user memory to be added to the current query. I think I don't understand it because there are too many subclauses for me to grok it. Yeah, so I think maybe like just my interpretation of it, which is just like, I think I didn't do a good job of explaining is that um right so um So this prompt up here is basically what gets sent To the language model and what elicits a response from the language model. And so one of those responses is, you know, so eventually it returns like a chat response. It may call a tool. So if, yeah, let's say kind of like a new user message comes in It generates this prompt. So the user input comes from what the user enters. And then the user memory is just pulled. It's just a simple database read. So it's just a simple, very low latency API call to a relational database that just contains a string. That might just be a lookup on the basis of a user ID And so you just inject this into the prompt. And so basically every language model call is going to contain this user memory. And the agent is going to have the option of updating this string. And if it does, that's just going to get pushed to the relational database Once that's done. That'll just be available in the relational database and be provided you know be provided be kind of funneled into the prompt for the next response. Did that clarify the question or clarify the question And yeah, there's no vector DB yet, but I will get to that. What. In the next section sorry go ahead. Sure. Oh. Okay, but I need some question here because as a context one is for each LLM, it is uh limited okay so if our user memory is getting bigger Okay, there won't be some error here. Mm-hmm. And can't handle this and can they give you the response. Mm-hmm. Oh, so you're saying just like eventually you're going to hit token limits yeah so this so that's a great point. And that's exactly, you know, I mentioned I'm going to talk about kind of like the trade-offs here. But you've kind of anticipated one of the major trade-offs associated with this approach. What I will say is that in my experience. You can actually go pretty far with this approach. And it's dead simple and it's super low latency. And you can just, this is an approach that doesn't require any, I guess it does require the tool call to to decide what information gets saved, but in terms of just injecting this context into every language model response. It's an extremely efficient strategy. And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs.", "metadata": {"speaker": "Unknown", "workshop_id": "WS4", "token_count": 1037, "position": 21, "timestamp": "Chunk 22", "source": "sentence_chunking", "original_chunk_id": "6927769f-eb5e-462b-b957-b3fcd530b67a"}, "id": "WS4_6927769f-eb5e-462b-b957-b3fcd530b67a", "relevance": 1.0}, {"text": "What the language model does is it calls a relational database for the existing string for that user And so the existing string for that user, it's just like all the kind of accumulated information up to that point in time. And so this language model call And yeah, actually, I kind of talk about it more here, you know, just kind of like getting concrete about like, hey, what does the actual prompt look like um right so you um in kind of constructing this prompt and doing the update We're reading. Reading the existing memory And so here's the existing memory piece. Here it gets read from the relational database. Here's the new information that was just passed by the agent. I think it's always good practice. This isn't the subject of the talk, but just in my experience with prompting, meta context is always very important like giving given the language model some understanding of kind of like what are the broader objectives you know what company do you work for and kind of Are you trying to maximize revenue or maximize happiness or what have you, those type of things. And then the instructions, which here involve just incorporating the new information and kind of integrating it into the existing string And then providing that update So yeah, so basically how this goes is okay so we're generating this update prompt, basically the prompt that gets passed to the language model along with the new information. And then this language takes the new information and it just updates the string. So it's, you know, basically couldn't get more simple than that. I'll pause there in case it actually is the case that it could get more simple and it's been hard to follow what I've said. So I'll just pause for a couple It all makes perfect sense to me. We have a question from Luca that I'm not sure I understand. Yeah. And the question is, do we get the final response only from the LLM as user memory to be added to the current query. I think I don't understand it because there are too many subclauses for me to grok it. Yeah, so I think maybe like just my interpretation of it, which is just like, I think I didn't do a good job of explaining is that um right so um So this prompt up here is basically what gets sent To the language model and what elicits a response from the language model. And so one of those responses is, you know, so eventually it returns like a chat response. It may call a tool. So if, yeah, let's say kind of like a new user message comes in It generates this prompt. So the user input comes from what the user enters. And then the user memory is just pulled. It's just a simple database read. So it's just a simple, very low latency API call to a relational database that just contains a string. That might just be a lookup on the basis of a user ID And so you just inject this into the prompt. And so basically every language model call is going to contain this user memory. And the agent is going to have the option of updating this string. And if it does, that's just going to get pushed to the relational database Once that's done. That'll just be available in the relational database and be provided you know be provided be kind of funneled into the prompt for the next response. Did that clarify the question or clarify the question And yeah, there's no vector DB yet, but I will get to that. What. In the next section sorry go ahead. Sure. Oh. Okay, but I need some question here because as a context one is for each LLM, it is uh limited okay so if our user memory is getting bigger Okay, there won't be some error here. Mm-hmm. And can't handle this and can they give you the response. Mm-hmm. Oh, so you're saying just like eventually you're going to hit token limits yeah so this so that's a great point. And that's exactly, you know, I mentioned I'm going to talk about kind of like the trade-offs here. But you've kind of anticipated one of the major trade-offs associated with this approach. What I will say is that in my experience. You can actually go pretty far with this approach. And it's dead simple and it's super low latency. And you can just, this is an approach that doesn't require any, I guess it does require the tool call to to decide what information gets saved, but in terms of just injecting this context into every language model response. It's an extremely efficient strategy. And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs.", "metadata": {"token_count": 1037, "timestamp": "Chunk 22", "workshop_id": "WS4", "source": "sentence_chunking", "position": 21, "speaker": "Unknown", "original_chunk_id": "15e15f0c-6283-4643-b794-4a249ce929b9"}, "id": "WS4_15e15f0c-6283-4643-b794-4a249ce929b9", "relevance": 1.0}, {"text": "Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts. And you may notice that if you looked at 134 hugo bowne-anderson: the code base of the rag system I built on our transcripts, the 1st approximation wasn't even rag yet. 135 hugo bowne-anderson: and I had the issue that the Workshop Transcript was just too long, like it didn't fit in any context window. Essentially. So I did need to break it up in some way, or of the models I was using. And then thinking through, we saw with like summarization. It may seem like it's easy to think whether something is a good summary or not. But in summarization, when evaluating, we're balancing conciseness and completeness and a variety of things right. And there are trade-offs there, and tensions. And similarly. 136 hugo bowne-anderson: with these types of information retrieval systems, we want to balance precision. So getting relevant results 137 hugo bowne-anderson: and recall. So not missing important information, and depending on your use case, like, if you're trying to like. 138 hugo bowne-anderson: if you're introspecting into your own medical documents or something, perhaps your fine getting 139 hugo bowne-anderson: more results with some being less relevant. Because you're not missing important ones as well like you're casting the net wider. Right. Perhaps that's a design choice you make, and that. So that comes down to your particular use case. 140 hugo bowne-anderson: So 141 hugo bowne-anderson: there are lots of different techniques. I'm not going to go through all of these, but I will just say that nearly all the time. Now you'll use transformer based 142 hugo bowne-anderson: embeddings. And so we'll go through what embeddings are. But once again it's taking strings and embedding them in a vector space of of some sort. 143 hugo bowne-anderson: You may not need openai or anything along those lines. There are some wonderful 144 hugo bowne-anderson: embeddings, models which are way more cost, efficient and supported far more in the open source space that you can find on hugging face. And we'll see, you know several of those today, but I think it is important to recognize that the lineage here 145 hugo bowne-anderson: that you know bag of words to tfidf to word to vet kind of led us where we are today. So time permitting, we'll go through a bit of that in today's workshop. 146 hugo bowne-anderson: So what is our job. Our job is to find relevant information in the Corpus based on the prompt. Okay, so we need to quote unquote, understand the prompt. So we to do that. We want to extract the intent or key information from the query, okay, and I would encourage you all. And I I don't think about this enough honestly myself. And oh, there are far smarter people who think about this. But 147 hugo bowne-anderson: really thinking about building intent based systems where we actually, we don't just want to take a query and return. The most likely thing we actually want to deeply understand, quote unquote the intent of the user. Okay, so it extracting the intent or key information from their query. Then, understanding the prompt, so searching the corpus to find the most relevant chunks documents that align with the prompts intent. 148 hugo bowne-anderson: So how do we do this. 149 hugo bowne-anderson: We have out. 150 hugo bowne-anderson: you know, perhaps, structured data, some unstructured data. I kind of, you know, we talk about natural languages being unstructured data, which is a very like computational way to think about it, because clearly natural language is structured to claim it's unstructured is complete nonsense, right. But you know, in in the technical space we do call it that. So we take all of these things. That's our corpus. 151 hugo bowne-anderson: Then we 152 hugo bowne-anderson: use some vector database or something simpler. And I'll talk through how to think about what tools to adopt in a second 153 hugo bowne-anderson: in a text embedding model. Then, based on the query, we retrieve some chunks and then generate the response, using a large language model. Okay. And you'll note that we are using, quote unquote AI and Embeddings and that type of stuff. But we're not using generative capabilities until the very end. To be very clear. Right. And that's important to recognize. That's another thing. Think about what parts of your system need generative capabilities. Maybe it's the only the final step right. And you don't need to ping expensive Apis anywhere else in the system.", "metadata": {"token_count": 1065, "position": 7, "timestamp": "Chunk 8", "source": "sentence_chunking", "workshop_id": "WS5", "original_chunk_id": "b9b2042e-6330-403d-9d87-6e2969a64364", "speaker": "Unknown"}, "id": "WS5_b9b2042e-6330-403d-9d87-6e2969a64364", "relevance": 1.0}, {"text": "Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts. And you may notice that if you looked at 134 hugo bowne-anderson: the code base of the rag system I built on our transcripts, the 1st approximation wasn't even rag yet. 135 hugo bowne-anderson: and I had the issue that the Workshop Transcript was just too long, like it didn't fit in any context window. Essentially. So I did need to break it up in some way, or of the models I was using. And then thinking through, we saw with like summarization. It may seem like it's easy to think whether something is a good summary or not. But in summarization, when evaluating, we're balancing conciseness and completeness and a variety of things right. And there are trade-offs there, and tensions. And similarly. 136 hugo bowne-anderson: with these types of information retrieval systems, we want to balance precision. So getting relevant results 137 hugo bowne-anderson: and recall. So not missing important information, and depending on your use case, like, if you're trying to like. 138 hugo bowne-anderson: if you're introspecting into your own medical documents or something, perhaps your fine getting 139 hugo bowne-anderson: more results with some being less relevant. Because you're not missing important ones as well like you're casting the net wider. Right. Perhaps that's a design choice you make, and that. So that comes down to your particular use case. 140 hugo bowne-anderson: So 141 hugo bowne-anderson: there are lots of different techniques. I'm not going to go through all of these, but I will just say that nearly all the time. Now you'll use transformer based 142 hugo bowne-anderson: embeddings. And so we'll go through what embeddings are. But once again it's taking strings and embedding them in a vector space of of some sort. 143 hugo bowne-anderson: You may not need openai or anything along those lines. There are some wonderful 144 hugo bowne-anderson: embeddings, models which are way more cost, efficient and supported far more in the open source space that you can find on hugging face. And we'll see, you know several of those today, but I think it is important to recognize that the lineage here 145 hugo bowne-anderson: that you know bag of words to tfidf to word to vet kind of led us where we are today. So time permitting, we'll go through a bit of that in today's workshop. 146 hugo bowne-anderson: So what is our job. Our job is to find relevant information in the Corpus based on the prompt. Okay, so we need to quote unquote, understand the prompt. So we to do that. We want to extract the intent or key information from the query, okay, and I would encourage you all. And I I don't think about this enough honestly myself. And oh, there are far smarter people who think about this. But 147 hugo bowne-anderson: really thinking about building intent based systems where we actually, we don't just want to take a query and return. The most likely thing we actually want to deeply understand, quote unquote the intent of the user. Okay, so it extracting the intent or key information from their query. Then, understanding the prompt, so searching the corpus to find the most relevant chunks documents that align with the prompts intent. 148 hugo bowne-anderson: So how do we do this. 149 hugo bowne-anderson: We have out. 150 hugo bowne-anderson: you know, perhaps, structured data, some unstructured data. I kind of, you know, we talk about natural languages being unstructured data, which is a very like computational way to think about it, because clearly natural language is structured to claim it's unstructured is complete nonsense, right. But you know, in in the technical space we do call it that. So we take all of these things. That's our corpus. 151 hugo bowne-anderson: Then we 152 hugo bowne-anderson: use some vector database or something simpler. And I'll talk through how to think about what tools to adopt in a second 153 hugo bowne-anderson: in a text embedding model. Then, based on the query, we retrieve some chunks and then generate the response, using a large language model. Okay. And you'll note that we are using, quote unquote AI and Embeddings and that type of stuff. But we're not using generative capabilities until the very end. To be very clear. Right. And that's important to recognize. That's another thing. Think about what parts of your system need generative capabilities. Maybe it's the only the final step right. And you don't need to ping expensive Apis anywhere else in the system.", "metadata": {"timestamp": "Chunk 8", "workshop_id": "WS5", "position": 7, "token_count": 1065, "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "8ace80a5-98ba-435b-9d29-ae860c4e56c5"}, "id": "WS5_8ace80a5-98ba-435b-9d29-ae860c4e56c5", "relevance": 1.0}]}}{"question": "Is evaluation driven development mentioned in this workshop at all? If so, how many times?", "response": "Yes, evaluation driven development (EDD) is mentioned multiple times throughout the workshop. It is emphasized as a crucial approach to ensure that the development process remains focused on evaluating and improving the model's performance iteratively. \n\nSpecifically, the term \"evaluation driven development\" is explicitly mentioned at least three times in the provided transcript sections. The speaker discusses the importance of building an iterative loop of building, monitoring, evaluating, and improving, even before launching a product. \n\nTo implement EDD effectively, consider the following action items:\n1. **Build an Evaluation Harness**: Create a reproducible script to evaluate your system.\n2. **Iterate Based on Feedback**: Use error analysis to prioritize which aspects of the model to improve.\n3. **Focus on Upstream Failures**: Identify and address the most significant failures first before moving to downstream issues.\n\nBy following these steps, you can enhance your development process and ensure that your evaluations are meaningful and actionable.", "context_info": {"num_chunks": 10, "context_tokens": 10323, "completion_tokens": 193, "embedding_tokens": 15360, "workshops_used": ["WS1", "WS3"], "chunks": [{"text": "And 113 hugo bowne-anderson: wow. This slide still scares me, but it's it's real, it's it's real pain. And for some reason my 114 hugo bowne-anderson: screen isn't quite rendering everything it's slightly bigger than it it should be. But what I want to say is that 115 hugo bowne-anderson: if you think about as you see here. 116 hugo bowne-anderson: how we build software and think about 117 hugo bowne-anderson: vibes and excitement as a function of time with traditional software building. 118 hugo bowne-anderson: It's 119 hugo bowne-anderson: it's pretty boring. You're going through the motions you're doing your hello world, basic features. Then you start adding unit tests. You're like, Oh, this is then you're like, okay. Now, I'm going to scale and optimize, load balance. And as you, as you go on, it gets more and more exciting, and this has been inverted with generative AI. Generally you can get a flashy demo 120 hugo bowne-anderson: immediately, and we'll we'll do that today. 121 hugo bowne-anderson: And then suddenly, you're like, Okay, that didn't like when I type that in that that's cool. But when I type anything else in it gives me a garbled response. So you're like, Wait. It looks right when I do the demo. But I try to do anything else, and it literally lacked basic functionality. And then it's like, Oh, wait all these hallucinations. 122 hugo bowne-anderson: and then you're in, and you've got thousands of conversations already happening, and you don't even know how to monitor them properly or look at them. And then you want to integrate it into your classic business software stacks. And suddenly what started off as a flashy demo just just went down and down, and and down, and what. 123 Sanket Firodiya: Thank you. 124 hugo bowne-anderson: Okay, was that something. Someone. 125 hugo bowne-anderson: What we want to do. 126 hugo bowne-anderson: The only thing we want to shift here is this part of the Gen. AI curve. 127 hugo bowne-anderson: Oh, we don't want to touch software engineering. We don't even want to bring excitement down at the start. We just want to make sure that you stay as excited as possible as a function of time. And the reason in all honesty that flashy Demos still here 128 hugo bowne-anderson: is because people who build them lack the ability 129 hugo bowne-anderson: to evaluate properly and develop a robust evaluation system. 130 hugo bowne-anderson: not because the system is bad and at all right. So that's what we're here to really talk about. I do want to frame most things in terms of evaluation, driven development, although we'll we'll dovetail out of a lot of different different topics. Okay. 131 hugo bowne-anderson: so 132 hugo bowne-anderson: our way out is Edd, evaluation, driven development. Where you have a iterative loop right where you build, you monitor, you evaluate, you improve now. 133 hugo bowne-anderson: even before launching a product. And this is something we'll talk about as the course goes on. You can, you can build a synthetic data set and evaluation harnesses to help you even before real users exist. And if you don't know what an evaluation harness is, don't worry about it. It's essentially a script, something reproducible that you use to evaluate your system. So one example I like to give is. 134 hugo bowne-anderson: I was working on something recently where we were using Gpt. 4.0 Mini, and then Google released Gemini 2.5. And we wanted to play around with it. And it was like, Okay, I'm going to switch this out. But how do I even know beyond vibes that it's going to be 135 hugo bowne-anderson: good or not. So if you have a basic harness with a few tests, measuring latency and cost and things of business interest as well. This can get you a long way. So at the start, I actually encourage people to build what I call an Mve. So for any Mvp minimum viable product, you have an Mve a minimum viable evaluation as well. Right. 136 hugo bowne-anderson: So just thinking through kind of how this happens. You have an Mvp, you have synthetic user queries. 137 hugo bowne-anderson: You get the Mvp to give you responses, label them by hand, use the label data to build a basic evaluation harness, for example.", "metadata": {"timestamp": "Chunk 8", "position": 7, "source": "sentence_chunking", "workshop_id": "WS1", "original_chunk_id": "25aad6eb-69ee-4f84-aacd-ce1398ad2954", "token_count": 1067, "speaker": "Unknown"}, "id": "WS1_25aad6eb-69ee-4f84-aacd-ce1398ad2954", "relevance": 1.0}, {"text": "And 113 hugo bowne-anderson: wow. This slide still scares me, but it's it's real, it's it's real pain. And for some reason my 114 hugo bowne-anderson: screen isn't quite rendering everything it's slightly bigger than it it should be. But what I want to say is that 115 hugo bowne-anderson: if you think about as you see here. 116 hugo bowne-anderson: how we build software and think about 117 hugo bowne-anderson: vibes and excitement as a function of time with traditional software building. 118 hugo bowne-anderson: It's 119 hugo bowne-anderson: it's pretty boring. You're going through the motions you're doing your hello world, basic features. Then you start adding unit tests. You're like, Oh, this is then you're like, okay. Now, I'm going to scale and optimize, load balance. And as you, as you go on, it gets more and more exciting, and this has been inverted with generative AI. Generally you can get a flashy demo 120 hugo bowne-anderson: immediately, and we'll we'll do that today. 121 hugo bowne-anderson: And then suddenly, you're like, Okay, that didn't like when I type that in that that's cool. But when I type anything else in it gives me a garbled response. So you're like, Wait. It looks right when I do the demo. But I try to do anything else, and it literally lacked basic functionality. And then it's like, Oh, wait all these hallucinations. 122 hugo bowne-anderson: and then you're in, and you've got thousands of conversations already happening, and you don't even know how to monitor them properly or look at them. And then you want to integrate it into your classic business software stacks. And suddenly what started off as a flashy demo just just went down and down, and and down, and what. 123 Sanket Firodiya: Thank you. 124 hugo bowne-anderson: Okay, was that something. Someone. 125 hugo bowne-anderson: What we want to do. 126 hugo bowne-anderson: The only thing we want to shift here is this part of the Gen. AI curve. 127 hugo bowne-anderson: Oh, we don't want to touch software engineering. We don't even want to bring excitement down at the start. We just want to make sure that you stay as excited as possible as a function of time. And the reason in all honesty that flashy Demos still here 128 hugo bowne-anderson: is because people who build them lack the ability 129 hugo bowne-anderson: to evaluate properly and develop a robust evaluation system. 130 hugo bowne-anderson: not because the system is bad and at all right. So that's what we're here to really talk about. I do want to frame most things in terms of evaluation, driven development, although we'll we'll dovetail out of a lot of different different topics. Okay. 131 hugo bowne-anderson: so 132 hugo bowne-anderson: our way out is Edd, evaluation, driven development. Where you have a iterative loop right where you build, you monitor, you evaluate, you improve now. 133 hugo bowne-anderson: even before launching a product. And this is something we'll talk about as the course goes on. You can, you can build a synthetic data set and evaluation harnesses to help you even before real users exist. And if you don't know what an evaluation harness is, don't worry about it. It's essentially a script, something reproducible that you use to evaluate your system. So one example I like to give is. 134 hugo bowne-anderson: I was working on something recently where we were using Gpt. 4.0 Mini, and then Google released Gemini 2.5. And we wanted to play around with it. And it was like, Okay, I'm going to switch this out. But how do I even know beyond vibes that it's going to be 135 hugo bowne-anderson: good or not. So if you have a basic harness with a few tests, measuring latency and cost and things of business interest as well. This can get you a long way. So at the start, I actually encourage people to build what I call an Mve. So for any Mvp minimum viable product, you have an Mve a minimum viable evaluation as well. Right. 136 hugo bowne-anderson: So just thinking through kind of how this happens. You have an Mvp, you have synthetic user queries. 137 hugo bowne-anderson: You get the Mvp to give you responses, label them by hand, use the label data to build a basic evaluation harness, for example.", "metadata": {"original_chunk_id": "c8719fe9-2239-4539-a2c5-80acb917a765", "workshop_id": "WS1", "source": "sentence_chunking", "position": 7, "speaker": "Unknown", "timestamp": "Chunk 8", "token_count": 1067}, "id": "WS1_c8719fe9-2239-4539-a2c5-80acb917a765", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 5", "position": 4, "original_chunk_id": "f4c88392-cba7-4006-910b-c8336207f0c1", "workshop_id": "WS3", "token_count": 1063}, "id": "WS3_f4c88392-cba7-4006-910b-c8336207f0c1", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"source": "sentence_chunking", "workshop_id": "WS3", "token_count": 1063, "original_chunk_id": "38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "position": 4, "timestamp": "Chunk 5", "speaker": "Unknown"}, "id": "WS3_38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "relevance": 1.0}, {"text": "It's like, oh, let me write evals Let me write evals prospectively about every single conceivable problem I can brainstorm in my mind. You don't want to do that. I would say, you know, you want to respond to the error analysis And kind of let that guide you. Because like, yeah, there's an infinite surface area for you to test and for you to eval. And so you need to sort of Yeah, kind of like kind of let the error analysis help you prioritize. So like, for example, if it's like you know, even things like RAG, I would say Let the error analysis show you that RAG is a problem. And then specifically retrieval is a problem You know, before you go in depth with trying to evaluate like set up eval hardnesses for your retrieval. You might get to it anyways. You know you need to prioritize and focus And so again, the error analysis helps you do that and you can respond to the error analysis. I don't know if that's clear. I'm just trying to give some intuition. That's super clear. And well, for me, and it is, you really are speaking and I see a lot of people nodding. You're speaking to the iterative nature of it. And I'd be interested in your thoughts on so I built a basic retrieval app on workshop transcripts that I've shared with the class here. And I get them to label them Apostle Faio give a reason and that type of stuff. And in the first In the first iteration, I'm not even getting them to give pass or fail based on groundedness, actually. The question isn't even, is it a retrieval problem or not. That might come out in the wash. But so my question is, when first even evaluating stuff, you don't necessarily want to think across all dimensions, right. So when evaluating stuff focus on what is the most upstream failure I see. So basically, if you imagine a chain of events or even a multi-turn conversation or whatever Whenever I notice like the first failure, I just stop and I take note of that failure. That's a heuristic that helps you simplify things, right. Um and like basically you fix the upstream failures before you fix the downstream failures. That's what kind of that results in. So that's like a simplifying way to go about things that tends to tends to work. And like, yeah. And then when you're first doing air analysis, I wouldn't I would just focus on You know\u2026 I wouldn't even try to categorize it off the bat. I would just say like, yeah, just\u2026 observe and learn. About what's going on. Build some intuition and then maybe start categorizing later. Love it. It's diagnostic, right. And not prescriptive at this point. We've got a lot of great questions in here. Could people give a plus or a thumb to questions that they would like answered as well. Because we may not have time for all of them. So have a quick look. In Discord, there is a good question from David about, you mentioned the costs of additional evals and he says we're not just talking monetary costs. I actually think we're not monetary can, but There are complexity costs in terms of having more evals, but what do you think about the space of costs for Increased evaluations, haven't it. Yeah, I mean, there's like, you know, there's definitely Depending on the kind of eval, there's compute APR costs. But the most important one is your attention. You have a limited attention bandwidth. And about what you are going to pay attention to. You need to be very careful like not to completely overwhelm that and have it be high signal. As possible which means like constantly thinking about your evals curating them removing some things that shouldn't be making sure the tests if you have like\u2026 there's, you know, there's There's advice that goes around all the time. It's like. I just saw one that I wrote about one recently a YC company called YC. Case text. Um you know kind of went on this podcast and said hey you should have You should try to get to 100%. Pass rate on your evals. And that is bad. Because if you get 100% pass rate means your eval is not telling you anything. So you should not saturate your evals. You should have them be sufficiently difficult enough so that, hey, like they don't they're telling you something. They're just passing all the time. Might as well not have them. I don't know if that answered the question. Definitely does. We have a question from Namit, which a lot of people would love more insight into. If possible, could you share any examples of quote unquote difficult evals. Difficult emails. Okay, let me see. Let me think of this.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 21", "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "70963725-45a6-4b98-be13-65beb48998ab", "token_count": 1006, "position": 20}, "id": "WS3_70963725-45a6-4b98-be13-65beb48998ab", "relevance": 1.0}, {"text": "It's like, oh, let me write evals Let me write evals prospectively about every single conceivable problem I can brainstorm in my mind. You don't want to do that. I would say, you know, you want to respond to the error analysis And kind of let that guide you. Because like, yeah, there's an infinite surface area for you to test and for you to eval. And so you need to sort of Yeah, kind of like kind of let the error analysis help you prioritize. So like, for example, if it's like you know, even things like RAG, I would say Let the error analysis show you that RAG is a problem. And then specifically retrieval is a problem You know, before you go in depth with trying to evaluate like set up eval hardnesses for your retrieval. You might get to it anyways. You know you need to prioritize and focus And so again, the error analysis helps you do that and you can respond to the error analysis. I don't know if that's clear. I'm just trying to give some intuition. That's super clear. And well, for me, and it is, you really are speaking and I see a lot of people nodding. You're speaking to the iterative nature of it. And I'd be interested in your thoughts on so I built a basic retrieval app on workshop transcripts that I've shared with the class here. And I get them to label them Apostle Faio give a reason and that type of stuff. And in the first In the first iteration, I'm not even getting them to give pass or fail based on groundedness, actually. The question isn't even, is it a retrieval problem or not. That might come out in the wash. But so my question is, when first even evaluating stuff, you don't necessarily want to think across all dimensions, right. So when evaluating stuff focus on what is the most upstream failure I see. So basically, if you imagine a chain of events or even a multi-turn conversation or whatever Whenever I notice like the first failure, I just stop and I take note of that failure. That's a heuristic that helps you simplify things, right. Um and like basically you fix the upstream failures before you fix the downstream failures. That's what kind of that results in. So that's like a simplifying way to go about things that tends to tends to work. And like, yeah. And then when you're first doing air analysis, I wouldn't I would just focus on You know\u2026 I wouldn't even try to categorize it off the bat. I would just say like, yeah, just\u2026 observe and learn. About what's going on. Build some intuition and then maybe start categorizing later. Love it. It's diagnostic, right. And not prescriptive at this point. We've got a lot of great questions in here. Could people give a plus or a thumb to questions that they would like answered as well. Because we may not have time for all of them. So have a quick look. In Discord, there is a good question from David about, you mentioned the costs of additional evals and he says we're not just talking monetary costs. I actually think we're not monetary can, but There are complexity costs in terms of having more evals, but what do you think about the space of costs for Increased evaluations, haven't it. Yeah, I mean, there's like, you know, there's definitely Depending on the kind of eval, there's compute APR costs. But the most important one is your attention. You have a limited attention bandwidth. And about what you are going to pay attention to. You need to be very careful like not to completely overwhelm that and have it be high signal. As possible which means like constantly thinking about your evals curating them removing some things that shouldn't be making sure the tests if you have like\u2026 there's, you know, there's There's advice that goes around all the time. It's like. I just saw one that I wrote about one recently a YC company called YC. Case text. Um you know kind of went on this podcast and said hey you should have You should try to get to 100%. Pass rate on your evals. And that is bad. Because if you get 100% pass rate means your eval is not telling you anything. So you should not saturate your evals. You should have them be sufficiently difficult enough so that, hey, like they don't they're telling you something. They're just passing all the time. Might as well not have them. I don't know if that answered the question. Definitely does. We have a question from Namit, which a lot of people would love more insight into. If possible, could you share any examples of quote unquote difficult evals. Difficult emails. Okay, let me see. Let me think of this.", "metadata": {"token_count": 1006, "position": 20, "speaker": "Unknown", "timestamp": "Chunk 21", "workshop_id": "WS3", "original_chunk_id": "9fb263f0-54e7-4f65-9572-d9011737cad8", "source": "sentence_chunking"}, "id": "WS3_9fb263f0-54e7-4f65-9572-d9011737cad8", "relevance": 1.0}, {"text": "Please don't all do it right now because it it will do them in sequence, not in parallel. And I think I've got a bit of a cold start problem because it hasn't run in a tick. But the proof is in the pudding. It's funny that I haven't done this in front of anyone yet and every other time it's worked. The instructions were Peter Van Anderson and Stefan Kroczyk. Fantastic. I'm giving it a pass. Now, any of you who interact with this, I'd really like you to write it because that's what's going to help us iterate, right. And now I'm going to say um reason for writing. It's correct and concise and no extraneous material. You could imagine that if it said the instructors in this workshop are Hugo and Stefan and the sky is blue today um And lunatics are getting into power all over the Western world. That would be Perhaps correct, but it contains information I didn't ask for. And my name is Hugo and i'm just getting you all to do that. So now it's submitted. I just want to show you These are just the basic logs I have. And you'll see. Here. That's the log now. This is stored directly to a modal volume. And I get all the logs. So this is one way to get real. Product feedback is to ask the user for product feedback, essentially. But I've launched this app internally to you all, right. But let's say I wanted to build something. I wanted to get a sense of how a product works before launching it, which I actually did because I didn't want to I didn't want to give you all something that was really bad. Okay. Because I fear embarrassment and shame deep down um but What do you do before you launch something you will generally\u2026 Just test it out yourself, right. Like, give it a query. Who are the instructors. Who is this. Who is that, right. What do they talk about. You can also start defining personas and realistic scenarios to simulate user needs and generate synthetic questions based around them. So you can use LLMs to generate responses. Then you can label outputs manually to define correctness and failure modes. You can build an evaluation harness using those labels. Once again, we'll see how this plays out. Then you can start to compare outputs from multiple models And then you can use failure analysis to drive iteration and improvement. Fight of your analysis. All I'm saying here is a pivot table based on failure modes and then prioritizing what you work on, right. An MVE is a fast, transparent loop for testing and improving LLM apps even before you have real users. So let's get into the details here. Firstly, why do you need evaluation even without users. Then what EDD looks like. Then building the loop and then a bit about observability and instrumentation in DevonProd, which Steph will go more into later this week. So what do you need evaluation, even without users. So this is an all too common story. I have documentation. Let's build a retrieval system, maybe add an agent. Then it's like, oh, we shipped an MVP. Does it work. Or it's like Gemini just dropped 2.5. Let's switch. And then the question is, wait, how do i even know like if it's any good like what do I even do now like i built something exciting Not only do I see this story all the time. It's the story I had with the app that I shipped to you all last week. I was like, we have workshop transcripts. Let's build a retrieval system. If something's not in there, I want to add an ancient later. I shipped it to you all and I was like, does it work. And I was like, well, kind of, but I don't know. And then I want to switch out a model so this isn't a story in the abstract or one I only see out there. It's literally what's just happened with me and I do want to say it's important that this is a common story because so many organizations and people and so many organizations and people Institutions have so many things in docs which aren't leveraged. So it is actually an incredible moment to realize that maybe we can surface Institutional knowledge and personal knowledge in ways never before imagined. But it's can we build systems that do it reliably And usefully. So when people say to me, how do I test it. I say to them, what is the goal of the product. Who will be the user. In what scenarios will they use it. And most of the time they say, oh, we just want people to be able to get anything they want from the docs. And they say, oh, any user is a customer, internal, anyone. And they'll use it in any scenario when they interact with our company. And I say that's not good enough. I'd say that may be something we want at the end, perhaps, but it's not well defined. Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product.", "metadata": {"workshop_id": "WS3", "original_chunk_id": "32698773-187e-4682-a6e0-4dceb4336769", "speaker": "Unknown", "position": 6, "timestamp": "Chunk 7", "token_count": 1061, "source": "sentence_chunking"}, "id": "WS3_32698773-187e-4682-a6e0-4dceb4336769", "relevance": 1.0}, {"text": "Please don't all do it right now because it it will do them in sequence, not in parallel. And I think I've got a bit of a cold start problem because it hasn't run in a tick. But the proof is in the pudding. It's funny that I haven't done this in front of anyone yet and every other time it's worked. The instructions were Peter Van Anderson and Stefan Kroczyk. Fantastic. I'm giving it a pass. Now, any of you who interact with this, I'd really like you to write it because that's what's going to help us iterate, right. And now I'm going to say um reason for writing. It's correct and concise and no extraneous material. You could imagine that if it said the instructors in this workshop are Hugo and Stefan and the sky is blue today um And lunatics are getting into power all over the Western world. That would be Perhaps correct, but it contains information I didn't ask for. And my name is Hugo and i'm just getting you all to do that. So now it's submitted. I just want to show you These are just the basic logs I have. And you'll see. Here. That's the log now. This is stored directly to a modal volume. And I get all the logs. So this is one way to get real. Product feedback is to ask the user for product feedback, essentially. But I've launched this app internally to you all, right. But let's say I wanted to build something. I wanted to get a sense of how a product works before launching it, which I actually did because I didn't want to I didn't want to give you all something that was really bad. Okay. Because I fear embarrassment and shame deep down um but What do you do before you launch something you will generally\u2026 Just test it out yourself, right. Like, give it a query. Who are the instructors. Who is this. Who is that, right. What do they talk about. You can also start defining personas and realistic scenarios to simulate user needs and generate synthetic questions based around them. So you can use LLMs to generate responses. Then you can label outputs manually to define correctness and failure modes. You can build an evaluation harness using those labels. Once again, we'll see how this plays out. Then you can start to compare outputs from multiple models And then you can use failure analysis to drive iteration and improvement. Fight of your analysis. All I'm saying here is a pivot table based on failure modes and then prioritizing what you work on, right. An MVE is a fast, transparent loop for testing and improving LLM apps even before you have real users. So let's get into the details here. Firstly, why do you need evaluation even without users. Then what EDD looks like. Then building the loop and then a bit about observability and instrumentation in DevonProd, which Steph will go more into later this week. So what do you need evaluation, even without users. So this is an all too common story. I have documentation. Let's build a retrieval system, maybe add an agent. Then it's like, oh, we shipped an MVP. Does it work. Or it's like Gemini just dropped 2.5. Let's switch. And then the question is, wait, how do i even know like if it's any good like what do I even do now like i built something exciting Not only do I see this story all the time. It's the story I had with the app that I shipped to you all last week. I was like, we have workshop transcripts. Let's build a retrieval system. If something's not in there, I want to add an ancient later. I shipped it to you all and I was like, does it work. And I was like, well, kind of, but I don't know. And then I want to switch out a model so this isn't a story in the abstract or one I only see out there. It's literally what's just happened with me and I do want to say it's important that this is a common story because so many organizations and people and so many organizations and people Institutions have so many things in docs which aren't leveraged. So it is actually an incredible moment to realize that maybe we can surface Institutional knowledge and personal knowledge in ways never before imagined. But it's can we build systems that do it reliably And usefully. So when people say to me, how do I test it. I say to them, what is the goal of the product. Who will be the user. In what scenarios will they use it. And most of the time they say, oh, we just want people to be able to get anything they want from the docs. And they say, oh, any user is a customer, internal, anyone. And they'll use it in any scenario when they interact with our company. And I say that's not good enough. I'd say that may be something we want at the end, perhaps, but it's not well defined. Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 7", "speaker": "Unknown", "source": "sentence_chunking", "position": 6, "original_chunk_id": "b46329fd-78ef-449b-b353-66cb1057a3f7", "token_count": 1061}, "id": "WS3_b46329fd-78ef-449b-b353-66cb1057a3f7", "relevance": 1.0}, {"text": "You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate. Often you need to look at your data 192 hugo bowne-anderson: and results in quantify performance. This will be a combination of domain expertise, binary classification and then classifying failure modes. And then you want to iterate. So iterate quickly, using prompt engineering, perhaps fine tuning, changing business logic. The reason, I say, perhaps fine tuning is. I think it probably isn't necessary in most cases anymore. But there are some in which it is so. As I mentioned at the start. 193 hugo bowne-anderson: We have our traditional software development lifecycle, which is, I say, straightforward. It's relatively straightforward compared to Llm. Powered software. That's the real statement I want to make don't get traditional software can be horribly challenging to build. But you do start with specs. You build, you test, you deploy. 194 hugo bowne-anderson: and then perhaps you monitor for a while, figure out what's up, make some small changes, whatever it is. And then you start working on the next version, and there will be version releases. Then, right. So you will start working again and iterating. 195 hugo bowne-anderson: based on what you how you see your software is performing. But the point is that you're iterating on the next version of it as opposed to having something continuously changing and continuously iterating. 196 hugo bowne-anderson: So we have software specs 197 hugo bowne-anderson: for Llm. Powered applications right. And this is an iterative process that spans development and production. You have software specs, you build, you deploy your value, you monitor and evaluate, including all of these things that I really I mentioned at the start of this workshop which we'll get into in this course. 198 hugo bowne-anderson: So 199 hugo bowne-anderson: the question I said at the start, how do we get out of aic a a Oc AI poc, I actually should change that because someone said someone actually said to me, they didn't know what a proof of concept was, and they thought I was using the term person of color purgatory. And I I definitely have never intended that so I probably should should change that 200 hugo bowne-anderson: So the 1st challenge non-determinism. How do we deal with that. We log inputs and outputs. We evaluate logs. We iterate on prompts and context, we use Api knobs to reduce variance. 201 hugo bowne-anderson: How about the challenge of hallucinations. 202 hugo bowne-anderson: Well, similarly, we log inputs and outputs in dev and prod, we use domain specific expertise to evaluate output. In both, we build systems and processes to automate assessments such as unit tests and data sets. So testing sets product feedback hooks. Similarly, for for evaluation. 203 hugo bowne-anderson: for iteration, we build a software development lifecycle that enables you to quickly build, deploy, monitor and evaluate. And in terms of business value. We really need to align outputs with business metric and optimize workflows to achieve measurable return on investment. 204 hugo bowne-anderson: So 205 hugo bowne-anderson: without further ado, I would love to jump into Github. I would I have spoken for a while, so I would love if there's been any, any comments in discord that could be relevant. Jeff, William, Nathan Stefan, or anything else anyone would like to add, I really I welcome it. 206 Stefan Krawczyk: Nothing needs to be, I guess, raised in terms of, I guess, just to reiterate what Hugo has been saying so or at least one of the themes you come to understand this course is really, you gotta look at your data right. And so and so this will be one of the key things that hopefully we'll we'll drill into you as the course goes. But. 207 hugo bowne-anderson: Yeah. But Jeff. 208 Stefan Krawczyk: William, or Nathan, or anything. 209 hugo bowne-anderson: I just did want to add, though you do want to look at your data. But something I've learned from you, Stefan, is how much once you've done that, how you can start automating pieces of that right. And so speaking to your workshop next week, but doing the scientific approach, then robustifying it, so to speak. 210 hugo bowne-anderson: But yeah, Hugo. 211 Nathan Danielsen: So 1 1 quick question you that was raised in discord. What did you mean by fine tuning isn't really useful use these days. William gave a good initial answer. But what's your can you. And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 12", "token_count": 1066, "position": 11, "original_chunk_id": "b27608ac-f641-4d73-be5e-7287f87dc21c", "workshop_id": "WS1"}, "id": "WS1_b27608ac-f641-4d73-be5e-7287f87dc21c", "relevance": 1.0}, {"text": "You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate. Often you need to look at your data 192 hugo bowne-anderson: and results in quantify performance. This will be a combination of domain expertise, binary classification and then classifying failure modes. And then you want to iterate. So iterate quickly, using prompt engineering, perhaps fine tuning, changing business logic. The reason, I say, perhaps fine tuning is. I think it probably isn't necessary in most cases anymore. But there are some in which it is so. As I mentioned at the start. 193 hugo bowne-anderson: We have our traditional software development lifecycle, which is, I say, straightforward. It's relatively straightforward compared to Llm. Powered software. That's the real statement I want to make don't get traditional software can be horribly challenging to build. But you do start with specs. You build, you test, you deploy. 194 hugo bowne-anderson: and then perhaps you monitor for a while, figure out what's up, make some small changes, whatever it is. And then you start working on the next version, and there will be version releases. Then, right. So you will start working again and iterating. 195 hugo bowne-anderson: based on what you how you see your software is performing. But the point is that you're iterating on the next version of it as opposed to having something continuously changing and continuously iterating. 196 hugo bowne-anderson: So we have software specs 197 hugo bowne-anderson: for Llm. Powered applications right. And this is an iterative process that spans development and production. You have software specs, you build, you deploy your value, you monitor and evaluate, including all of these things that I really I mentioned at the start of this workshop which we'll get into in this course. 198 hugo bowne-anderson: So 199 hugo bowne-anderson: the question I said at the start, how do we get out of aic a a Oc AI poc, I actually should change that because someone said someone actually said to me, they didn't know what a proof of concept was, and they thought I was using the term person of color purgatory. And I I definitely have never intended that so I probably should should change that 200 hugo bowne-anderson: So the 1st challenge non-determinism. How do we deal with that. We log inputs and outputs. We evaluate logs. We iterate on prompts and context, we use Api knobs to reduce variance. 201 hugo bowne-anderson: How about the challenge of hallucinations. 202 hugo bowne-anderson: Well, similarly, we log inputs and outputs in dev and prod, we use domain specific expertise to evaluate output. In both, we build systems and processes to automate assessments such as unit tests and data sets. So testing sets product feedback hooks. Similarly, for for evaluation. 203 hugo bowne-anderson: for iteration, we build a software development lifecycle that enables you to quickly build, deploy, monitor and evaluate. And in terms of business value. We really need to align outputs with business metric and optimize workflows to achieve measurable return on investment. 204 hugo bowne-anderson: So 205 hugo bowne-anderson: without further ado, I would love to jump into Github. I would I have spoken for a while, so I would love if there's been any, any comments in discord that could be relevant. Jeff, William, Nathan Stefan, or anything else anyone would like to add, I really I welcome it. 206 Stefan Krawczyk: Nothing needs to be, I guess, raised in terms of, I guess, just to reiterate what Hugo has been saying so or at least one of the themes you come to understand this course is really, you gotta look at your data right. And so and so this will be one of the key things that hopefully we'll we'll drill into you as the course goes. But. 207 hugo bowne-anderson: Yeah. But Jeff. 208 Stefan Krawczyk: William, or Nathan, or anything. 209 hugo bowne-anderson: I just did want to add, though you do want to look at your data. But something I've learned from you, Stefan, is how much once you've done that, how you can start automating pieces of that right. And so speaking to your workshop next week, but doing the scientific approach, then robustifying it, so to speak. 210 hugo bowne-anderson: But yeah, Hugo. 211 Nathan Danielsen: So 1 1 quick question you that was raised in discord. What did you mean by fine tuning isn't really useful use these days. William gave a good initial answer. But what's your can you. And upon that, just a little bit more. 212 hugo bowne-anderson: So did I say.", "metadata": {"timestamp": "Chunk 12", "workshop_id": "WS1", "token_count": 1066, "position": 11, "original_chunk_id": "888c931c-9e92-487d-be09-286a3064367d", "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS1_888c931c-9e92-487d-be09-286a3064367d", "relevance": 1.0}]}}