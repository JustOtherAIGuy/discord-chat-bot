{"question": "Can you explain how to effectively use prompt engineering to improve the quality of the outputs from my LLM in a data analysis context?", "response": "To enhance the quality of outputs from your LLM in data analysis, start by clearly defining your objectives and the specific tasks you want the model to perform. Here\u2019s a step-by-step approach:\n\n1. **Identify Key Tasks**: Break down your data analysis into atomic tasks, such as data extraction, summarization, or generating insights. This helps in creating focused prompts.\n\n2. **Use Structured Prompts**: Design prompts that provide context and specify the desired output format. For example, when extracting data, include examples of the expected structure.\n\n3. **Iterate with Examples**: Provide both good and bad examples in your prompts. This helps the model learn what constitutes a quality response. For instance, if summarizing a report, show examples of concise versus verbose summaries.\n\n4. **Incorporate Evaluation Criteria**: Define what makes an output acceptable. Criteria could include accuracy, relevance, and completeness. This guides the model in generating better responses.\n\n5. **Leverage Observability**: Monitor the outputs closely. Use tools to analyze input-output pairs, identifying patterns where the model performs well or poorly. This data can inform further prompt refinements.\n\n6. **Test and Iterate**: Regularly test variations of your prompts and evaluate their effectiveness. Use A/B testing to compare different approaches and refine based on performance.\n\nBy following these steps, you can systematically improve the quality of your LLM outputs in data analysis tasks.", "context_info": {"num_chunks": 10, "context_tokens": 10408, "completion_tokens": 289, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS6"], "chunks": [{"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"position": 13, "timestamp": "Chunk 14", "workshop_id": "WS6", "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "303c95ae-1d27-4792-ab45-321e8d4778a5", "token_count": 1056}, "id": "WS6_303c95ae-1d27-4792-ab45-321e8d4778a5", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"timestamp": "Chunk 14", "speaker": "Unknown", "source": "sentence_chunking", "position": 13, "original_chunk_id": "e3b959c4-0d2f-4659-bc95-7dd30946df77", "workshop_id": "WS6", "token_count": 1056}, "id": "WS6_e3b959c4-0d2f-4659-bc95-7dd30946df77", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"source": "sentence_chunking", "workshop_id": "WS3", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "position": 10, "timestamp": "Chunk 11", "token_count": 1062, "speaker": "Unknown"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "token_count": 1062, "workshop_id": "WS3", "timestamp": "Chunk 11", "speaker": "Unknown", "source": "sentence_chunking", "position": 10}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"workshop_id": "WS6", "original_chunk_id": "981ee013-55da-46a9-a83e-dd1b1322ac44", "speaker": "Unknown", "timestamp": "Chunk 19", "token_count": 1062, "source": "sentence_chunking", "position": 18}, "id": "WS6_981ee013-55da-46a9-a83e-dd1b1322ac44", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"timestamp": "Chunk 19", "token_count": 1062, "position": 18, "original_chunk_id": "0023cc64-b3ca-486f-9dd8-dbbc987bf306", "speaker": "Unknown", "workshop_id": "WS6", "source": "sentence_chunking"}, "id": "WS6_0023cc64-b3ca-486f-9dd8-dbbc987bf306", "relevance": 1.0}, {"text": "And this was a case where, like. 479 Phillip Carter: I didn't need a sophisticated Eval to tell me that this was a problem. I could just literally see it in the inputs and outputs. And I'm like, Okay, this is categorically wrong here. And what do I do to fix this. Well. 480 Phillip Carter: I didn't have error rates, or like some variation of what is my error rate in the few shot examples in the prompt. So I just put it in there and then it stopped happening. I couldn't reproduce like I could. I could reproduce that problem before, but then I failed to reproduce it after I put the few shot example in there 481 Phillip Carter: and then deployed it, and then I verified again like this is why observability is really important. I verified after 24\u00a0h or so of of usage that like this actually did mitigate the problem substantially. And and so like, that's where I think, like. 482 Phillip Carter: you know, I'm saying observability a lot. But but, like, you know, it's as if I work for an observability company. But I think it's really really important for doing any kind of Ml. Work that, like 483 Phillip Carter: the work that you do is not divorced from production, like production, is the true reality of what is actually happening to your systems. And like, that's what's going to be the source of whatever fixes you have to do and whatever makes it as easy as possible to get that real data and pull it back into whatever development environment you have is going to save you so much time like in our case we designed it. So it's super super easy for us to just see that. 484 Phillip Carter: And whether we pull something down and annotate it for an Eval, or if we just say, Oh, well, we're just going to fix that right now, or just try a fix and see how it works. We have that freedom. And it was really easy. So yeah, there's those kinds of levers. Our retrieval is very often a lever. I think it probably shouldn't surprise too many people that if you're doing rag of any sort, like the kind of output that you get is going to be a function of how good the retrieval process was 485 Phillip Carter: In our case we learned a couple different things like honeycomb, has a concept of a stale column in a schema, and the name of a column 486 Phillip Carter: might be relevant from a semantic standpoint to the input that somebody gave. But if it is a stale column, then it's not worth bringing in, because it won't have any data. It's just a name of a column like, if you try to query against this column, you're going to get no results. And then that actually results in a really bad user experience. So you want to say, Okay, well, my rag process is not just doing semantic similarity or basic hybrid search. It is incorporating other aspects like 487 Phillip Carter: data liveness. And we've not done this. But I think we could go down the rabbit hole on on rag in particular, and explore a lot of different options in parallel, that we have some beliefs that will significantly improve query assistant 488 Phillip Carter: compared to how it exists today and make it much more relevant, for, like harder tasks that some of our, some of our larger customers have, so I don't know. Anyways like those are the the main levers that I had there. Fine tuning is sort of like the nuclear option in my mind, because, like, you need to have this whole more 489 Phillip Carter: larger process of generating really good representative data. And then you fine tune the thing. And you got to figure out how you're going to measure it. And like, do you need to have different evals for the fine-tuned model compared to what you have for Gpt. 3.5. I'm not sure if anybody's really written that. But like, if you use an Eval data set to fine tune, a model like, is that good. Is that bad like in our case. It was okay. But like we didn't 490 Phillip Carter: and anyways, I I don't wanna talk about that one. I feel like we've talked about forever. But 491 Phillip Carter: bottom line when you have it really easy to see your data, whether it's through an observability tool or in a spreadsheet, or whatever. You can just make it very readily apparent that, like certain inputs, do not yield good outputs, and sometimes you can just fix it with a few shot, prompt or 492 Phillip Carter: try prompt variation and and do an ab test and measure how that performs like. There's a lot of tools at your disposal that are pretty easy to deploy once you have that baseline understanding of how inputs and outputs sort of correlate with each other. 493 hugo bowne-anderson: Amazing.", "metadata": {"source": "sentence_chunking", "token_count": 1040, "timestamp": "Chunk 22", "original_chunk_id": "65302b25-b6f4-4bef-b10a-6cf12069ce95", "speaker": "Unknown", "workshop_id": "WS6", "position": 21}, "id": "WS6_65302b25-b6f4-4bef-b10a-6cf12069ce95", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"position": 14, "speaker": "Unknown", "source": "sentence_chunking", "token_count": 1068, "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "workshop_id": "WS3", "timestamp": "Chunk 15"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "position": 14, "timestamp": "Chunk 15", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "source": "sentence_chunking", "workshop_id": "WS3", "token_count": 1068}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "And this was a case where, like. 479 Phillip Carter: I didn't need a sophisticated Eval to tell me that this was a problem. I could just literally see it in the inputs and outputs. And I'm like, Okay, this is categorically wrong here. And what do I do to fix this. Well. 480 Phillip Carter: I didn't have error rates, or like some variation of what is my error rate in the few shot examples in the prompt. So I just put it in there and then it stopped happening. I couldn't reproduce like I could. I could reproduce that problem before, but then I failed to reproduce it after I put the few shot example in there 481 Phillip Carter: and then deployed it, and then I verified again like this is why observability is really important. I verified after 24\u00a0h or so of of usage that like this actually did mitigate the problem substantially. And and so like, that's where I think, like. 482 Phillip Carter: you know, I'm saying observability a lot. But but, like, you know, it's as if I work for an observability company. But I think it's really really important for doing any kind of Ml. Work that, like 483 Phillip Carter: the work that you do is not divorced from production, like production, is the true reality of what is actually happening to your systems. And like, that's what's going to be the source of whatever fixes you have to do and whatever makes it as easy as possible to get that real data and pull it back into whatever development environment you have is going to save you so much time like in our case we designed it. So it's super super easy for us to just see that. 484 Phillip Carter: And whether we pull something down and annotate it for an Eval, or if we just say, Oh, well, we're just going to fix that right now, or just try a fix and see how it works. We have that freedom. And it was really easy. So yeah, there's those kinds of levers. Our retrieval is very often a lever. I think it probably shouldn't surprise too many people that if you're doing rag of any sort, like the kind of output that you get is going to be a function of how good the retrieval process was 485 Phillip Carter: In our case we learned a couple different things like honeycomb, has a concept of a stale column in a schema, and the name of a column 486 Phillip Carter: might be relevant from a semantic standpoint to the input that somebody gave. But if it is a stale column, then it's not worth bringing in, because it won't have any data. It's just a name of a column like, if you try to query against this column, you're going to get no results. And then that actually results in a really bad user experience. So you want to say, Okay, well, my rag process is not just doing semantic similarity or basic hybrid search. It is incorporating other aspects like 487 Phillip Carter: data liveness. And we've not done this. But I think we could go down the rabbit hole on on rag in particular, and explore a lot of different options in parallel, that we have some beliefs that will significantly improve query assistant 488 Phillip Carter: compared to how it exists today and make it much more relevant, for, like harder tasks that some of our, some of our larger customers have, so I don't know. Anyways like those are the the main levers that I had there. Fine tuning is sort of like the nuclear option in my mind, because, like, you need to have this whole more 489 Phillip Carter: larger process of generating really good representative data. And then you fine tune the thing. And you got to figure out how you're going to measure it. And like, do you need to have different evals for the fine-tuned model compared to what you have for Gpt. 3.5. I'm not sure if anybody's really written that. But like, if you use an Eval data set to fine tune, a model like, is that good. Is that bad like in our case. It was okay. But like we didn't 490 Phillip Carter: and anyways, I I don't wanna talk about that one. I feel like we've talked about forever. But 491 Phillip Carter: bottom line when you have it really easy to see your data, whether it's through an observability tool or in a spreadsheet, or whatever. You can just make it very readily apparent that, like certain inputs, do not yield good outputs, and sometimes you can just fix it with a few shot, prompt or 492 Phillip Carter: try prompt variation and and do an ab test and measure how that performs like. There's a lot of tools at your disposal that are pretty easy to deploy once you have that baseline understanding of how inputs and outputs sort of correlate with each other. 493 hugo bowne-anderson: Amazing.", "metadata": {"workshop_id": "WS6", "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "7fae5add-2859-45d9-8cbc-9204e6369132", "timestamp": "Chunk 22", "position": 21, "token_count": 1040}, "id": "WS6_7fae5add-2859-45d9-8cbc-9204e6369132", "relevance": 1.0}]}}{"question": "Can you explain the concept of retrieval-augmented generation (RAG) and how it can enhance the capabilities of LLMs in my workflow?", "response": "Retrieval-augmented generation (RAG) is a powerful approach that combines the strengths of information retrieval with language model generation. It enhances the capabilities of large language models (LLMs) by allowing them to access and utilize external information, which is crucial for filling knowledge gaps and providing up-to-date, context-specific responses.\n\nIn your workflow, RAG can be implemented by embedding your documents into a vector database. When a query is made, the system retrieves relevant information from this database, which is then used to inform the LLM's output. This process not only improves accuracy but also helps ground the model's responses in real, relevant data, reducing the risk of hallucination.\n\nTo effectively integrate RAG, consider these steps:\n1. **Document Preparation**: Organize your documents for embedding.\n2. **Embedding**: Convert your text into vectors for efficient retrieval.\n3. **Querying**: Use the LLM to generate responses based on the retrieved context.\n4. **Feedback Loop**: Continuously refine your retrieval methods to enhance relevance and efficiency.\n\nBy leveraging RAG, you can significantly improve the relevance and accuracy of the outputs generated by your LLM, making it a valuable addition to your workflow.", "context_info": {"num_chunks": 10, "context_tokens": 10083, "completion_tokens": 246, "embedding_tokens": 15360, "workshops_used": ["WS4", "WS5", "WS6"], "chunks": [{"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"workshop_id": "WS5", "timestamp": "Chunk 7", "speaker": "Unknown", "token_count": 1023, "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "position": 6, "source": "sentence_chunking"}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246", "token_count": 1023, "speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 7", "position": 6, "workshop_id": "WS5"}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "But these types of tool uses are incredibly important. And then there's retrieval. 98 hugo bowne-anderson: which is what we're here to talk talk about today. But the idea is, if you have a corpus of documents, how do you. How do you give that information to your Llm. And how do you. How do you query it. So these are the 3, the 3 pillars of thinking about building agents and thinking about 99 hugo bowne-anderson: like giving your Llm systems more capabilities. Essentially. We're at the early stages of working with this type of quote unquote software and figuring out how to give it a bit more that it can do to help us. In the work we want to do. Okay. So 100 hugo bowne-anderson: firstly, why do we even need to add context. Okay, so I think that a lot of this should be obvious. But we want to fill knowledge gaps. 101 hugo bowne-anderson: Llms are trained on fixed data sets. Can't quote unquote, know everything we can give it external information for domain, specific proprietary, time-sensitive knowledge. 102 hugo bowne-anderson: it can help ground and increase accuracy. So we've seen systems. And this is something in llama indexes favor. And for you, all of you who played around with the app I've built. I've seen some of you have asked the question that you knew wasn't in the Transcript, and it replied, this isn't in the Transcript, whatever, and you gave it a pass. You're like that's good. It didn't hallucinate something there, right. So grounding in the actual documents can work to constrain the models outputs now still know we do this through prompting, like 103 hugo bowne-anderson: we literally in the prompt say, hey. If you can't find it, please do not. Pretty. Please do not hallucinate my Llm, you know. So, and that's also the page it like we've got people at Deepmind who are like, just put it in the prompt right. So. And I don't mean to be flippant about that. But it 104 hugo bowne-anderson: it's a pretty interesting way to build software as far as I'm concerned, and it it works better than other ways. But it is a pretty wild thing to be doing. We get task specific relevance. 105 hugo bowne-anderson: We can get dynamic and up to date information. 106 hugo bowne-anderson: and this is actually one of my favorite uses. It can enable explainability. And I know William and I discussed this in in January, and there's some nuance to this, and I'm not actually only talking about rag here. I'll share these links. Actually let me. I've got them all 107 hugo bowne-anderson: or 2 of them here. 108 hugo bowne-anderson: And I'm just gonna oh, 3 of them. I'll paste them in discord. 109 hugo bowne-anderson: I love this title. It's classic. Tim O'reilly. The r and Rag stands for stands for royalties. And what O'reilly has done is they've used rag systems 110 hugo bowne-anderson: on their books. Sorry they've used the Reg system on their catalog of books so that I can ask a question when I'm in their platform, and it returns 111 hugo bowne-anderson: sources from particular books, and then I can click through to them, and if the source is helpful for me, then 112 hugo bowne-anderson: the rag system allows O'reilly to pay royalties to that that author. And I think this is an incredible use of this technology. To be perfectly honest, I think, William, and and correct me if I'm wrong, William. But William made a point that this doesn't mean it has to be rag right. There are other information retrieval systems that we can. 113 hugo bowne-anderson: you know, used for attribution and that type of stuff. And I totally agree with that that critique 114 hugo bowne-anderson: But it is something which allows us to think more more robustly through attribution and in classic Tim O'reilly style. He's talked about it as one way to think about fixing AI's original sin. And of course we do live in a space which we find incredibly useful. 115 hugo bowne-anderson: but where the big labs have essentially sucked up the world's information without paying anything 116 hugo bowne-anderson: for it. Of course, this isn't new. I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this.", "metadata": {"original_chunk_id": "514ae663-d4d7-4116-820c-2566e481c591", "token_count": 1035, "source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 6", "workshop_id": "WS5", "position": 5}, "id": "WS5_514ae663-d4d7-4116-820c-2566e481c591", "relevance": 1.0}, {"text": "But these types of tool uses are incredibly important. And then there's retrieval. 98 hugo bowne-anderson: which is what we're here to talk talk about today. But the idea is, if you have a corpus of documents, how do you. How do you give that information to your Llm. And how do you. How do you query it. So these are the 3, the 3 pillars of thinking about building agents and thinking about 99 hugo bowne-anderson: like giving your Llm systems more capabilities. Essentially. We're at the early stages of working with this type of quote unquote software and figuring out how to give it a bit more that it can do to help us. In the work we want to do. Okay. So 100 hugo bowne-anderson: firstly, why do we even need to add context. Okay, so I think that a lot of this should be obvious. But we want to fill knowledge gaps. 101 hugo bowne-anderson: Llms are trained on fixed data sets. Can't quote unquote, know everything we can give it external information for domain, specific proprietary, time-sensitive knowledge. 102 hugo bowne-anderson: it can help ground and increase accuracy. So we've seen systems. And this is something in llama indexes favor. And for you, all of you who played around with the app I've built. I've seen some of you have asked the question that you knew wasn't in the Transcript, and it replied, this isn't in the Transcript, whatever, and you gave it a pass. You're like that's good. It didn't hallucinate something there, right. So grounding in the actual documents can work to constrain the models outputs now still know we do this through prompting, like 103 hugo bowne-anderson: we literally in the prompt say, hey. If you can't find it, please do not. Pretty. Please do not hallucinate my Llm, you know. So, and that's also the page it like we've got people at Deepmind who are like, just put it in the prompt right. So. And I don't mean to be flippant about that. But it 104 hugo bowne-anderson: it's a pretty interesting way to build software as far as I'm concerned, and it it works better than other ways. But it is a pretty wild thing to be doing. We get task specific relevance. 105 hugo bowne-anderson: We can get dynamic and up to date information. 106 hugo bowne-anderson: and this is actually one of my favorite uses. It can enable explainability. And I know William and I discussed this in in January, and there's some nuance to this, and I'm not actually only talking about rag here. I'll share these links. Actually let me. I've got them all 107 hugo bowne-anderson: or 2 of them here. 108 hugo bowne-anderson: And I'm just gonna oh, 3 of them. I'll paste them in discord. 109 hugo bowne-anderson: I love this title. It's classic. Tim O'reilly. The r and Rag stands for stands for royalties. And what O'reilly has done is they've used rag systems 110 hugo bowne-anderson: on their books. Sorry they've used the Reg system on their catalog of books so that I can ask a question when I'm in their platform, and it returns 111 hugo bowne-anderson: sources from particular books, and then I can click through to them, and if the source is helpful for me, then 112 hugo bowne-anderson: the rag system allows O'reilly to pay royalties to that that author. And I think this is an incredible use of this technology. To be perfectly honest, I think, William, and and correct me if I'm wrong, William. But William made a point that this doesn't mean it has to be rag right. There are other information retrieval systems that we can. 113 hugo bowne-anderson: you know, used for attribution and that type of stuff. And I totally agree with that that critique 114 hugo bowne-anderson: But it is something which allows us to think more more robustly through attribution and in classic Tim O'reilly style. He's talked about it as one way to think about fixing AI's original sin. And of course we do live in a space which we find incredibly useful. 115 hugo bowne-anderson: but where the big labs have essentially sucked up the world's information without paying anything 116 hugo bowne-anderson: for it. Of course, this isn't new. I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this.", "metadata": {"position": 5, "workshop_id": "WS5", "token_count": 1035, "original_chunk_id": "4c8cc0f3-45ae-49a4-bc9c-be2ec3e5019e", "source": "sentence_chunking", "timestamp": "Chunk 6", "speaker": "Unknown"}, "id": "WS5_4c8cc0f3-45ae-49a4-bc9c-be2ec3e5019e", "relevance": 1.0}, {"text": "58 hugo bowne-anderson: and then to trigger actions. So activate appropriate strategies, tools or next steps based on context and inputs. So a lot of people say, make decisions or dynamically, choose Llms aren't making decisions, and they're not choosing anything in terms of having like 59 hugo bowne-anderson: what we assume is agency, at least with ourselves and and other humans. But they do trigger actions based on context and inputs. And actually, when we 60 hugo bowne-anderson: let me bring up this Whoa, bring up this blog post. 61 hugo bowne-anderson: We discussed this yesterday with Marva from hugging face. But we'll talk about that's not it. 62 hugo bowne-anderson: We'll talk about how anthropic has has approached this, which is super useful, but I do want to say this. Beautiful blog post from 63 hugo bowne-anderson: hugging face. I'm just gonna paste it in our workshop channel. 64 hugo bowne-anderson: Let's see. Where did they say, sorry about this. Oh, yeah. 65 hugo bowne-anderson: AI agents are programs where Llm outputs control the workflow. Okay. 66 hugo bowne-anderson: that's a really nice, I think. 67 hugo bowne-anderson: definition or heuristic for thinking about what these, what these systems do. Okay, so 68 hugo bowne-anderson: and just to be clear, chat Gpt is agentic. So I can't even remember what example 69 hugo bowne-anderson: this is, let's see, what did I do here. 70 hugo bowne-anderson: hey, Chatgpt. I would like you to develop a marketing campaign for a product that yes, I'm getting to develop. I'm speaking to develop a marketing campaign. And 71 hugo bowne-anderson: I'm actually just okay. 72 hugo bowne-anderson: Let me see if I can. 73 hugo bowne-anderson: Sorry about this people who would have thought embedding Youtube videos in wouldn't work. 74 hugo bowne-anderson: So I got it to develop a marketing campaign for a product that inserts advertising in people's dreams. 75 hugo bowne-anderson: okay. And you may recall the Nicolas Cage Movie Dream Scenario. I came up with this before that movie, though, although Nicolas Cage does seem to steal all my good ideas, 76 hugo bowne-anderson: And I asked for visuals. Right. And I didn't say, use these tools or whatever. And it quote unquote decided, or its output triggered using Dali 3. Okay. And it's been doing that for some time. Similarly, if I ask for something that maybe outside, maybe outside its weights. 77 hugo bowne-anderson: it will 78 hugo bowne-anderson: it will automatically search the web right without me telling it to. So there is a dynamic response to Llm output that results in it. Using a tool. Okay. And you can try it. For example, you can say, what's the weather today in Sydney to an Llm. Or wherever you are. And it will often use a web search tool without asking asking you whether you want it or not. And without you telling it, right. So a lot of these systems are already agentic. 79 hugo bowne-anderson: But okay, I do want to clarify. Agent is mostly a marketing term. And that's why we were all 80 hugo bowne-anderson: very pleased when anthropic put out this blog post 81 hugo bowne-anderson: called building effective Llms. And actually, could someone paste that in in the chat. I thought I had the link here. But in the interest of time. I won't. 82 hugo bowne-anderson: You're on finding it. I mean, it's 1 that I suppose I mentioned most workshops. Actually, but it's essential. And it's essential to how I currently think about augmenting these systems. So we've talked about 83 hugo bowne-anderson: the importance of memory which a lot of people really don't talk about. And I asked Eddie from Fondue to kind of come and give a talk about like best practices for starting, working, working with memory, and and 2 patterns in particular. Then we can augment Llms by 84 hugo bowne-anderson: having retrieval right. And that's often in the form of rag as we went through earlier this week. But we want to be able to ground it in the context of documents and not just its weights.", "metadata": {"position": 3, "speaker": "Unknown", "workshop_id": "WS6", "token_count": 1035, "timestamp": "Chunk 4", "original_chunk_id": "66b88cac-129a-470a-8b42-67de4ab93364", "source": "sentence_chunking"}, "id": "WS6_66b88cac-129a-470a-8b42-67de4ab93364", "relevance": 1.0}, {"text": "58 hugo bowne-anderson: and then to trigger actions. So activate appropriate strategies, tools or next steps based on context and inputs. So a lot of people say, make decisions or dynamically, choose Llms aren't making decisions, and they're not choosing anything in terms of having like 59 hugo bowne-anderson: what we assume is agency, at least with ourselves and and other humans. But they do trigger actions based on context and inputs. And actually, when we 60 hugo bowne-anderson: let me bring up this Whoa, bring up this blog post. 61 hugo bowne-anderson: We discussed this yesterday with Marva from hugging face. But we'll talk about that's not it. 62 hugo bowne-anderson: We'll talk about how anthropic has has approached this, which is super useful, but I do want to say this. Beautiful blog post from 63 hugo bowne-anderson: hugging face. I'm just gonna paste it in our workshop channel. 64 hugo bowne-anderson: Let's see. Where did they say, sorry about this. Oh, yeah. 65 hugo bowne-anderson: AI agents are programs where Llm outputs control the workflow. Okay. 66 hugo bowne-anderson: that's a really nice, I think. 67 hugo bowne-anderson: definition or heuristic for thinking about what these, what these systems do. Okay, so 68 hugo bowne-anderson: and just to be clear, chat Gpt is agentic. So I can't even remember what example 69 hugo bowne-anderson: this is, let's see, what did I do here. 70 hugo bowne-anderson: hey, Chatgpt. I would like you to develop a marketing campaign for a product that yes, I'm getting to develop. I'm speaking to develop a marketing campaign. And 71 hugo bowne-anderson: I'm actually just okay. 72 hugo bowne-anderson: Let me see if I can. 73 hugo bowne-anderson: Sorry about this people who would have thought embedding Youtube videos in wouldn't work. 74 hugo bowne-anderson: So I got it to develop a marketing campaign for a product that inserts advertising in people's dreams. 75 hugo bowne-anderson: okay. And you may recall the Nicolas Cage Movie Dream Scenario. I came up with this before that movie, though, although Nicolas Cage does seem to steal all my good ideas, 76 hugo bowne-anderson: And I asked for visuals. Right. And I didn't say, use these tools or whatever. And it quote unquote decided, or its output triggered using Dali 3. Okay. And it's been doing that for some time. Similarly, if I ask for something that maybe outside, maybe outside its weights. 77 hugo bowne-anderson: it will 78 hugo bowne-anderson: it will automatically search the web right without me telling it to. So there is a dynamic response to Llm output that results in it. Using a tool. Okay. And you can try it. For example, you can say, what's the weather today in Sydney to an Llm. Or wherever you are. And it will often use a web search tool without asking asking you whether you want it or not. And without you telling it, right. So a lot of these systems are already agentic. 79 hugo bowne-anderson: But okay, I do want to clarify. Agent is mostly a marketing term. And that's why we were all 80 hugo bowne-anderson: very pleased when anthropic put out this blog post 81 hugo bowne-anderson: called building effective Llms. And actually, could someone paste that in in the chat. I thought I had the link here. But in the interest of time. I won't. 82 hugo bowne-anderson: You're on finding it. I mean, it's 1 that I suppose I mentioned most workshops. Actually, but it's essential. And it's essential to how I currently think about augmenting these systems. So we've talked about 83 hugo bowne-anderson: the importance of memory which a lot of people really don't talk about. And I asked Eddie from Fondue to kind of come and give a talk about like best practices for starting, working, working with memory, and and 2 patterns in particular. Then we can augment Llms by 84 hugo bowne-anderson: having retrieval right. And that's often in the form of rag as we went through earlier this week. But we want to be able to ground it in the context of documents and not just its weights.", "metadata": {"timestamp": "Chunk 4", "source": "sentence_chunking", "position": 3, "speaker": "Unknown", "original_chunk_id": "4baa84b2-b8c2-47a4-8b22-0ba866ab1cbd", "token_count": 1035, "workshop_id": "WS6"}, "id": "WS6_4baa84b2-b8c2-47a4-8b22-0ba866ab1cbd", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"timestamp": "Chunk 23", "speaker": "Unknown", "original_chunk_id": "4eedb1cc-f056-44e3-9b97-0d4db328d700", "position": 22, "token_count": 1036, "source": "sentence_chunking", "workshop_id": "WS4"}, "id": "WS4_4eedb1cc-f056-44e3-9b97-0d4db328d700", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"workshop_id": "WS4", "token_count": 1036, "source": "sentence_chunking", "timestamp": "Chunk 23", "original_chunk_id": "cd32a46d-1bc2-42cc-bf15-ddc5bb384a17", "speaker": "Unknown", "position": 22}, "id": "WS4_cd32a46d-1bc2-42cc-bf15-ddc5bb384a17", "relevance": 1.0}, {"text": "And once you do this it'll become clear. Oh, if I improve my information retrieval, I'll get 5% lift. 75 hugo bowne-anderson: If I improve the way I'm ingesting Pdfs, I'll actually get 20% lift right. So it's, you know, once again, use all the multi agentic multiturn, infinite rag, you know 76 hugo bowne-anderson: one 77 hugo bowne-anderson: trillion token context windows that that you want. Just know if you're looking to improve your business metrics. This this process will get you a long way. And then in workshop 4. Stefan took us all through the 2 iteration loops of testing in dev and and production, and even introduced the concept how to implement crcd, and these types of things. And we even gave you a blueprint for doing that with Github pull requests. 78 hugo bowne-anderson: So I'm excited to see if any of you want to play around with. 79 hugo bowne-anderson: So today, we're going to focus on context. 80 hugo bowne-anderson: So the basic idea here. And this is something William Horton made made a point of, and IA hundred 10% agree with one of the 81 hugo bowne-anderson: one of the 82 hugo bowne-anderson: most impactful use cases for Llms that I've seen in businesses and in personal use is people being able to query internal documents. 83 hugo bowne-anderson: Right. And so thinking about. Then how do we build systems which allow people to do this robustly, quickly, efficiently, right. So how do we get external context into the Llm system. Right. And how do we make sure it's not like just drawing information from its weights as well. So that's really what we're here to talk about today. 84 hugo bowne-anderson: To put it another way, and we'll be going through this blog post in detail in the next workshop. This is from. 85 hugo bowne-anderson: Let me just share the link in discord. And this is a blog post that I you know I'm 86 hugo bowne-anderson: I really. I mentioned most workshops to be honest. 87 hugo bowne-anderson: I just saw Jeff's yeah, totally. 88 hugo bowne-anderson: So this is a blog post called building effective agents. And the basic idea here is that, you know, if we're going to talk about agents, we probably should define it, and we should probably think about 89 hugo bowne-anderson: how do we move from a single Llm. Call 90 hugo bowne-anderson: to something more agentic and 91 hugo bowne-anderson: Barry and Eric at anthropic formulated as augmented Llms. Where we start building different types of workflows that we can think about as being on the agentic continuum. But even before that, knowing that Llms 92 hugo bowne-anderson: you give it tokens, it gives tokens out okay. And then along the continuum to building agentic type things, then we integrate other types of tooling and properties into the system. So the 1st one to think about really is, is memory right. Well, it may not be the first, st but it's an important one, and we've discussed that, in brief. And that's why I had Eddie from Fondue to come and talk to you about simple ways to think about memory last week. 93 hugo bowne-anderson: The next one is 94 hugo bowne-anderson: tool calls, okay, and this is something we're going to be talking about later later this week. And, in fact, for anyone who wants to join the the workshop with ravine next next week, doing a lot of 95 hugo bowne-anderson: tool calls, using Olama and Gradio and Gemma. 3 are all all locally, which will be super fun. But you could imagine, for example. 96 hugo bowne-anderson: you want to get the weather today somewhere and the cutoff date of Vlm you're working with was who knows when. And so you can implement a basic tool call to a weather Api. Similarly, you'll notice that chat Gpt. Perhaps these days you'll ask it something. You won't tell it to search the web, but it will recognize that perhaps 97 hugo bowne-anderson: searching the web will help it. Now, who knows whether that's true or not. To be honest. But you'll see it will dynamically search the web without even being told to, and that's more along the agentic side. But these types of tool uses are incredibly important. And then there's retrieval.", "metadata": {"workshop_id": "WS5", "position": 4, "source": "sentence_chunking", "original_chunk_id": "4d2d3e29-8883-4bcd-9c1a-fe7bbe9df289", "token_count": 1050, "speaker": "Unknown", "timestamp": "Chunk 5"}, "id": "WS5_4d2d3e29-8883-4bcd-9c1a-fe7bbe9df289", "relevance": 1.0}, {"text": "And once you do this it'll become clear. Oh, if I improve my information retrieval, I'll get 5% lift. 75 hugo bowne-anderson: If I improve the way I'm ingesting Pdfs, I'll actually get 20% lift right. So it's, you know, once again, use all the multi agentic multiturn, infinite rag, you know 76 hugo bowne-anderson: one 77 hugo bowne-anderson: trillion token context windows that that you want. Just know if you're looking to improve your business metrics. This this process will get you a long way. And then in workshop 4. Stefan took us all through the 2 iteration loops of testing in dev and and production, and even introduced the concept how to implement crcd, and these types of things. And we even gave you a blueprint for doing that with Github pull requests. 78 hugo bowne-anderson: So I'm excited to see if any of you want to play around with. 79 hugo bowne-anderson: So today, we're going to focus on context. 80 hugo bowne-anderson: So the basic idea here. And this is something William Horton made made a point of, and IA hundred 10% agree with one of the 81 hugo bowne-anderson: one of the 82 hugo bowne-anderson: most impactful use cases for Llms that I've seen in businesses and in personal use is people being able to query internal documents. 83 hugo bowne-anderson: Right. And so thinking about. Then how do we build systems which allow people to do this robustly, quickly, efficiently, right. So how do we get external context into the Llm system. Right. And how do we make sure it's not like just drawing information from its weights as well. So that's really what we're here to talk about today. 84 hugo bowne-anderson: To put it another way, and we'll be going through this blog post in detail in the next workshop. This is from. 85 hugo bowne-anderson: Let me just share the link in discord. And this is a blog post that I you know I'm 86 hugo bowne-anderson: I really. I mentioned most workshops to be honest. 87 hugo bowne-anderson: I just saw Jeff's yeah, totally. 88 hugo bowne-anderson: So this is a blog post called building effective agents. And the basic idea here is that, you know, if we're going to talk about agents, we probably should define it, and we should probably think about 89 hugo bowne-anderson: how do we move from a single Llm. Call 90 hugo bowne-anderson: to something more agentic and 91 hugo bowne-anderson: Barry and Eric at anthropic formulated as augmented Llms. Where we start building different types of workflows that we can think about as being on the agentic continuum. But even before that, knowing that Llms 92 hugo bowne-anderson: you give it tokens, it gives tokens out okay. And then along the continuum to building agentic type things, then we integrate other types of tooling and properties into the system. So the 1st one to think about really is, is memory right. Well, it may not be the first, st but it's an important one, and we've discussed that, in brief. And that's why I had Eddie from Fondue to come and talk to you about simple ways to think about memory last week. 93 hugo bowne-anderson: The next one is 94 hugo bowne-anderson: tool calls, okay, and this is something we're going to be talking about later later this week. And, in fact, for anyone who wants to join the the workshop with ravine next next week, doing a lot of 95 hugo bowne-anderson: tool calls, using Olama and Gradio and Gemma. 3 are all all locally, which will be super fun. But you could imagine, for example. 96 hugo bowne-anderson: you want to get the weather today somewhere and the cutoff date of Vlm you're working with was who knows when. And so you can implement a basic tool call to a weather Api. Similarly, you'll notice that chat Gpt. Perhaps these days you'll ask it something. You won't tell it to search the web, but it will recognize that perhaps 97 hugo bowne-anderson: searching the web will help it. Now, who knows whether that's true or not. To be honest. But you'll see it will dynamically search the web without even being told to, and that's more along the agentic side. But these types of tool uses are incredibly important. And then there's retrieval.", "metadata": {"position": 4, "speaker": "Unknown", "workshop_id": "WS5", "token_count": 1050, "original_chunk_id": "bf36823a-27cc-4f5e-a105-6e9d28292d3e", "timestamp": "Chunk 5", "source": "sentence_chunking"}, "id": "WS5_bf36823a-27cc-4f5e-a105-6e9d28292d3e", "relevance": 1.0}]}}{"question": "Can you explain the role of prompt engineering in improving the performance of LLM applications?", "response": "Prompt engineering plays a crucial role in enhancing the performance of LLM applications by refining how inputs are structured and processed. It involves crafting specific prompts that guide the model to generate desired outputs, ensuring clarity and relevance. \n\nTo improve performance, start by analyzing the types of errors your model produces, such as hallucinations or incomplete responses. For instance, if you identify that 30% of errors are hallucinations, focus on adjusting prompts to minimize these occurrences. Use a systematic approach, like creating a pivot table to categorize errors and prioritize high-impact issues.\n\nAdditionally, develop a minimum viable evaluation harness that includes good and bad examples, along with clear evaluation criteria. This helps the model learn what constitutes acceptable responses. Iteratively refine your prompts based on feedback from the model's outputs, ensuring they align with the expected outcomes.\n\nIn summary, effective prompt engineering involves continuous iteration, evaluation, and adjustment to enhance the model's accuracy and reliability, ultimately leading to better performance in real-world applications.", "context_info": {"num_chunks": 10, "context_tokens": 10360, "completion_tokens": 196, "embedding_tokens": 15360, "workshops_used": ["WS4", "WS3", "WS6"], "chunks": [{"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"original_chunk_id": "303c95ae-1d27-4792-ab45-321e8d4778a5", "position": 13, "token_count": 1056, "source": "sentence_chunking", "workshop_id": "WS6", "speaker": "Unknown", "timestamp": "Chunk 14"}, "id": "WS6_303c95ae-1d27-4792-ab45-321e8d4778a5", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"position": 13, "timestamp": "Chunk 14", "token_count": 1056, "source": "sentence_chunking", "original_chunk_id": "e3b959c4-0d2f-4659-bc95-7dd30946df77", "workshop_id": "WS6", "speaker": "Unknown"}, "id": "WS6_e3b959c4-0d2f-4659-bc95-7dd30946df77", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"speaker": "Unknown", "original_chunk_id": "c9a18485-8306-4216-9342-b906b362f26b", "token_count": 1048, "source": "sentence_chunking", "workshop_id": "WS4", "timestamp": "Chunk 15", "position": 14}, "id": "WS4_c9a18485-8306-4216-9342-b906b362f26b", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"speaker": "Unknown", "token_count": 1048, "position": 14, "original_chunk_id": "6c6748ec-6e50-49e9-83a8-6600e01683d8", "workshop_id": "WS4", "source": "sentence_chunking", "timestamp": "Chunk 15"}, "id": "WS4_6c6748ec-6e50-49e9-83a8-6600e01683d8", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"position": 10, "timestamp": "Chunk 11", "source": "sentence_chunking", "token_count": 1062, "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "speaker": "Unknown", "workshop_id": "WS3"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "workshop_id": "WS3", "source": "sentence_chunking", "timestamp": "Chunk 11", "token_count": 1062, "speaker": "Unknown", "position": 10}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm. Call there may be forgetting there may be hallucinations. Almost all the time. If you parallelize different calls. 343 hugo bowne-anderson: So you have one extract the name from a Linkedin profile. You have one extract skills, one extract position, and so on, and then you combine them and aggregate them. You will almost always get a performance increase, it will always cost you more as well. So once again, this is a cost and latency versus 344 hugo bowne-anderson: performance, trade off, and you may be fine with slightly worse, slightly less accurate results. You want to get someone's name right. But if the skills are incomplete or something. Maybe that's that's chill. But once again it comes down to your business problem. We don't know whether you know, 85% recall is good or not right. 345 hugo bowne-anderson: So you can go through the example. There, and I'm happy to chat about it on on discord. 346 hugo bowne-anderson: There's also 347 hugo bowne-anderson: a routing pattern which I'm not gonna get into now. But essentially what the call does is, it routes it to any number of of different Llms and then has an output. And so the example we go through here. Is. It ingests a Linkedin profile and then decides whether to reach out to that person for hiring 348 hugo bowne-anderson: or for business development. Okay, depending on their profile, depending how their profile corresponds with either of these tasks for the organization we're writing for and using different email templates based on our classification. So these are all examples you can use in the projects you're working on as well. And you'll see available routing, hiring collaboration. 349 hugo bowne-anderson: a related one is the orchestrator worker. The the main difference with this one is the orchestrator really controls all all the calls and what's happening underneath and then you get a get a synthesizer. So I'm not going to go into too many details there, but you can. You can look at that, and we can chat about the anthropic blog post as well. The the final example that I do want to. 350 hugo bowne-anderson: Talk about a bit more. 351 hugo bowne-anderson: Huh. 352 hugo bowne-anderson: Yeah, that's great. Sorry about that. Is the evaluator optimizer workflow 353 hugo bowne-anderson: right where we have and this is getting further along the agentic continuum or spectrum, if you will. Once again. I don't even like this idea of a continuum or spectrum. It's like a. 354 hugo bowne-anderson: It's a deeply multi-dimensional space kind of like what is production. Right. This, as as William, you know, showed us earlier this week, and fantastically is, there are so many dimensions to what it is to put something in production, and similarly, to to make something agentic isn't only a 1 dimensional thing. There are so many different ways we can move so. But this is going further towards quote unquote agency, where 355 hugo bowne-anderson: we have a generator which provides some solution to what we're trying to do, such as generating an email. Then we have an evaluator which will reject it and give it feedback or accept it. And then it, it goes back and forth. And so I've given a toy example here 356 hugo bowne-anderson: which is drafting an email from a Linkedin 357 hugo bowne-anderson: profile where the generator creates an initial email based on the profile. Then the evaluator reviews the email for clarity grammatical accuracy and audience alignment. Now, I've said, if improvements are needed, the optimizer revises the email using the evaluators feedback. 358 hugo bowne-anderson: That's an exercise for you all to to do that, because what I've actually done here, I've been slightly cheeky, but I've done it for good reason 359 hugo bowne-anderson: is where let me actually look at this. 360 hugo bowne-anderson: So what we have is step one is the generation of the email. 361 hugo bowne-anderson: Step 2 is evaluate the email and give feedback step 3 is sending it back to the 1st Llm 362 hugo bowne-anderson: to optimize it based on feedback. Okay. And then that's accepted. So this I've predefined. How many, how many steps it will go through now. 363 hugo bowne-anderson: what I want you all to do.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 15", "position": 14, "workshop_id": "WS6", "token_count": 1051, "original_chunk_id": "3bda139e-6400-4b76-90dd-64837d1027f3"}, "id": "WS6_3bda139e-6400-4b76-90dd-64837d1027f3", "relevance": 1.0}, {"text": "342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm. Call there may be forgetting there may be hallucinations. Almost all the time. If you parallelize different calls. 343 hugo bowne-anderson: So you have one extract the name from a Linkedin profile. You have one extract skills, one extract position, and so on, and then you combine them and aggregate them. You will almost always get a performance increase, it will always cost you more as well. So once again, this is a cost and latency versus 344 hugo bowne-anderson: performance, trade off, and you may be fine with slightly worse, slightly less accurate results. You want to get someone's name right. But if the skills are incomplete or something. Maybe that's that's chill. But once again it comes down to your business problem. We don't know whether you know, 85% recall is good or not right. 345 hugo bowne-anderson: So you can go through the example. There, and I'm happy to chat about it on on discord. 346 hugo bowne-anderson: There's also 347 hugo bowne-anderson: a routing pattern which I'm not gonna get into now. But essentially what the call does is, it routes it to any number of of different Llms and then has an output. And so the example we go through here. Is. It ingests a Linkedin profile and then decides whether to reach out to that person for hiring 348 hugo bowne-anderson: or for business development. Okay, depending on their profile, depending how their profile corresponds with either of these tasks for the organization we're writing for and using different email templates based on our classification. So these are all examples you can use in the projects you're working on as well. And you'll see available routing, hiring collaboration. 349 hugo bowne-anderson: a related one is the orchestrator worker. The the main difference with this one is the orchestrator really controls all all the calls and what's happening underneath and then you get a get a synthesizer. So I'm not going to go into too many details there, but you can. You can look at that, and we can chat about the anthropic blog post as well. The the final example that I do want to. 350 hugo bowne-anderson: Talk about a bit more. 351 hugo bowne-anderson: Huh. 352 hugo bowne-anderson: Yeah, that's great. Sorry about that. Is the evaluator optimizer workflow 353 hugo bowne-anderson: right where we have and this is getting further along the agentic continuum or spectrum, if you will. Once again. I don't even like this idea of a continuum or spectrum. It's like a. 354 hugo bowne-anderson: It's a deeply multi-dimensional space kind of like what is production. Right. This, as as William, you know, showed us earlier this week, and fantastically is, there are so many dimensions to what it is to put something in production, and similarly, to to make something agentic isn't only a 1 dimensional thing. There are so many different ways we can move so. But this is going further towards quote unquote agency, where 355 hugo bowne-anderson: we have a generator which provides some solution to what we're trying to do, such as generating an email. Then we have an evaluator which will reject it and give it feedback or accept it. And then it, it goes back and forth. And so I've given a toy example here 356 hugo bowne-anderson: which is drafting an email from a Linkedin 357 hugo bowne-anderson: profile where the generator creates an initial email based on the profile. Then the evaluator reviews the email for clarity grammatical accuracy and audience alignment. Now, I've said, if improvements are needed, the optimizer revises the email using the evaluators feedback. 358 hugo bowne-anderson: That's an exercise for you all to to do that, because what I've actually done here, I've been slightly cheeky, but I've done it for good reason 359 hugo bowne-anderson: is where let me actually look at this. 360 hugo bowne-anderson: So what we have is step one is the generation of the email. 361 hugo bowne-anderson: Step 2 is evaluate the email and give feedback step 3 is sending it back to the 1st Llm 362 hugo bowne-anderson: to optimize it based on feedback. Okay. And then that's accepted. So this I've predefined. How many, how many steps it will go through now. 363 hugo bowne-anderson: what I want you all to do.", "metadata": {"original_chunk_id": "9a25852c-09ab-495f-a803-39ee1be2afe7", "token_count": 1051, "source": "sentence_chunking", "position": 14, "workshop_id": "WS6", "speaker": "Unknown", "timestamp": "Chunk 15"}, "id": "WS6_9a25852c-09ab-495f-a803-39ee1be2afe7", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"speaker": "Unknown", "original_chunk_id": "0023cc64-b3ca-486f-9dd8-dbbc987bf306", "timestamp": "Chunk 19", "source": "sentence_chunking", "workshop_id": "WS6", "position": 18, "token_count": 1062}, "id": "WS6_0023cc64-b3ca-486f-9dd8-dbbc987bf306", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"token_count": 1062, "source": "sentence_chunking", "timestamp": "Chunk 19", "workshop_id": "WS6", "original_chunk_id": "981ee013-55da-46a9-a83e-dd1b1322ac44", "position": 18, "speaker": "Unknown"}, "id": "WS6_981ee013-55da-46a9-a83e-dd1b1322ac44", "relevance": 1.0}]}}{"question": "Can you explain the role of synthetic data in developing LLM applications and how it fits into the Software Development Life Cycle (SDLC)?", "response": "Synthetic data plays a crucial role in developing LLM applications by enabling the creation of realistic user queries and scenarios before launching the product. This approach allows developers to simulate user interactions, which is essential for training and evaluating models effectively.\n\nIn the Software Development Life Cycle (SDLC), synthetic data fits into several key phases:\n\n1. **Planning**: Define user personas and scenarios to guide the development process.\n2. **Development**: Generate synthetic queries that mimic real user behavior, which helps in building a Minimum Viable Product (MVP).\n3. **Testing**: Use the synthetic data to create a test set, allowing for the evaluation of model performance and identification of failure modes.\n4. **Evaluation**: Label the outputs from the model based on the synthetic queries, which helps in refining the evaluation harness and improving the model iteratively.\n5. **Deployment**: After launching, monitor real user interactions and compare them against the synthetic data to ensure the model adapts and evolves effectively.\n\nBy integrating synthetic data throughout the SDLC, developers can enhance model accuracy, reduce costs, and streamline the overall development process.", "context_info": {"num_chunks": 10, "context_tokens": 10376, "completion_tokens": 224, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS1", "WS4", "WS2"], "chunks": [{"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 9", "original_chunk_id": "6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "position": 8, "token_count": 1067, "workshop_id": "WS3"}, "id": "WS3_6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"original_chunk_id": "9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "token_count": 1067, "position": 8, "workshop_id": "WS3", "speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 9"}, "id": "WS3_9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "relevance": 1.0}, {"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"timestamp": "Chunk 8", "source": "sentence_chunking", "position": 7, "original_chunk_id": "97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "token_count": 1061, "workshop_id": "WS3", "speaker": "Unknown"}, "id": "WS3_97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "relevance": 1.0}, {"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"original_chunk_id": "acbab402-9567-43c9-b2a9-caa05bdd8999", "position": 7, "workshop_id": "WS3", "token_count": 1061, "source": "sentence_chunking", "timestamp": "Chunk 8", "speaker": "Unknown"}, "id": "WS3_acbab402-9567-43c9-b2a9-caa05bdd8999", "relevance": 1.0}, {"text": "And then within it, you'll see kind of these kind of boxes that represent spans and so There could be an overarching span that underneath is calling various different things. So you can then see the breakdown of the span as it kind of Who responds, what type of cuisine do you prefer would be, you know, this kind of represent one trace and then the next input would then represented by another trace, right. And so this is where, depending if it's agents or other things, you know, different call paths or code and logic can be taken. And so, uh. So therefore, I mentioned spans This is the other word you'll hear if you hear about tracing, it spans right and so In a LLM app or something like this will be different these bands will correspond to Extraction going for to an API for retrieval the vector database for retrieval coming back you know that the LLM call, how long did it take. And so spans, you can think of it, if you're familiar with logging, it's kind of like a structured log where There are IDs that help it capture the relationship between like, so what was my parents span so what created me But then also some sort of, you know, it does have a payload that you can kind of, that is fairly flexible. There's a lot of UIs here. There's a lot of vendors here. This is an example of UI, but if you think about for LLM side. So Datadog has now built a product here but This does come from distributed tracing, which is between distributed systems. The only difference LLM providers here in the space have done is they've just provided a different UI treatment. To uh two traces. And so very, very important pretty much all of them have this kind of interface. So if you can kind of see this kind of tree type structure of relationship between like spans and things and then When you cook on them, you can kind of see more information, and this is what was logged along with that span. Right. How do traces fit in right it is you know unless you can use them in development, right. So as you're running your app locally this is you can actually leverage this as a way to do data collection but then When it goes to production, you can also do the same thing. So this is the interesting difference between regular software engineering If you are familiar with distributed tracing and Datadog, you would only use that in production But here with Gen AI tools, you can actually use the tracing tools locally in development for yourself as well. And so\u2026 what do those tools look like. How do you integrate them. So I'm just picking Langfuse just because it's a popular open source project. The main thing that you'll kind of see is like there is, you know, you have to install some libraries You then potentially, if you're using their cloud service or you basically have to put in some sort of endpoint And then you're instrumenting so you're instrumenting Using that particular library Importing those modules and then say in this case here, decorating functions that then When this function runs, it is capturing certain aspects about it. And then this doesn't have to be LLM calls. It could be any internal function right and so That's just kind of one example. I think, you know, Arise Phoenix, just to give you another example, right, is like you install something, import things to set things up and then wherever you want to kind of track or do more stuff right you Basically using that SDK, so to speak, to kind of instrument to create traces. Yeah. So nothing too complex, but But I want to say in the last year and a half, there's like a lot of these tracing providers initially when they came out, they were like very they were proprietary right meaning that the data format they sent was Something that they came up. But OpenTelemetry, which is an open standard, we can see a lot of convergence on vendors using this. And so one of the things you want to avoid is vendor lock-in. And so OpenTelemetry allows you to kind of uncheck yourself from that. And so specifically. As I mentioned, spans are a bit like a structured log. Opentelemetry gives a very specific vendor agnostic standard that a lot of vendors buy into. That then allows you to specifies how things should be formatted and how things should be logged. They even have a standard that's kind of, you know, in terms of the standard, they have a standard around what attributes and what they're called to be logged. So this is where if you use OpenTelemetry and a provider that supports ingesting it. They then very easily can then, this is where you can attach metadata like the LLM token usage, the LLM, how many tokens were input, how many tokens were output What were the inputs. What was the type of model. And so they're providing a structured schema that then any downstream consumer, as long as they support it, can ingest it.", "metadata": {"position": 10, "timestamp": "Chunk 11", "token_count": 1029, "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "70265733-6a58-4bff-a8db-edbb94471961", "workshop_id": "WS4"}, "id": "WS4_70265733-6a58-4bff-a8db-edbb94471961", "relevance": 1.0}, {"text": "And then within it, you'll see kind of these kind of boxes that represent spans and so There could be an overarching span that underneath is calling various different things. So you can then see the breakdown of the span as it kind of Who responds, what type of cuisine do you prefer would be, you know, this kind of represent one trace and then the next input would then represented by another trace, right. And so this is where, depending if it's agents or other things, you know, different call paths or code and logic can be taken. And so, uh. So therefore, I mentioned spans This is the other word you'll hear if you hear about tracing, it spans right and so In a LLM app or something like this will be different these bands will correspond to Extraction going for to an API for retrieval the vector database for retrieval coming back you know that the LLM call, how long did it take. And so spans, you can think of it, if you're familiar with logging, it's kind of like a structured log where There are IDs that help it capture the relationship between like, so what was my parents span so what created me But then also some sort of, you know, it does have a payload that you can kind of, that is fairly flexible. There's a lot of UIs here. There's a lot of vendors here. This is an example of UI, but if you think about for LLM side. So Datadog has now built a product here but This does come from distributed tracing, which is between distributed systems. The only difference LLM providers here in the space have done is they've just provided a different UI treatment. To uh two traces. And so very, very important pretty much all of them have this kind of interface. So if you can kind of see this kind of tree type structure of relationship between like spans and things and then When you cook on them, you can kind of see more information, and this is what was logged along with that span. Right. How do traces fit in right it is you know unless you can use them in development, right. So as you're running your app locally this is you can actually leverage this as a way to do data collection but then When it goes to production, you can also do the same thing. So this is the interesting difference between regular software engineering If you are familiar with distributed tracing and Datadog, you would only use that in production But here with Gen AI tools, you can actually use the tracing tools locally in development for yourself as well. And so\u2026 what do those tools look like. How do you integrate them. So I'm just picking Langfuse just because it's a popular open source project. The main thing that you'll kind of see is like there is, you know, you have to install some libraries You then potentially, if you're using their cloud service or you basically have to put in some sort of endpoint And then you're instrumenting so you're instrumenting Using that particular library Importing those modules and then say in this case here, decorating functions that then When this function runs, it is capturing certain aspects about it. And then this doesn't have to be LLM calls. It could be any internal function right and so That's just kind of one example. I think, you know, Arise Phoenix, just to give you another example, right, is like you install something, import things to set things up and then wherever you want to kind of track or do more stuff right you Basically using that SDK, so to speak, to kind of instrument to create traces. Yeah. So nothing too complex, but But I want to say in the last year and a half, there's like a lot of these tracing providers initially when they came out, they were like very they were proprietary right meaning that the data format they sent was Something that they came up. But OpenTelemetry, which is an open standard, we can see a lot of convergence on vendors using this. And so one of the things you want to avoid is vendor lock-in. And so OpenTelemetry allows you to kind of uncheck yourself from that. And so specifically. As I mentioned, spans are a bit like a structured log. Opentelemetry gives a very specific vendor agnostic standard that a lot of vendors buy into. That then allows you to specifies how things should be formatted and how things should be logged. They even have a standard that's kind of, you know, in terms of the standard, they have a standard around what attributes and what they're called to be logged. So this is where if you use OpenTelemetry and a provider that supports ingesting it. They then very easily can then, this is where you can attach metadata like the LLM token usage, the LLM, how many tokens were input, how many tokens were output What were the inputs. What was the type of model. And so they're providing a structured schema that then any downstream consumer, as long as they support it, can ingest it.", "metadata": {"position": 10, "token_count": 1029, "speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS4", "original_chunk_id": "a6d99531-2ae0-4f8d-a810-cb420f7cc61e", "timestamp": "Chunk 11"}, "id": "WS4_a6d99531-2ae0-4f8d-a810-cb420f7cc61e", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"workshop_id": "WS1", "timestamp": "Chunk 11", "token_count": 1050, "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "position": 10}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "speaker": "Unknown", "workshop_id": "WS1", "token_count": 1050, "source": "sentence_chunking", "position": 10, "timestamp": "Chunk 11"}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}, {"text": "And then since then, I've actually come back to it a couple of times and continued to kind of build on it. If you've seen inference time compute scaling come up either in this class or elsewhere. It's the idea that you like you take a model, you have it generate a bunch of answers, and then you take only the ones that are right. So I applied that in this one as well. So I generate a bunch of QR codes and then I take only the ones that scan. So only the ones that I'm able to actually scan By running like a little QR code scanner on also running on Modal. Little Python library that scans QR codes and then take those and then find the one that looks best according to like a little aesthetic judge and show that. So that's uh this thing here. This QR code, by the way, should\u2026 get you to the form to fill out to get your $1,000 in modal credits. So\u2026 You can head to that bit.ly slash modal dash credits to get your thousand bucks in credits. So yeah, so this guy here, it's like it's a diffusion model. I think I actually also host See, the front end actually is, I think, hosted on Vercel probably, but the back end, all that stuff runs on modal. Generation QR code evaluation and um And like a little job queue. So that's one project. Another one, another fun little thing built on Moto you can check out. This is Twitter-95.com. So this is a little fake version of Twitter, like as if It was around in 1995, or I guess 1996, because it's been up for a year. So we've moved forward a year. So it's got a bunch of celebrities and people from the 90s like Francis Collins here. What did he do. He did like\u2026 Oh yeah, Human Genome Project. So we got some famous people from the 90s, Tim Berners-Lee. And others. And these are all language models pretending to be celebrities from the 90s to make like a little fake social media experience from the past. So you can find the code for this online as well. And it's like, you know, you got\u2026 extracting information from the past and then posting it putting it into the language models context and then having the models run, all that's done off of Off of modal. So that's kind of like the, you know, the sorts of stuff that I've been that uh Like Moodle makes possible these like sort of more, you know, complex applications, more like custom uses of a diffusion model or a language model that are a little off the beaten path. So yeah, I guess maybe\u2026 Um\u2026 Oh yeah, since this is building LLM apps in particular. I also wanted to show real quick, just like we put up one of the examples we put up Most recently is this latency sensitive latency sensitive serving. So like to serve yourself like a little chat bot This just got added recently and is probably going to be useful for things you want to build in this class if you're thinking about building. Agent or a little chat bot. And\u2026 So let me just run the thing. Show you what you get. So this\u2026 This is all running on modal. It's a single file Python to deploy this thing, build a tensorRTLM engine from scratch And that's\u2026 Uh\u2026 you get this nice, very fast and snappy Interaction. Oh, you go out and answer this renowned professor of data science and statistics at uc berkeley. That's approximately correct. It's a small model, so it doesn't get\u2026 It doesn't get everything right. I also don't mind that lie or that hallucination. I'm sorry. I'm sorry to call you. Yeah, I prefer that to other types of hallucinations it could give. Yeah. Yeah, really a professor at Berkeley. That sounds wrong. Yeah. I'm a large language model and I sometimes make errors. Yeah. Um anyway so that they're like the main thing there is like the model starts like starts responding once it's live in about you know 250 milliseconds or so if you have like short chat prompts, which is enough to feel nice and interactive. All the code to figure out how to do that and serve that on modal is on is in the examples there. Yeah, so the modal is this like very generic infrastructure platform designed to make building those things possible. So I want to talk about like. What this infrastructure platform is and why it matters, like more broadly even than just thinking about like building LLM apps I think the main thing once you go, once you get past the point where you're initially like. You know, trying things out, working in a notebook to figure out like what the model does\u2026 in response to different inputs, do your evaluations.", "metadata": {"workshop_id": "WS2", "timestamp": "Chunk 6", "source": "sentence_chunking", "position": 5, "speaker": "Unknown", "token_count": 999, "original_chunk_id": "cda4270d-1415-466e-b58c-297627939ec4"}, "id": "WS2_cda4270d-1415-466e-b58c-297627939ec4", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"position": 10, "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "token_count": 1062, "speaker": "Unknown", "workshop_id": "WS3", "timestamp": "Chunk 11", "source": "sentence_chunking"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}]}}{"question": "Can you explain what prompt engineering involves and how it impacts the performance of LLMs in real-world applications?", "response": "Prompt engineering is the process of designing and refining the inputs given to a language model (LLM) to elicit the best possible responses. It involves crafting prompts that are clear, specific, and contextually relevant, ensuring that the model understands the task at hand. \n\nIn real-world applications, effective prompt engineering can significantly enhance the performance of LLMs by reducing errors such as hallucinations (incorrect or fabricated information) and improving the relevance and accuracy of the outputs. For instance, by providing structured examples and clear evaluation criteria, you can guide the model to produce responses that are not only factually correct but also aligned with user expectations.\n\nTo implement prompt engineering effectively, consider the following steps:\n1. **Iterate on Prompts**: Test different variations of prompts to see which yields the best results.\n2. **Use Contextual Information**: Incorporate relevant background information to help the model understand the task better.\n3. **Evaluate Outputs**: Regularly assess the quality of the model's responses and adjust prompts accordingly.\n\nBy focusing on these aspects, you can create a robust system that leverages LLMs effectively for various applications.", "context_info": {"num_chunks": 10, "context_tokens": 10365, "completion_tokens": 230, "embedding_tokens": 15360, "workshops_used": ["WS4", "WS3", "WS6"], "chunks": [{"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"token_count": 1056, "original_chunk_id": "303c95ae-1d27-4792-ab45-321e8d4778a5", "timestamp": "Chunk 14", "position": 13, "speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS6"}, "id": "WS6_303c95ae-1d27-4792-ab45-321e8d4778a5", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"original_chunk_id": "e3b959c4-0d2f-4659-bc95-7dd30946df77", "position": 13, "timestamp": "Chunk 14", "token_count": 1056, "workshop_id": "WS6", "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS6_e3b959c4-0d2f-4659-bc95-7dd30946df77", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"speaker": "Unknown", "workshop_id": "WS4", "original_chunk_id": "c9a18485-8306-4216-9342-b906b362f26b", "token_count": 1048, "timestamp": "Chunk 15", "source": "sentence_chunking", "position": 14}, "id": "WS4_c9a18485-8306-4216-9342-b906b362f26b", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"workshop_id": "WS4", "timestamp": "Chunk 15", "token_count": 1048, "speaker": "Unknown", "position": 14, "source": "sentence_chunking", "original_chunk_id": "6c6748ec-6e50-49e9-83a8-6600e01683d8"}, "id": "WS4_6c6748ec-6e50-49e9-83a8-6600e01683d8", "relevance": 1.0}, {"text": "What the language model does is it calls a relational database for the existing string for that user And so the existing string for that user, it's just like all the kind of accumulated information up to that point in time. And so this language model call And yeah, actually, I kind of talk about it more here, you know, just kind of like getting concrete about like, hey, what does the actual prompt look like um right so you um in kind of constructing this prompt and doing the update We're reading. Reading the existing memory And so here's the existing memory piece. Here it gets read from the relational database. Here's the new information that was just passed by the agent. I think it's always good practice. This isn't the subject of the talk, but just in my experience with prompting, meta context is always very important like giving given the language model some understanding of kind of like what are the broader objectives you know what company do you work for and kind of Are you trying to maximize revenue or maximize happiness or what have you, those type of things. And then the instructions, which here involve just incorporating the new information and kind of integrating it into the existing string And then providing that update So yeah, so basically how this goes is okay so we're generating this update prompt, basically the prompt that gets passed to the language model along with the new information. And then this language takes the new information and it just updates the string. So it's, you know, basically couldn't get more simple than that. I'll pause there in case it actually is the case that it could get more simple and it's been hard to follow what I've said. So I'll just pause for a couple It all makes perfect sense to me. We have a question from Luca that I'm not sure I understand. Yeah. And the question is, do we get the final response only from the LLM as user memory to be added to the current query. I think I don't understand it because there are too many subclauses for me to grok it. Yeah, so I think maybe like just my interpretation of it, which is just like, I think I didn't do a good job of explaining is that um right so um So this prompt up here is basically what gets sent To the language model and what elicits a response from the language model. And so one of those responses is, you know, so eventually it returns like a chat response. It may call a tool. So if, yeah, let's say kind of like a new user message comes in It generates this prompt. So the user input comes from what the user enters. And then the user memory is just pulled. It's just a simple database read. So it's just a simple, very low latency API call to a relational database that just contains a string. That might just be a lookup on the basis of a user ID And so you just inject this into the prompt. And so basically every language model call is going to contain this user memory. And the agent is going to have the option of updating this string. And if it does, that's just going to get pushed to the relational database Once that's done. That'll just be available in the relational database and be provided you know be provided be kind of funneled into the prompt for the next response. Did that clarify the question or clarify the question And yeah, there's no vector DB yet, but I will get to that. What. In the next section sorry go ahead. Sure. Oh. Okay, but I need some question here because as a context one is for each LLM, it is uh limited okay so if our user memory is getting bigger Okay, there won't be some error here. Mm-hmm. And can't handle this and can they give you the response. Mm-hmm. Oh, so you're saying just like eventually you're going to hit token limits yeah so this so that's a great point. And that's exactly, you know, I mentioned I'm going to talk about kind of like the trade-offs here. But you've kind of anticipated one of the major trade-offs associated with this approach. What I will say is that in my experience. You can actually go pretty far with this approach. And it's dead simple and it's super low latency. And you can just, this is an approach that doesn't require any, I guess it does require the tool call to to decide what information gets saved, but in terms of just injecting this context into every language model response. It's an extremely efficient strategy. And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs.", "metadata": {"original_chunk_id": "6927769f-eb5e-462b-b957-b3fcd530b67a", "source": "sentence_chunking", "position": 21, "workshop_id": "WS4", "timestamp": "Chunk 22", "token_count": 1037, "speaker": "Unknown"}, "id": "WS4_6927769f-eb5e-462b-b957-b3fcd530b67a", "relevance": 1.0}, {"text": "What the language model does is it calls a relational database for the existing string for that user And so the existing string for that user, it's just like all the kind of accumulated information up to that point in time. And so this language model call And yeah, actually, I kind of talk about it more here, you know, just kind of like getting concrete about like, hey, what does the actual prompt look like um right so you um in kind of constructing this prompt and doing the update We're reading. Reading the existing memory And so here's the existing memory piece. Here it gets read from the relational database. Here's the new information that was just passed by the agent. I think it's always good practice. This isn't the subject of the talk, but just in my experience with prompting, meta context is always very important like giving given the language model some understanding of kind of like what are the broader objectives you know what company do you work for and kind of Are you trying to maximize revenue or maximize happiness or what have you, those type of things. And then the instructions, which here involve just incorporating the new information and kind of integrating it into the existing string And then providing that update So yeah, so basically how this goes is okay so we're generating this update prompt, basically the prompt that gets passed to the language model along with the new information. And then this language takes the new information and it just updates the string. So it's, you know, basically couldn't get more simple than that. I'll pause there in case it actually is the case that it could get more simple and it's been hard to follow what I've said. So I'll just pause for a couple It all makes perfect sense to me. We have a question from Luca that I'm not sure I understand. Yeah. And the question is, do we get the final response only from the LLM as user memory to be added to the current query. I think I don't understand it because there are too many subclauses for me to grok it. Yeah, so I think maybe like just my interpretation of it, which is just like, I think I didn't do a good job of explaining is that um right so um So this prompt up here is basically what gets sent To the language model and what elicits a response from the language model. And so one of those responses is, you know, so eventually it returns like a chat response. It may call a tool. So if, yeah, let's say kind of like a new user message comes in It generates this prompt. So the user input comes from what the user enters. And then the user memory is just pulled. It's just a simple database read. So it's just a simple, very low latency API call to a relational database that just contains a string. That might just be a lookup on the basis of a user ID And so you just inject this into the prompt. And so basically every language model call is going to contain this user memory. And the agent is going to have the option of updating this string. And if it does, that's just going to get pushed to the relational database Once that's done. That'll just be available in the relational database and be provided you know be provided be kind of funneled into the prompt for the next response. Did that clarify the question or clarify the question And yeah, there's no vector DB yet, but I will get to that. What. In the next section sorry go ahead. Sure. Oh. Okay, but I need some question here because as a context one is for each LLM, it is uh limited okay so if our user memory is getting bigger Okay, there won't be some error here. Mm-hmm. And can't handle this and can they give you the response. Mm-hmm. Oh, so you're saying just like eventually you're going to hit token limits yeah so this so that's a great point. And that's exactly, you know, I mentioned I'm going to talk about kind of like the trade-offs here. But you've kind of anticipated one of the major trade-offs associated with this approach. What I will say is that in my experience. You can actually go pretty far with this approach. And it's dead simple and it's super low latency. And you can just, this is an approach that doesn't require any, I guess it does require the tool call to to decide what information gets saved, but in terms of just injecting this context into every language model response. It's an extremely efficient strategy. And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs.", "metadata": {"original_chunk_id": "15e15f0c-6283-4643-b794-4a249ce929b9", "workshop_id": "WS4", "position": 21, "source": "sentence_chunking", "token_count": 1037, "speaker": "Unknown", "timestamp": "Chunk 22"}, "id": "WS4_15e15f0c-6283-4643-b794-4a249ce929b9", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"original_chunk_id": "981ee013-55da-46a9-a83e-dd1b1322ac44", "timestamp": "Chunk 19", "speaker": "Unknown", "token_count": 1062, "source": "sentence_chunking", "position": 18, "workshop_id": "WS6"}, "id": "WS6_981ee013-55da-46a9-a83e-dd1b1322ac44", "relevance": 1.0}, {"text": "431 Phillip Carter: But at the end of the day it's like there's inputs. And then we build up a little thing. We do some rag over a larger set of data. 432 Phillip Carter: we pass it to a function which in this case is an open AI Llm call, and then we get an output, and then we check to see. Do we like the output or not. And then, once you have enough of that. So we did a lot of this ad hoc at first, st and actually released to users just for everyone, because it was actually good enough for most use cases from what we could tell, and we were a little bit off the mark, but we were able to fix up a few things as we went, but eventually we got to a point where it's like, okay, there's sort of a steady state of behavior where it has a lot of inputs and has a lot of outputs. 433 Phillip Carter: And I have a lot of users using this thing. So I'm getting a wide variety of inputs. And I'm getting a wide variety of outputs as well. And what this is doing for me is allowing me to build up basically, this Csv of 434 Phillip Carter: what's the input what's the full prompt when we select in this case it's called column selection. It's this retrieval process for the given input, what columns did we retrieve from a larger schema a handful of other fields. What's the output. If there was an error, what was the error like. Did it fail to parse Json. Did it. Did it fail. Query, specification parsing rules. Did it fail. There's a handful of other failure cases, basically, and record all of that in like one big row of data. 435 Phillip Carter: And then we can say, Okay, great. I'm going to back out for the next 60 days, 60 days of usage. And I'm just going to download all of that. And that gives me a big Csv, now of real inputs and outputs. 436 Phillip Carter: And now that I have the Csv, I can load it up into Google sheets. 437 Phillip Carter: And that's where the magic really happens. Because you have enough of that data, you can start pulling little samples of it. You can say, Okay, well, I saw that the user gave this input. 438 Phillip Carter: do I think the output was good. Or do I think it was bad. Or was it somewhere in between. 439 Phillip Carter: Okay, I'm going to record that. 440 Phillip Carter: And then I do that again and again and again, and eventually I arrive at something 441 Phillip Carter: that I can then actually feed into a language model. 442 Phillip Carter: and that language model. I can use that to sort of iterate with this language model, to create this Llm. As judge that is able to sort of read. What I've initially said, I think is maybe good or bad. I can also add, in the prompt sort of my own characteristics of what makes for a good output. Given a kind of input 443 Phillip Carter: and then, now, I have a judge function. 444 Phillip Carter: Then I can feed in that full data set, and you want to anonymize certain things because some users might put in some stuff that you don't necessarily want to feed to an arbitrary language model right away. But I can feed that to a language model. And I can say, Hey, these are kinds of inputs and outputs. And all of that. I want you to produce inputs 445 Phillip Carter: that are similar to the kinds of inputs that are real world here. But like, tweak it a bunch and try to add some variation that that may not exist in the initial data set that you have. 446 Phillip Carter: and it'll spit out as many inputs as you want. 447 Phillip Carter: And then, because I have that task function right where I pass in the input. And in this case I ask it to basically generate the full text of a prompt for for each like fake. Request that I'm going to send. I can package that up through a script, send it off the language model, get a real output of, like the way that our feature actually behaves for that kind of input. 448 Phillip Carter: And now I have a process by which I can start feeding synthetic data into the system and getting useful outputs for that. In that process I can also create a judge that will then act as a given a set of criteria and the stuff that I talked about earlier. It can say, Okay, well, we are going to judge the output 449 Phillip Carter: based off of this stuff that I sort of initially set up. And now I can have a flywheel where I'm generating inputs that are representative of the kinds of real world inputs that people give us, and I have the function, and it responds. And then I have a judge that says I'm gonna decide if this output was good or not 450 Phillip Carter: going to repeat that process.", "metadata": {"speaker": "Unknown", "token_count": 1062, "position": 18, "original_chunk_id": "0023cc64-b3ca-486f-9dd8-dbbc987bf306", "timestamp": "Chunk 19", "workshop_id": "WS6", "source": "sentence_chunking"}, "id": "WS6_0023cc64-b3ca-486f-9dd8-dbbc987bf306", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"token_count": 1062, "timestamp": "Chunk 11", "speaker": "Unknown", "position": 10, "workshop_id": "WS3", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "source": "sentence_chunking"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"timestamp": "Chunk 11", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "position": 10, "source": "sentence_chunking", "workshop_id": "WS3", "speaker": "Unknown", "token_count": 1062}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}]}}{"question": "Can you provide examples of effective prompt engineering techniques that can help improve the quality of responses from my LLM implementations?", "response": "To enhance the quality of responses from your LLM implementations, consider these effective prompt engineering techniques:\n\n1. **Clear Instructions**: Be explicit about what you want. For example, instead of asking, \"Tell me about the Eiffel Tower,\" specify, \"Provide a brief history of the Eiffel Tower, including its construction date and purpose.\"\n\n2. **Contextual Information**: Include relevant context to guide the model. For instance, \"In the context of a job application, draft a cover letter for a software engineer position.\"\n\n3. **Structured Prompts**: Use structured formats like JSON to define inputs and expected outputs. This helps the model understand the required structure, e.g., \"Extract the following details from this LinkedIn profile: name, current position, and skills.\"\n\n4. **Iterative Refinement**: Test and refine your prompts based on the outputs. If the response is vague, adjust the prompt to ask for more specific details.\n\n5. **Examples of Good and Bad Responses**: Provide examples within the prompt to illustrate what constitutes a good response versus a poor one. This can help the model align its output with your expectations.\n\n6. **Feedback Loops**: Implement a system for evaluating responses and providing feedback to the model, which can help improve future outputs.\n\nBy applying these techniques, you can significantly enhance the relevance and accuracy of the responses generated by your LLM.", "context_info": {"num_chunks": 10, "context_tokens": 10368, "completion_tokens": 281, "embedding_tokens": 15360, "workshops_used": ["WS6", "WS3", "WS4"], "chunks": [{"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"timestamp": "Chunk 11", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "token_count": 1062, "source": "sentence_chunking", "workshop_id": "WS3", "speaker": "Unknown", "position": 10}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "token_count": 1062, "timestamp": "Chunk 11", "workshop_id": "WS3", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "position": 10}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"token_count": 1068, "position": 14, "timestamp": "Chunk 15", "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS3", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 15", "position": 14, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "workshop_id": "WS3", "token_count": 1068}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "This response. And it actually says the response does not directly answer the question with specific details from workshop one. Once again, I don't think this has access to workshop one. So I want to then The problem is with the judge there, not the response. And just to be clear, it isn't judges all the way down. We use human evaluation to align the judge. With ourselves. You can see that the Gemini model It passed. And it was quite good. So this is somewhere maybe we want to improve on the judge. Now here we can see what is the difference between LM fine-tuning prompt engineering. Pretty nice response here. Directly answers the question, great. The Gemini response, I messed up with my rate limiting. I didn't do it. Quite right. And so I got an error and I actually love that I did this. This is a teachable moment um So I'm glad that I messed this up. You can see the judge said this response does not address the question at all. Instead, it provides an error message related to API rate limits, which is irrelevant to the question. It's totally right. Perhaps I'd like my judge there to say, hey, yo, it looks like you erred out. You need to fix this. Um like it doesn't seem to be interpreting this as an issue with the system really like an an era, but it says enough there. So this is out of scope for now. But it's something that's incredibly important. I'll link with resources to it and perhaps we can have a session Later on, but I don't want to put the cart before the horse. Aligning LLM judge with your own evaluations is important. And you can even see as a figure like plotting over time, you know, alignment between LLN Judge and human. After a few iterations of this type of thing You get relatively close a lot of the time. And I'll actually link to one of Eugene Yad. If somebody wants to find Eugene Yan's blog post a line eval. It's A-L-I-G-n- E-V-A-L and put in the workshop three chat, he actually built um so he's this dude at Amazon. And he, oh, sorry, I'm just looking at workshop three channel now. So I haven't been looking at it. So people who are tagging me If there are serious issues, someone can turn their mic on and chat. But Eugene has built a small thing which gamifies aligning evals, essentially, which is pretty cool. So then to wrap up this section, I just want to talk about observability and instrumentation in Devon prod. So we've talked about the core loop of evaluation of cost, accuracy, latency user business metrics and using automated evaluation to do these scalable evaluations. And how does this work. We want to log it like a system, right. We want to have logging on the user side. So system prompts, user prompts, tool calls, function calls. There was a great question. That hopefully we get to talk about with Hamill, that Vishrut asked in workshop three in my Hamill thread with his post about What if you can't log user prompts. I actually wonder\u2026 William, because you work in a highly regulated space if you've had any products you've had to build where you can't log some user prompts and that type of stuff. Yeah, I mean, I have a couple of thoughts on this. I think like I have had use cases where we absolutely weren't Actually, no, that's not true. There was a use case for internal. I built an internal like chat system that I owned and people didn't want that data getting exported to the data warehouse because you can imagine like managers using it to write reviews and then like That ends up in the data warehouse and all of a sudden someone's going through data and sees like their draft review. I've had to handle company sensitive stuff. What I was going to say to the question about healthcare is that In my experience, the company has already had to be handling sensitive data. Like at the company I worked at. Most recently, we already had the user's healthcare claims So like we already needed infrastructure in place to be able to handle things according to the requirements of hipa and so it wasn't really an issue to then be able to Also like log chats because we already had like the infrastructure in place It's slightly more challenging because it's unstructured data. Like if it's a claim you know what fields to mask in order to make it not like THI because it's structured data structured data Whereas if it's text. How exactly to redact the text to meet the requirements of Privacy, that is, I think, a very complex topic and we definitely ran into challenges there Fascinating. Well, hopefully we get to discuss that a bit more with Hamill. As well. But in the limit where you're able to log things, which is a lot of the case. You log things on the user side, including tool calls, function calls, if any. And that's with the LM as well.", "metadata": {"workshop_id": "WS3", "position": 11, "token_count": 1043, "timestamp": "Chunk 12", "original_chunk_id": "f0863d5a-dbf0-4447-9ca4-1c1bda125b3e", "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS3_f0863d5a-dbf0-4447-9ca4-1c1bda125b3e", "relevance": 1.0}, {"text": "This response. And it actually says the response does not directly answer the question with specific details from workshop one. Once again, I don't think this has access to workshop one. So I want to then The problem is with the judge there, not the response. And just to be clear, it isn't judges all the way down. We use human evaluation to align the judge. With ourselves. You can see that the Gemini model It passed. And it was quite good. So this is somewhere maybe we want to improve on the judge. Now here we can see what is the difference between LM fine-tuning prompt engineering. Pretty nice response here. Directly answers the question, great. The Gemini response, I messed up with my rate limiting. I didn't do it. Quite right. And so I got an error and I actually love that I did this. This is a teachable moment um So I'm glad that I messed this up. You can see the judge said this response does not address the question at all. Instead, it provides an error message related to API rate limits, which is irrelevant to the question. It's totally right. Perhaps I'd like my judge there to say, hey, yo, it looks like you erred out. You need to fix this. Um like it doesn't seem to be interpreting this as an issue with the system really like an an era, but it says enough there. So this is out of scope for now. But it's something that's incredibly important. I'll link with resources to it and perhaps we can have a session Later on, but I don't want to put the cart before the horse. Aligning LLM judge with your own evaluations is important. And you can even see as a figure like plotting over time, you know, alignment between LLN Judge and human. After a few iterations of this type of thing You get relatively close a lot of the time. And I'll actually link to one of Eugene Yad. If somebody wants to find Eugene Yan's blog post a line eval. It's A-L-I-G-n- E-V-A-L and put in the workshop three chat, he actually built um so he's this dude at Amazon. And he, oh, sorry, I'm just looking at workshop three channel now. So I haven't been looking at it. So people who are tagging me If there are serious issues, someone can turn their mic on and chat. But Eugene has built a small thing which gamifies aligning evals, essentially, which is pretty cool. So then to wrap up this section, I just want to talk about observability and instrumentation in Devon prod. So we've talked about the core loop of evaluation of cost, accuracy, latency user business metrics and using automated evaluation to do these scalable evaluations. And how does this work. We want to log it like a system, right. We want to have logging on the user side. So system prompts, user prompts, tool calls, function calls. There was a great question. That hopefully we get to talk about with Hamill, that Vishrut asked in workshop three in my Hamill thread with his post about What if you can't log user prompts. I actually wonder\u2026 William, because you work in a highly regulated space if you've had any products you've had to build where you can't log some user prompts and that type of stuff. Yeah, I mean, I have a couple of thoughts on this. I think like I have had use cases where we absolutely weren't Actually, no, that's not true. There was a use case for internal. I built an internal like chat system that I owned and people didn't want that data getting exported to the data warehouse because you can imagine like managers using it to write reviews and then like That ends up in the data warehouse and all of a sudden someone's going through data and sees like their draft review. I've had to handle company sensitive stuff. What I was going to say to the question about healthcare is that In my experience, the company has already had to be handling sensitive data. Like at the company I worked at. Most recently, we already had the user's healthcare claims So like we already needed infrastructure in place to be able to handle things according to the requirements of hipa and so it wasn't really an issue to then be able to Also like log chats because we already had like the infrastructure in place It's slightly more challenging because it's unstructured data. Like if it's a claim you know what fields to mask in order to make it not like THI because it's structured data structured data Whereas if it's text. How exactly to redact the text to meet the requirements of Privacy, that is, I think, a very complex topic and we definitely ran into challenges there Fascinating. Well, hopefully we get to discuss that a bit more with Hamill. As well. But in the limit where you're able to log things, which is a lot of the case. You log things on the user side, including tool calls, function calls, if any. And that's with the LM as well.", "metadata": {"speaker": "Unknown", "token_count": 1043, "position": 11, "source": "sentence_chunking", "timestamp": "Chunk 12", "original_chunk_id": "32750c97-ce60-4ae4-a753-dd7c768ef6d9", "workshop_id": "WS3"}, "id": "WS3_32750c97-ce60-4ae4-a753-dd7c768ef6d9", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"workshop_id": "WS4", "speaker": "Unknown", "timestamp": "Chunk 15", "position": 14, "original_chunk_id": "c9a18485-8306-4216-9342-b906b362f26b", "token_count": 1048, "source": "sentence_chunking"}, "id": "WS4_c9a18485-8306-4216-9342-b906b362f26b", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"position": 14, "source": "sentence_chunking", "timestamp": "Chunk 15", "speaker": "Unknown", "original_chunk_id": "6c6748ec-6e50-49e9-83a8-6600e01683d8", "token_count": 1048, "workshop_id": "WS4"}, "id": "WS4_6c6748ec-6e50-49e9-83a8-6600e01683d8", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"token_count": 1056, "workshop_id": "WS6", "speaker": "Unknown", "source": "sentence_chunking", "position": 13, "timestamp": "Chunk 14", "original_chunk_id": "303c95ae-1d27-4792-ab45-321e8d4778a5"}, "id": "WS6_303c95ae-1d27-4792-ab45-321e8d4778a5", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"position": 13, "token_count": 1056, "speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS6", "timestamp": "Chunk 14", "original_chunk_id": "e3b959c4-0d2f-4659-bc95-7dd30946df77"}, "id": "WS6_e3b959c4-0d2f-4659-bc95-7dd30946df77", "relevance": 1.0}]}}{"question": "Can you provide insights into how to generate synthetic data for training LLMs, especially in scenarios where labeled data is scarce?", "response": "Generating synthetic data for training LLMs is a strategic approach, especially when labeled data is limited. Start by defining your target user personas and their specific needs. This helps in creating realistic scenarios and queries that reflect actual user interactions.\n\n1. **Build an MVP**: Develop a Minimum Viable Product (MVP) that can generate synthetic queries. This could involve using existing models to simulate user interactions.\n\n2. **Synthetic Query Generation**: Create synthetic user queries based on the defined personas. This can be done by leveraging language models to generate diverse and contextually relevant questions.\n\n3. **Labeling Responses**: Manually label the responses generated by your MVP. This can be done in a structured format, such as spreadsheets or JSON, to facilitate easy analysis.\n\n4. **Evaluation Harness**: Use the labeled data to build an evaluation harness. This includes creating a test set and employing techniques like fuzzy matching or regex to assess the quality of responses.\n\n5. **Iterate and Improve**: Continuously monitor the performance of your model using the evaluation harness. Adjust your synthetic data generation process based on feedback and performance metrics.\n\nBy following these steps, you can effectively create a data flywheel that enhances your LLM's training process, even in the absence of extensive labeled datasets.", "context_info": {"num_chunks": 10, "context_tokens": 10291, "completion_tokens": 258, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS5", "WS4", "WS2"], "chunks": [{"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS3", "position": 7, "original_chunk_id": "97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "token_count": 1061, "timestamp": "Chunk 8"}, "id": "WS3_97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "relevance": 1.0}, {"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "acbab402-9567-43c9-b2a9-caa05bdd8999", "workshop_id": "WS3", "position": 7, "timestamp": "Chunk 8", "token_count": 1061}, "id": "WS3_acbab402-9567-43c9-b2a9-caa05bdd8999", "relevance": 1.0}, {"text": "And then within it, you'll see kind of these kind of boxes that represent spans and so There could be an overarching span that underneath is calling various different things. So you can then see the breakdown of the span as it kind of Who responds, what type of cuisine do you prefer would be, you know, this kind of represent one trace and then the next input would then represented by another trace, right. And so this is where, depending if it's agents or other things, you know, different call paths or code and logic can be taken. And so, uh. So therefore, I mentioned spans This is the other word you'll hear if you hear about tracing, it spans right and so In a LLM app or something like this will be different these bands will correspond to Extraction going for to an API for retrieval the vector database for retrieval coming back you know that the LLM call, how long did it take. And so spans, you can think of it, if you're familiar with logging, it's kind of like a structured log where There are IDs that help it capture the relationship between like, so what was my parents span so what created me But then also some sort of, you know, it does have a payload that you can kind of, that is fairly flexible. There's a lot of UIs here. There's a lot of vendors here. This is an example of UI, but if you think about for LLM side. So Datadog has now built a product here but This does come from distributed tracing, which is between distributed systems. The only difference LLM providers here in the space have done is they've just provided a different UI treatment. To uh two traces. And so very, very important pretty much all of them have this kind of interface. So if you can kind of see this kind of tree type structure of relationship between like spans and things and then When you cook on them, you can kind of see more information, and this is what was logged along with that span. Right. How do traces fit in right it is you know unless you can use them in development, right. So as you're running your app locally this is you can actually leverage this as a way to do data collection but then When it goes to production, you can also do the same thing. So this is the interesting difference between regular software engineering If you are familiar with distributed tracing and Datadog, you would only use that in production But here with Gen AI tools, you can actually use the tracing tools locally in development for yourself as well. And so\u2026 what do those tools look like. How do you integrate them. So I'm just picking Langfuse just because it's a popular open source project. The main thing that you'll kind of see is like there is, you know, you have to install some libraries You then potentially, if you're using their cloud service or you basically have to put in some sort of endpoint And then you're instrumenting so you're instrumenting Using that particular library Importing those modules and then say in this case here, decorating functions that then When this function runs, it is capturing certain aspects about it. And then this doesn't have to be LLM calls. It could be any internal function right and so That's just kind of one example. I think, you know, Arise Phoenix, just to give you another example, right, is like you install something, import things to set things up and then wherever you want to kind of track or do more stuff right you Basically using that SDK, so to speak, to kind of instrument to create traces. Yeah. So nothing too complex, but But I want to say in the last year and a half, there's like a lot of these tracing providers initially when they came out, they were like very they were proprietary right meaning that the data format they sent was Something that they came up. But OpenTelemetry, which is an open standard, we can see a lot of convergence on vendors using this. And so one of the things you want to avoid is vendor lock-in. And so OpenTelemetry allows you to kind of uncheck yourself from that. And so specifically. As I mentioned, spans are a bit like a structured log. Opentelemetry gives a very specific vendor agnostic standard that a lot of vendors buy into. That then allows you to specifies how things should be formatted and how things should be logged. They even have a standard that's kind of, you know, in terms of the standard, they have a standard around what attributes and what they're called to be logged. So this is where if you use OpenTelemetry and a provider that supports ingesting it. They then very easily can then, this is where you can attach metadata like the LLM token usage, the LLM, how many tokens were input, how many tokens were output What were the inputs. What was the type of model. And so they're providing a structured schema that then any downstream consumer, as long as they support it, can ingest it.", "metadata": {"position": 10, "speaker": "Unknown", "timestamp": "Chunk 11", "original_chunk_id": "70265733-6a58-4bff-a8db-edbb94471961", "workshop_id": "WS4", "token_count": 1029, "source": "sentence_chunking"}, "id": "WS4_70265733-6a58-4bff-a8db-edbb94471961", "relevance": 1.0}, {"text": "And then within it, you'll see kind of these kind of boxes that represent spans and so There could be an overarching span that underneath is calling various different things. So you can then see the breakdown of the span as it kind of Who responds, what type of cuisine do you prefer would be, you know, this kind of represent one trace and then the next input would then represented by another trace, right. And so this is where, depending if it's agents or other things, you know, different call paths or code and logic can be taken. And so, uh. So therefore, I mentioned spans This is the other word you'll hear if you hear about tracing, it spans right and so In a LLM app or something like this will be different these bands will correspond to Extraction going for to an API for retrieval the vector database for retrieval coming back you know that the LLM call, how long did it take. And so spans, you can think of it, if you're familiar with logging, it's kind of like a structured log where There are IDs that help it capture the relationship between like, so what was my parents span so what created me But then also some sort of, you know, it does have a payload that you can kind of, that is fairly flexible. There's a lot of UIs here. There's a lot of vendors here. This is an example of UI, but if you think about for LLM side. So Datadog has now built a product here but This does come from distributed tracing, which is between distributed systems. The only difference LLM providers here in the space have done is they've just provided a different UI treatment. To uh two traces. And so very, very important pretty much all of them have this kind of interface. So if you can kind of see this kind of tree type structure of relationship between like spans and things and then When you cook on them, you can kind of see more information, and this is what was logged along with that span. Right. How do traces fit in right it is you know unless you can use them in development, right. So as you're running your app locally this is you can actually leverage this as a way to do data collection but then When it goes to production, you can also do the same thing. So this is the interesting difference between regular software engineering If you are familiar with distributed tracing and Datadog, you would only use that in production But here with Gen AI tools, you can actually use the tracing tools locally in development for yourself as well. And so\u2026 what do those tools look like. How do you integrate them. So I'm just picking Langfuse just because it's a popular open source project. The main thing that you'll kind of see is like there is, you know, you have to install some libraries You then potentially, if you're using their cloud service or you basically have to put in some sort of endpoint And then you're instrumenting so you're instrumenting Using that particular library Importing those modules and then say in this case here, decorating functions that then When this function runs, it is capturing certain aspects about it. And then this doesn't have to be LLM calls. It could be any internal function right and so That's just kind of one example. I think, you know, Arise Phoenix, just to give you another example, right, is like you install something, import things to set things up and then wherever you want to kind of track or do more stuff right you Basically using that SDK, so to speak, to kind of instrument to create traces. Yeah. So nothing too complex, but But I want to say in the last year and a half, there's like a lot of these tracing providers initially when they came out, they were like very they were proprietary right meaning that the data format they sent was Something that they came up. But OpenTelemetry, which is an open standard, we can see a lot of convergence on vendors using this. And so one of the things you want to avoid is vendor lock-in. And so OpenTelemetry allows you to kind of uncheck yourself from that. And so specifically. As I mentioned, spans are a bit like a structured log. Opentelemetry gives a very specific vendor agnostic standard that a lot of vendors buy into. That then allows you to specifies how things should be formatted and how things should be logged. They even have a standard that's kind of, you know, in terms of the standard, they have a standard around what attributes and what they're called to be logged. So this is where if you use OpenTelemetry and a provider that supports ingesting it. They then very easily can then, this is where you can attach metadata like the LLM token usage, the LLM, how many tokens were input, how many tokens were output What were the inputs. What was the type of model. And so they're providing a structured schema that then any downstream consumer, as long as they support it, can ingest it.", "metadata": {"position": 10, "source": "sentence_chunking", "workshop_id": "WS4", "speaker": "Unknown", "timestamp": "Chunk 11", "original_chunk_id": "a6d99531-2ae0-4f8d-a810-cb420f7cc61e", "token_count": 1029}, "id": "WS4_a6d99531-2ae0-4f8d-a810-cb420f7cc61e", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "speaker": "Unknown", "position": 6, "timestamp": "Chunk 7", "token_count": 1023, "workshop_id": "WS5"}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"timestamp": "Chunk 7", "source": "sentence_chunking", "workshop_id": "WS5", "token_count": 1023, "speaker": "Unknown", "original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246", "position": 6}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"workshop_id": "WS3", "original_chunk_id": "6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "position": 8, "timestamp": "Chunk 9", "token_count": 1067, "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS3_6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"workshop_id": "WS3", "source": "sentence_chunking", "position": 8, "timestamp": "Chunk 9", "token_count": 1067, "speaker": "Unknown", "original_chunk_id": "9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39"}, "id": "WS3_9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "relevance": 1.0}, {"text": "And then since then, I've actually come back to it a couple of times and continued to kind of build on it. If you've seen inference time compute scaling come up either in this class or elsewhere. It's the idea that you like you take a model, you have it generate a bunch of answers, and then you take only the ones that are right. So I applied that in this one as well. So I generate a bunch of QR codes and then I take only the ones that scan. So only the ones that I'm able to actually scan By running like a little QR code scanner on also running on Modal. Little Python library that scans QR codes and then take those and then find the one that looks best according to like a little aesthetic judge and show that. So that's uh this thing here. This QR code, by the way, should\u2026 get you to the form to fill out to get your $1,000 in modal credits. So\u2026 You can head to that bit.ly slash modal dash credits to get your thousand bucks in credits. So yeah, so this guy here, it's like it's a diffusion model. I think I actually also host See, the front end actually is, I think, hosted on Vercel probably, but the back end, all that stuff runs on modal. Generation QR code evaluation and um And like a little job queue. So that's one project. Another one, another fun little thing built on Moto you can check out. This is Twitter-95.com. So this is a little fake version of Twitter, like as if It was around in 1995, or I guess 1996, because it's been up for a year. So we've moved forward a year. So it's got a bunch of celebrities and people from the 90s like Francis Collins here. What did he do. He did like\u2026 Oh yeah, Human Genome Project. So we got some famous people from the 90s, Tim Berners-Lee. And others. And these are all language models pretending to be celebrities from the 90s to make like a little fake social media experience from the past. So you can find the code for this online as well. And it's like, you know, you got\u2026 extracting information from the past and then posting it putting it into the language models context and then having the models run, all that's done off of Off of modal. So that's kind of like the, you know, the sorts of stuff that I've been that uh Like Moodle makes possible these like sort of more, you know, complex applications, more like custom uses of a diffusion model or a language model that are a little off the beaten path. So yeah, I guess maybe\u2026 Um\u2026 Oh yeah, since this is building LLM apps in particular. I also wanted to show real quick, just like we put up one of the examples we put up Most recently is this latency sensitive latency sensitive serving. So like to serve yourself like a little chat bot This just got added recently and is probably going to be useful for things you want to build in this class if you're thinking about building. Agent or a little chat bot. And\u2026 So let me just run the thing. Show you what you get. So this\u2026 This is all running on modal. It's a single file Python to deploy this thing, build a tensorRTLM engine from scratch And that's\u2026 Uh\u2026 you get this nice, very fast and snappy Interaction. Oh, you go out and answer this renowned professor of data science and statistics at uc berkeley. That's approximately correct. It's a small model, so it doesn't get\u2026 It doesn't get everything right. I also don't mind that lie or that hallucination. I'm sorry. I'm sorry to call you. Yeah, I prefer that to other types of hallucinations it could give. Yeah. Yeah, really a professor at Berkeley. That sounds wrong. Yeah. I'm a large language model and I sometimes make errors. Yeah. Um anyway so that they're like the main thing there is like the model starts like starts responding once it's live in about you know 250 milliseconds or so if you have like short chat prompts, which is enough to feel nice and interactive. All the code to figure out how to do that and serve that on modal is on is in the examples there. Yeah, so the modal is this like very generic infrastructure platform designed to make building those things possible. So I want to talk about like. What this infrastructure platform is and why it matters, like more broadly even than just thinking about like building LLM apps I think the main thing once you go, once you get past the point where you're initially like. You know, trying things out, working in a notebook to figure out like what the model does\u2026 in response to different inputs, do your evaluations.", "metadata": {"speaker": "Unknown", "token_count": 999, "source": "sentence_chunking", "position": 5, "original_chunk_id": "b17df94d-ef09-47f6-a5b6-c239d92689c0", "workshop_id": "WS2", "timestamp": "Chunk 6"}, "id": "WS2_b17df94d-ef09-47f6-a5b6-c239d92689c0", "relevance": 1.0}, {"text": "And then since then, I've actually come back to it a couple of times and continued to kind of build on it. If you've seen inference time compute scaling come up either in this class or elsewhere. It's the idea that you like you take a model, you have it generate a bunch of answers, and then you take only the ones that are right. So I applied that in this one as well. So I generate a bunch of QR codes and then I take only the ones that scan. So only the ones that I'm able to actually scan By running like a little QR code scanner on also running on Modal. Little Python library that scans QR codes and then take those and then find the one that looks best according to like a little aesthetic judge and show that. So that's uh this thing here. This QR code, by the way, should\u2026 get you to the form to fill out to get your $1,000 in modal credits. So\u2026 You can head to that bit.ly slash modal dash credits to get your thousand bucks in credits. So yeah, so this guy here, it's like it's a diffusion model. I think I actually also host See, the front end actually is, I think, hosted on Vercel probably, but the back end, all that stuff runs on modal. Generation QR code evaluation and um And like a little job queue. So that's one project. Another one, another fun little thing built on Moto you can check out. This is Twitter-95.com. So this is a little fake version of Twitter, like as if It was around in 1995, or I guess 1996, because it's been up for a year. So we've moved forward a year. So it's got a bunch of celebrities and people from the 90s like Francis Collins here. What did he do. He did like\u2026 Oh yeah, Human Genome Project. So we got some famous people from the 90s, Tim Berners-Lee. And others. And these are all language models pretending to be celebrities from the 90s to make like a little fake social media experience from the past. So you can find the code for this online as well. And it's like, you know, you got\u2026 extracting information from the past and then posting it putting it into the language models context and then having the models run, all that's done off of Off of modal. So that's kind of like the, you know, the sorts of stuff that I've been that uh Like Moodle makes possible these like sort of more, you know, complex applications, more like custom uses of a diffusion model or a language model that are a little off the beaten path. So yeah, I guess maybe\u2026 Um\u2026 Oh yeah, since this is building LLM apps in particular. I also wanted to show real quick, just like we put up one of the examples we put up Most recently is this latency sensitive latency sensitive serving. So like to serve yourself like a little chat bot This just got added recently and is probably going to be useful for things you want to build in this class if you're thinking about building. Agent or a little chat bot. And\u2026 So let me just run the thing. Show you what you get. So this\u2026 This is all running on modal. It's a single file Python to deploy this thing, build a tensorRTLM engine from scratch And that's\u2026 Uh\u2026 you get this nice, very fast and snappy Interaction. Oh, you go out and answer this renowned professor of data science and statistics at uc berkeley. That's approximately correct. It's a small model, so it doesn't get\u2026 It doesn't get everything right. I also don't mind that lie or that hallucination. I'm sorry. I'm sorry to call you. Yeah, I prefer that to other types of hallucinations it could give. Yeah. Yeah, really a professor at Berkeley. That sounds wrong. Yeah. I'm a large language model and I sometimes make errors. Yeah. Um anyway so that they're like the main thing there is like the model starts like starts responding once it's live in about you know 250 milliseconds or so if you have like short chat prompts, which is enough to feel nice and interactive. All the code to figure out how to do that and serve that on modal is on is in the examples there. Yeah, so the modal is this like very generic infrastructure platform designed to make building those things possible. So I want to talk about like. What this infrastructure platform is and why it matters, like more broadly even than just thinking about like building LLM apps I think the main thing once you go, once you get past the point where you're initially like. You know, trying things out, working in a notebook to figure out like what the model does\u2026 in response to different inputs, do your evaluations.", "metadata": {"speaker": "Unknown", "position": 5, "source": "sentence_chunking", "timestamp": "Chunk 6", "original_chunk_id": "cda4270d-1415-466e-b58c-297627939ec4", "token_count": 999, "workshop_id": "WS2"}, "id": "WS2_cda4270d-1415-466e-b58c-297627939ec4", "relevance": 1.0}]}}{"question": "Could you explain how to effectively implement prompt engineering techniques for optimizing LLM outputs in data analysis tasks?", "response": "To optimize LLM outputs in data analysis tasks through effective prompt engineering, start by clearly defining your objectives. Identify the specific information you need from your data and structure your prompts accordingly. \n\n1. **Contextualization**: Prepend relevant context to your prompts. This could include summarizing key data points or providing structured templates that outline the expected format of the response.\n\n2. **Iterative Refinement**: Use a few-shot learning approach by providing examples of both good and bad outputs. This helps the model understand the desired response style and content.\n\n3. **Evaluation Criteria**: Establish clear criteria for acceptable responses, such as factual accuracy and relevance to the data. This will guide the model in generating outputs that meet your needs.\n\n4. **Logging and Monitoring**: Implement robust logging to track inputs and outputs. This allows you to identify patterns in errors, such as retrieval issues or incomplete responses, and adjust your prompts accordingly.\n\n5. **Feedback Loop**: Regularly review the outputs and refine your prompts based on observed performance. This iterative process will enhance the model's ability to generate accurate and relevant insights.\n\nBy focusing on these techniques, you can significantly improve the effectiveness of LLMs in data analysis tasks.", "context_info": {"num_chunks": 10, "context_tokens": 10264, "completion_tokens": 246, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS5", "WS6", "WS1"], "chunks": [{"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"position": 13, "source": "sentence_chunking", "timestamp": "Chunk 14", "original_chunk_id": "303c95ae-1d27-4792-ab45-321e8d4778a5", "token_count": 1056, "speaker": "Unknown", "workshop_id": "WS6"}, "id": "WS6_303c95ae-1d27-4792-ab45-321e8d4778a5", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"original_chunk_id": "e3b959c4-0d2f-4659-bc95-7dd30946df77", "source": "sentence_chunking", "timestamp": "Chunk 14", "speaker": "Unknown", "token_count": 1056, "position": 13, "workshop_id": "WS6"}, "id": "WS6_e3b959c4-0d2f-4659-bc95-7dd30946df77", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"position": 10, "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "token_count": 1062, "speaker": "Unknown", "timestamp": "Chunk 11", "source": "sentence_chunking", "workshop_id": "WS3"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"workshop_id": "WS3", "source": "sentence_chunking", "timestamp": "Chunk 11", "position": 10, "token_count": 1062, "speaker": "Unknown", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "And this was a case where, like. 479 Phillip Carter: I didn't need a sophisticated Eval to tell me that this was a problem. I could just literally see it in the inputs and outputs. And I'm like, Okay, this is categorically wrong here. And what do I do to fix this. Well. 480 Phillip Carter: I didn't have error rates, or like some variation of what is my error rate in the few shot examples in the prompt. So I just put it in there and then it stopped happening. I couldn't reproduce like I could. I could reproduce that problem before, but then I failed to reproduce it after I put the few shot example in there 481 Phillip Carter: and then deployed it, and then I verified again like this is why observability is really important. I verified after 24\u00a0h or so of of usage that like this actually did mitigate the problem substantially. And and so like, that's where I think, like. 482 Phillip Carter: you know, I'm saying observability a lot. But but, like, you know, it's as if I work for an observability company. But I think it's really really important for doing any kind of Ml. Work that, like 483 Phillip Carter: the work that you do is not divorced from production, like production, is the true reality of what is actually happening to your systems. And like, that's what's going to be the source of whatever fixes you have to do and whatever makes it as easy as possible to get that real data and pull it back into whatever development environment you have is going to save you so much time like in our case we designed it. So it's super super easy for us to just see that. 484 Phillip Carter: And whether we pull something down and annotate it for an Eval, or if we just say, Oh, well, we're just going to fix that right now, or just try a fix and see how it works. We have that freedom. And it was really easy. So yeah, there's those kinds of levers. Our retrieval is very often a lever. I think it probably shouldn't surprise too many people that if you're doing rag of any sort, like the kind of output that you get is going to be a function of how good the retrieval process was 485 Phillip Carter: In our case we learned a couple different things like honeycomb, has a concept of a stale column in a schema, and the name of a column 486 Phillip Carter: might be relevant from a semantic standpoint to the input that somebody gave. But if it is a stale column, then it's not worth bringing in, because it won't have any data. It's just a name of a column like, if you try to query against this column, you're going to get no results. And then that actually results in a really bad user experience. So you want to say, Okay, well, my rag process is not just doing semantic similarity or basic hybrid search. It is incorporating other aspects like 487 Phillip Carter: data liveness. And we've not done this. But I think we could go down the rabbit hole on on rag in particular, and explore a lot of different options in parallel, that we have some beliefs that will significantly improve query assistant 488 Phillip Carter: compared to how it exists today and make it much more relevant, for, like harder tasks that some of our, some of our larger customers have, so I don't know. Anyways like those are the the main levers that I had there. Fine tuning is sort of like the nuclear option in my mind, because, like, you need to have this whole more 489 Phillip Carter: larger process of generating really good representative data. And then you fine tune the thing. And you got to figure out how you're going to measure it. And like, do you need to have different evals for the fine-tuned model compared to what you have for Gpt. 3.5. I'm not sure if anybody's really written that. But like, if you use an Eval data set to fine tune, a model like, is that good. Is that bad like in our case. It was okay. But like we didn't 490 Phillip Carter: and anyways, I I don't wanna talk about that one. I feel like we've talked about forever. But 491 Phillip Carter: bottom line when you have it really easy to see your data, whether it's through an observability tool or in a spreadsheet, or whatever. You can just make it very readily apparent that, like certain inputs, do not yield good outputs, and sometimes you can just fix it with a few shot, prompt or 492 Phillip Carter: try prompt variation and and do an ab test and measure how that performs like. There's a lot of tools at your disposal that are pretty easy to deploy once you have that baseline understanding of how inputs and outputs sort of correlate with each other. 493 hugo bowne-anderson: Amazing.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "65302b25-b6f4-4bef-b10a-6cf12069ce95", "workshop_id": "WS6", "position": 21, "token_count": 1040, "timestamp": "Chunk 22"}, "id": "WS6_65302b25-b6f4-4bef-b10a-6cf12069ce95", "relevance": 1.0}, {"text": "And this was a case where, like. 479 Phillip Carter: I didn't need a sophisticated Eval to tell me that this was a problem. I could just literally see it in the inputs and outputs. And I'm like, Okay, this is categorically wrong here. And what do I do to fix this. Well. 480 Phillip Carter: I didn't have error rates, or like some variation of what is my error rate in the few shot examples in the prompt. So I just put it in there and then it stopped happening. I couldn't reproduce like I could. I could reproduce that problem before, but then I failed to reproduce it after I put the few shot example in there 481 Phillip Carter: and then deployed it, and then I verified again like this is why observability is really important. I verified after 24\u00a0h or so of of usage that like this actually did mitigate the problem substantially. And and so like, that's where I think, like. 482 Phillip Carter: you know, I'm saying observability a lot. But but, like, you know, it's as if I work for an observability company. But I think it's really really important for doing any kind of Ml. Work that, like 483 Phillip Carter: the work that you do is not divorced from production, like production, is the true reality of what is actually happening to your systems. And like, that's what's going to be the source of whatever fixes you have to do and whatever makes it as easy as possible to get that real data and pull it back into whatever development environment you have is going to save you so much time like in our case we designed it. So it's super super easy for us to just see that. 484 Phillip Carter: And whether we pull something down and annotate it for an Eval, or if we just say, Oh, well, we're just going to fix that right now, or just try a fix and see how it works. We have that freedom. And it was really easy. So yeah, there's those kinds of levers. Our retrieval is very often a lever. I think it probably shouldn't surprise too many people that if you're doing rag of any sort, like the kind of output that you get is going to be a function of how good the retrieval process was 485 Phillip Carter: In our case we learned a couple different things like honeycomb, has a concept of a stale column in a schema, and the name of a column 486 Phillip Carter: might be relevant from a semantic standpoint to the input that somebody gave. But if it is a stale column, then it's not worth bringing in, because it won't have any data. It's just a name of a column like, if you try to query against this column, you're going to get no results. And then that actually results in a really bad user experience. So you want to say, Okay, well, my rag process is not just doing semantic similarity or basic hybrid search. It is incorporating other aspects like 487 Phillip Carter: data liveness. And we've not done this. But I think we could go down the rabbit hole on on rag in particular, and explore a lot of different options in parallel, that we have some beliefs that will significantly improve query assistant 488 Phillip Carter: compared to how it exists today and make it much more relevant, for, like harder tasks that some of our, some of our larger customers have, so I don't know. Anyways like those are the the main levers that I had there. Fine tuning is sort of like the nuclear option in my mind, because, like, you need to have this whole more 489 Phillip Carter: larger process of generating really good representative data. And then you fine tune the thing. And you got to figure out how you're going to measure it. And like, do you need to have different evals for the fine-tuned model compared to what you have for Gpt. 3.5. I'm not sure if anybody's really written that. But like, if you use an Eval data set to fine tune, a model like, is that good. Is that bad like in our case. It was okay. But like we didn't 490 Phillip Carter: and anyways, I I don't wanna talk about that one. I feel like we've talked about forever. But 491 Phillip Carter: bottom line when you have it really easy to see your data, whether it's through an observability tool or in a spreadsheet, or whatever. You can just make it very readily apparent that, like certain inputs, do not yield good outputs, and sometimes you can just fix it with a few shot, prompt or 492 Phillip Carter: try prompt variation and and do an ab test and measure how that performs like. There's a lot of tools at your disposal that are pretty easy to deploy once you have that baseline understanding of how inputs and outputs sort of correlate with each other. 493 hugo bowne-anderson: Amazing.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 22", "original_chunk_id": "7fae5add-2859-45d9-8cbc-9204e6369132", "token_count": 1040, "source": "sentence_chunking", "position": 21, "workshop_id": "WS6"}, "id": "WS6_7fae5add-2859-45d9-8cbc-9204e6369132", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"position": 6, "source": "sentence_chunking", "token_count": 1023, "timestamp": "Chunk 7", "workshop_id": "WS5", "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "speaker": "Unknown"}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"timestamp": "Chunk 7", "workshop_id": "WS5", "original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246", "position": 6, "token_count": 1023, "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 11", "token_count": 1050, "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "source": "sentence_chunking", "workshop_id": "WS1", "position": 10}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "token_count": 1050, "source": "sentence_chunking", "timestamp": "Chunk 11", "speaker": "Unknown", "position": 10, "workshop_id": "WS1"}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}]}}{"question": "How can I effectively evaluate the performance of my LLM application, and what metrics should I focus on to measure its reliability?", "response": "To effectively evaluate the performance of your LLM application, start by establishing clear evaluation criteria that align with your specific use cases. Focus on the following key metrics:\n\n1. **Correctness**: Ensure the output accurately answers the query or task. For example, in information retrieval, check if the response is relevant and complete.\n\n2. **Conciseness**: In summarization tasks, measure whether the output captures key points without unnecessary verbosity.\n\n3. **Completeness**: Assess if the response provides all necessary information. Incomplete answers can lead to user dissatisfaction.\n\n4. **User Feedback**: Incorporate feedback from real users to identify areas for improvement. This can help you understand how well the application meets user needs.\n\n5. **Error Analysis**: Classify errors (e.g., hallucinations, retrieval errors) to prioritize fixes based on their impact on user experience.\n\n6. **Human Evaluation**: Regularly calibrate your model against human judgments to ensure consistency and reliability.\n\n7. **Monitoring for Drift**: After launch, continuously monitor performance to detect any shifts in output quality over time.\n\nBy focusing on these metrics and maintaining a cycle of evaluation and iteration, you can enhance the reliability of your LLM application.", "context_info": {"num_chunks": 10, "context_tokens": 10410, "completion_tokens": 248, "embedding_tokens": 15360, "workshops_used": ["WS3"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"token_count": 1068, "source": "sentence_chunking", "workshop_id": "WS3", "timestamp": "Chunk 15", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "position": 14, "speaker": "Unknown"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "source": "sentence_chunking", "workshop_id": "WS3", "position": 14, "speaker": "Unknown", "token_count": 1068, "timestamp": "Chunk 15"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "position": 10, "workshop_id": "WS3", "timestamp": "Chunk 11", "token_count": 1062, "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "speaker": "Unknown", "workshop_id": "WS3", "source": "sentence_chunking", "timestamp": "Chunk 11", "position": 10, "token_count": 1062}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"workshop_id": "WS3", "position": 15, "original_chunk_id": "ef57fa87-2177-44f5-bab8-24ccb6a587f7", "speaker": "Unknown", "timestamp": "Chunk 16", "token_count": 1049, "source": "sentence_chunking"}, "id": "WS3_ef57fa87-2177-44f5-bab8-24ccb6a587f7", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 16", "token_count": 1049, "original_chunk_id": "62ef5b55-d219-45ff-a564-75ead9264997", "position": 15, "workshop_id": "WS3"}, "id": "WS3_62ef5b55-d219-45ff-a564-75ead9264997", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"timestamp": "Chunk 14", "speaker": "Unknown", "source": "sentence_chunking", "token_count": 1021, "position": 13, "workshop_id": "WS3", "original_chunk_id": "a75cb503-824d-4264-8ba3-c9b897398367"}, "id": "WS3_a75cb503-824d-4264-8ba3-c9b897398367", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"token_count": 1021, "position": 13, "timestamp": "Chunk 14", "workshop_id": "WS3", "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "6a43538e-cd1b-4b39-a447-314078bf7d81"}, "id": "WS3_6a43538e-cd1b-4b39-a447-314078bf7d81", "relevance": 1.0}, {"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"speaker": "Unknown", "workshop_id": "WS3", "original_chunk_id": "97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "position": 7, "source": "sentence_chunking", "token_count": 1061, "timestamp": "Chunk 8"}, "id": "WS3_97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "relevance": 1.0}, {"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"timestamp": "Chunk 8", "original_chunk_id": "acbab402-9567-43c9-b2a9-caa05bdd8999", "workshop_id": "WS3", "speaker": "Unknown", "token_count": 1061, "source": "sentence_chunking", "position": 7}, "id": "WS3_acbab402-9567-43c9-b2a9-caa05bdd8999", "relevance": 1.0}]}}{"question": "How can I effectively evaluate the performance of my LLM applications in production, and what specific metrics should I focus on?", "response": "To effectively evaluate the performance of your LLM applications in production, start by establishing clear evaluation criteria that align with your specific use cases. Focus on the following key metrics:\n\n1. **Correctness**: Ensure the output accurately reflects the required information. For example, in extraction tasks, check if all necessary data is captured.\n\n2. **Conciseness**: In summarization tasks, measure whether the output captures key points without unnecessary length.\n\n3. **Relevance**: For information retrieval, assess if the answers provided are accurate, relevant, and complete, grounded in the source documents.\n\n4. **User Satisfaction**: Gather feedback from users to understand their experience and identify areas for improvement.\n\n5. **Performance Monitoring**: Track agreement with human evaluations to ensure consistency and watch for drift or biases in the model's outputs.\n\n6. **Iterative Improvement**: Regularly calibrate your evaluation criteria based on user feedback and performance data, adjusting as necessary to maintain high standards.\n\nStart small with human evaluations, establish baselines, and gradually scale your evaluation processes as you build trust in your system. This approach will help you maintain oversight and ensure your LLM applications meet user needs effectively.", "context_info": {"num_chunks": 10, "context_tokens": 10300, "completion_tokens": 240, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS4"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"source": "sentence_chunking", "position": 14, "workshop_id": "WS3", "speaker": "Unknown", "timestamp": "Chunk 15", "token_count": 1068, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "timestamp": "Chunk 15", "source": "sentence_chunking", "speaker": "Unknown", "token_count": 1068, "workshop_id": "WS3", "position": 14}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"token_count": 1049, "source": "sentence_chunking", "timestamp": "Chunk 16", "original_chunk_id": "ef57fa87-2177-44f5-bab8-24ccb6a587f7", "workshop_id": "WS3", "speaker": "Unknown", "position": 15}, "id": "WS3_ef57fa87-2177-44f5-bab8-24ccb6a587f7", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"original_chunk_id": "62ef5b55-d219-45ff-a564-75ead9264997", "speaker": "Unknown", "workshop_id": "WS3", "token_count": 1049, "position": 15, "source": "sentence_chunking", "timestamp": "Chunk 16"}, "id": "WS3_62ef5b55-d219-45ff-a564-75ead9264997", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "source": "sentence_chunking", "workshop_id": "WS3", "timestamp": "Chunk 11", "token_count": 1062, "position": 10, "speaker": "Unknown"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "position": 10, "timestamp": "Chunk 11", "workshop_id": "WS3", "token_count": 1062}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"speaker": "Unknown", "workshop_id": "WS4", "token_count": 1012, "position": 8, "timestamp": "Chunk 9", "original_chunk_id": "79e3ed2f-8447-44f2-a41f-7288f51c8da7", "source": "sentence_chunking"}, "id": "WS4_79e3ed2f-8447-44f2-a41f-7288f51c8da7", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"original_chunk_id": "d492c886-fee2-48a6-b289-95214ec3d247", "timestamp": "Chunk 9", "token_count": 1012, "speaker": "Unknown", "position": 8, "source": "sentence_chunking", "workshop_id": "WS4"}, "id": "WS4_d492c886-fee2-48a6-b289-95214ec3d247", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"timestamp": "Chunk 14", "token_count": 1021, "position": 13, "speaker": "Unknown", "workshop_id": "WS3", "original_chunk_id": "a75cb503-824d-4264-8ba3-c9b897398367", "source": "sentence_chunking"}, "id": "WS3_a75cb503-824d-4264-8ba3-c9b897398367", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "timestamp": "Chunk 14", "workshop_id": "WS3", "original_chunk_id": "6a43538e-cd1b-4b39-a447-314078bf7d81", "position": 13, "token_count": 1021}, "id": "WS3_6a43538e-cd1b-4b39-a447-314078bf7d81", "relevance": 1.0}]}}{"question": "How can I effectively evaluate the performance of my LLM-based system, and what metrics should I focus on to assess its reliability in production environments?", "response": "To effectively evaluate the performance of your LLM-based system, start by establishing clear evaluation criteria that align with your specific use cases. Focus on the following key metrics:\n\n1. **Accuracy**: Ensure the output provides accurate, relevant, and complete answers. This can be assessed through human evaluations and automated metrics like BLEU for translation tasks or ROUGE for summarization.\n\n2. **Completeness**: Check if the responses are not only correct but also comprehensive. Incomplete answers can lead to user dissatisfaction.\n\n3. **Non-Determinism**: Recognize that LLMs can produce different outputs for the same input. Monitor this variability and assess how it impacts user experience.\n\n4. **User Feedback**: Incorporate user feedback to gauge the effectiveness of the responses. This can help identify areas for improvement.\n\n5. **Logging and Monitoring**: Implement robust logging to capture inputs, outputs, and any errors. This data is crucial for diagnosing issues and understanding system performance over time.\n\n6. **Regular Calibration**: Regularly compare your system's outputs against human judgments to ensure alignment and adjust your evaluation criteria as needed.\n\nBy focusing on these metrics and maintaining a cycle of evaluation and iteration, you can enhance the reliability of your LLM-based system in production environments.", "context_info": {"num_chunks": 10, "context_tokens": 10342, "completion_tokens": 257, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS1", "WS4"], "chunks": [{"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"workshop_id": "WS3", "source": "sentence_chunking", "original_chunk_id": "ef57fa87-2177-44f5-bab8-24ccb6a587f7", "position": 15, "speaker": "Unknown", "timestamp": "Chunk 16", "token_count": 1049}, "id": "WS3_ef57fa87-2177-44f5-bab8-24ccb6a587f7", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"original_chunk_id": "62ef5b55-d219-45ff-a564-75ead9264997", "position": 15, "timestamp": "Chunk 16", "token_count": 1049, "speaker": "Unknown", "workshop_id": "WS3", "source": "sentence_chunking"}, "id": "WS3_62ef5b55-d219-45ff-a564-75ead9264997", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS3", "position": 14, "token_count": 1068, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "timestamp": "Chunk 15"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"workshop_id": "WS3", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "source": "sentence_chunking", "token_count": 1068, "position": 14, "speaker": "Unknown", "timestamp": "Chunk 15"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"original_chunk_id": "79e3ed2f-8447-44f2-a41f-7288f51c8da7", "timestamp": "Chunk 9", "speaker": "Unknown", "workshop_id": "WS4", "token_count": 1012, "position": 8, "source": "sentence_chunking"}, "id": "WS4_79e3ed2f-8447-44f2-a41f-7288f51c8da7", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 9", "source": "sentence_chunking", "original_chunk_id": "d492c886-fee2-48a6-b289-95214ec3d247", "workshop_id": "WS4", "position": 8, "token_count": 1012}, "id": "WS4_d492c886-fee2-48a6-b289-95214ec3d247", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"speaker": "Unknown", "position": 10, "source": "sentence_chunking", "timestamp": "Chunk 11", "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "workshop_id": "WS1", "token_count": 1050}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"timestamp": "Chunk 11", "workshop_id": "WS1", "position": 10, "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "token_count": 1050}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 11", "token_count": 1062, "speaker": "Unknown", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "source": "sentence_chunking", "position": 10}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS3", "position": 10, "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "token_count": 1062, "timestamp": "Chunk 11"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}]}}{"question": "How can I evaluate the performance of my LLM application to ensure it meets the requirements of the project I'm working on?", "response": "To effectively evaluate the performance of your LLM application, follow these steps:\n\n1. **Define Evaluation Criteria**: Establish what \"good\" means for your specific use case. Consider dimensions like correctness, relevance, clarity, and completeness. For instance, in summarization, ensure the output captures key points concisely without unnecessary details.\n\n2. **Create a Minimum Viable Evaluation Framework (MVE)**: Develop a structured evaluation harness that includes both good and bad examples of outputs. This will help the LLM learn from its mistakes and improve over time.\n\n3. **Utilize Data Analysis**: Log and inspect your application\u2019s outputs. Identify failure modes by analyzing a sample of outputs (e.g., using pivot tables) to see where most errors occur. Focus on high-impact issues first, such as hallucinations or incomplete responses.\n\n4. **Incorporate User Feedback**: Engage potential users to gather insights on the application\u2019s performance. Their feedback can guide iterative improvements.\n\n5. **Automate Evaluation**: Implement Continuous Integration/Continuous Deployment (CICD) practices to automate the evaluation process, ensuring consistent and objective results.\n\nBy following these steps, you can systematically assess and enhance your LLM application to meet project requirements effectively.", "context_info": {"num_chunks": 10, "context_tokens": 10401, "completion_tokens": 250, "embedding_tokens": 15360, "workshops_used": ["WS3"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 15", "position": 14, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "source": "sentence_chunking", "token_count": 1068, "workshop_id": "WS3"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "token_count": 1068, "timestamp": "Chunk 15", "speaker": "Unknown", "workshop_id": "WS3", "position": 14}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"timestamp": "Chunk 11", "token_count": 1062, "position": 10, "workshop_id": "WS3", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"workshop_id": "WS3", "speaker": "Unknown", "timestamp": "Chunk 11", "position": 10, "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "source": "sentence_chunking", "token_count": 1062}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"token_count": 1021, "source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 14", "original_chunk_id": "a75cb503-824d-4264-8ba3-c9b897398367", "workshop_id": "WS3", "position": 13}, "id": "WS3_a75cb503-824d-4264-8ba3-c9b897398367", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"original_chunk_id": "6a43538e-cd1b-4b39-a447-314078bf7d81", "workshop_id": "WS3", "timestamp": "Chunk 14", "position": 13, "speaker": "Unknown", "token_count": 1021, "source": "sentence_chunking"}, "id": "WS3_6a43538e-cd1b-4b39-a447-314078bf7d81", "relevance": 1.0}, {"text": "And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products. Funny, though. I've shown this screenshot before. I just send this to everyone though. People reach out to me for consulting and I'm just like, can you just read Hamill's post and tell me if you still want to work with me. To be honest. And because everyone focuses on changing the behavior of the system Very few focus on evaluating quality and then debugging. How do you evaluate quality. Tests in one way. There are others, CICD, these things Debugging issues, how do you do this. You log and inspect your data and it It almost sounds silly. To say that's the process. Because it seems almost obvious, but very few people want to do it and want to do it well. And something we'll see today is If you figure out what the failure modes are, if you look at 100 traces of your app. And start writing down the failure modes and then do a pivot table with respect to failure mode, you'll see like 30% of the failures are due to one thing, right. And then you start working on that thing as opposed to the thing that results in 2% of the failures, right. Kind of semi rigorous data analysis can get you a long way in product development. So what are we doing. We're going to be talking about an MVP needing an MVE, okay. How to get to this before product launch. Defining good outputs, learn how to evaluate the quality of outputs based on correctness, relevance, clarity. Annotation metrics, then thinking about scaling to LLM as a judge. So all that means, this is one of those terms that people like people think is super sophisticated But essentially all this means is this giving an LLM some heuristics To judge the output of your system. And perhaps some perhaps examples from your annotated data saying this is a positive example for this reason. This is a negative example for this reason. And then allowing it to be a judge. And after that, and this is something that Maybe Philip Carter will talk about next week at Honeycomb, but I think I mentioned this. What they did there is Hamill got them to Look at the LM as a judge results and then write a judgment of the LLM as a judge and then iterate on the judge there. Essentially. So this speaks to one of the most important challenges is You have an LLM as a judge and you want to align it with your own human evaluations. Because remember, all we're doing here is scaling human behavior, right. And human tasks. To that point, one of the most fascinating aspects of building an LLM as a judge is having to write down what you want it to do. Which is incredible because you may think you know what you want to be judged. The process of writing it down is incredibly clarifying for oneself as a builder. Then we're going to get into iterative improvement. And then I'm going to show you a hands-on task. And then, of course, in the project and homework and Builders Club, you'll jump in and then Hamill will be joining in an hour. To talk about a field guide to rapidly improving AI products. So firstly, I just want to say this is the direction we'll be moving in in the second half. But I just want to say that like it's up to you and your team on any product you're working on to figure out what's important, right. In summarization is correctness important is being concise. Important is being complete important and conciseness correctness and completeness are three very different things, right. They're related. But thinking about what dimensions you're really interested in and what's important to you to optimize for initially. And I think I said this last time. I can't reiterate enough that we're still figuring out summarization with LLMs, okay. This is so exciting. It's incredibly early days. It's electricity before the light bulb, okay. So we're really really like getting to basics. Other things such as format and structure, style, tone, length appropriateness, safety. We even saw how prompt engineering can help with these things as well. Having said that, guardrails are incredibly important there. And that's something Stefan will be talking about more later this week. So I want to give kind of a subtalk here for the next half hour or so. About\u2026 What I mean by a minimum viable evaluation framework and how we can even build one before we launch a product. So what I've done is I've built a simple RAG app using workshop transcripts, which I've shared with all of you. And let me just um go to it. So Let me ask. Who were the instructors in this workshop. And any of you can query this. Please don't all do it right now because it it will do them in sequence, not in parallel. And I think I've got a bit of a cold start problem because it hasn't run in a tick.", "metadata": {"timestamp": "Chunk 6", "original_chunk_id": "37813972-c3de-4e0d-ac1c-c806276b7944", "position": 5, "speaker": "Unknown", "token_count": 1056, "source": "sentence_chunking", "workshop_id": "WS3"}, "id": "WS3_37813972-c3de-4e0d-ac1c-c806276b7944", "relevance": 1.0}, {"text": "And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products. Funny, though. I've shown this screenshot before. I just send this to everyone though. People reach out to me for consulting and I'm just like, can you just read Hamill's post and tell me if you still want to work with me. To be honest. And because everyone focuses on changing the behavior of the system Very few focus on evaluating quality and then debugging. How do you evaluate quality. Tests in one way. There are others, CICD, these things Debugging issues, how do you do this. You log and inspect your data and it It almost sounds silly. To say that's the process. Because it seems almost obvious, but very few people want to do it and want to do it well. And something we'll see today is If you figure out what the failure modes are, if you look at 100 traces of your app. And start writing down the failure modes and then do a pivot table with respect to failure mode, you'll see like 30% of the failures are due to one thing, right. And then you start working on that thing as opposed to the thing that results in 2% of the failures, right. Kind of semi rigorous data analysis can get you a long way in product development. So what are we doing. We're going to be talking about an MVP needing an MVE, okay. How to get to this before product launch. Defining good outputs, learn how to evaluate the quality of outputs based on correctness, relevance, clarity. Annotation metrics, then thinking about scaling to LLM as a judge. So all that means, this is one of those terms that people like people think is super sophisticated But essentially all this means is this giving an LLM some heuristics To judge the output of your system. And perhaps some perhaps examples from your annotated data saying this is a positive example for this reason. This is a negative example for this reason. And then allowing it to be a judge. And after that, and this is something that Maybe Philip Carter will talk about next week at Honeycomb, but I think I mentioned this. What they did there is Hamill got them to Look at the LM as a judge results and then write a judgment of the LLM as a judge and then iterate on the judge there. Essentially. So this speaks to one of the most important challenges is You have an LLM as a judge and you want to align it with your own human evaluations. Because remember, all we're doing here is scaling human behavior, right. And human tasks. To that point, one of the most fascinating aspects of building an LLM as a judge is having to write down what you want it to do. Which is incredible because you may think you know what you want to be judged. The process of writing it down is incredibly clarifying for oneself as a builder. Then we're going to get into iterative improvement. And then I'm going to show you a hands-on task. And then, of course, in the project and homework and Builders Club, you'll jump in and then Hamill will be joining in an hour. To talk about a field guide to rapidly improving AI products. So firstly, I just want to say this is the direction we'll be moving in in the second half. But I just want to say that like it's up to you and your team on any product you're working on to figure out what's important, right. In summarization is correctness important is being concise. Important is being complete important and conciseness correctness and completeness are three very different things, right. They're related. But thinking about what dimensions you're really interested in and what's important to you to optimize for initially. And I think I said this last time. I can't reiterate enough that we're still figuring out summarization with LLMs, okay. This is so exciting. It's incredibly early days. It's electricity before the light bulb, okay. So we're really really like getting to basics. Other things such as format and structure, style, tone, length appropriateness, safety. We even saw how prompt engineering can help with these things as well. Having said that, guardrails are incredibly important there. And that's something Stefan will be talking about more later this week. So I want to give kind of a subtalk here for the next half hour or so. About\u2026 What I mean by a minimum viable evaluation framework and how we can even build one before we launch a product. So what I've done is I've built a simple RAG app using workshop transcripts, which I've shared with all of you. And let me just um go to it. So Let me ask. Who were the instructors in this workshop. And any of you can query this. Please don't all do it right now because it it will do them in sequence, not in parallel. And I think I've got a bit of a cold start problem because it hasn't run in a tick.", "metadata": {"original_chunk_id": "eb81517b-90c2-417c-a8ea-478aafcbcd45", "timestamp": "Chunk 6", "workshop_id": "WS3", "token_count": 1056, "speaker": "Unknown", "source": "sentence_chunking", "position": 5}, "id": "WS3_eb81517b-90c2-417c-a8ea-478aafcbcd45", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"position": 8, "original_chunk_id": "6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS3", "token_count": 1067, "timestamp": "Chunk 9"}, "id": "WS3_6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"position": 8, "original_chunk_id": "9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "token_count": 1067, "workshop_id": "WS3", "timestamp": "Chunk 9", "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS3_9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "relevance": 1.0}]}}{"question": "What are some best practices for prompt engineering that can help me create more effective and reliable outputs from LLMs?", "response": "To enhance the effectiveness and reliability of outputs from LLMs, consider the following best practices for prompt engineering:\n\n1. **Clarity and Specificity**: Ensure your prompts are clear and specific. Avoid ambiguity by directly stating what you want the model to do. For example, instead of asking, \"Tell me about the Eiffel Tower,\" specify, \"Provide the year the Eiffel Tower was completed and its height.\"\n\n2. **Iterative Refinement**: Use an iterative approach to refine your prompts. Start with a basic prompt, evaluate the output, and adjust based on what works or doesn\u2019t. This helps in honing in on the most effective phrasing.\n\n3. **Structured Inputs**: When possible, provide structured data or context. For instance, if extracting information from a LinkedIn profile, format the input in JSON to guide the model in understanding the data structure.\n\n4. **Evaluation Criteria**: Establish clear evaluation criteria for acceptable responses. This includes checking for factual accuracy, relevance, and completeness. Define what \"good\" means for your specific use case.\n\n5. **Logging and Monitoring**: Implement logging to track inputs and outputs. This allows you to analyze performance and identify areas for improvement, ensuring that you can iterate quickly on your prompts.\n\n6. **Involve Domain Experts**: Engage domain experts in the evaluation process to ensure the outputs meet industry standards and expectations.\n\nBy applying these practices, you can significantly improve the quality of the outputs generated by LLMs.", "context_info": {"num_chunks": 10, "context_tokens": 10373, "completion_tokens": 299, "embedding_tokens": 15360, "workshops_used": ["WS4", "WS3", "WS1", "WS6"], "chunks": [{"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 11", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "position": 10, "token_count": 1062, "workshop_id": "WS3"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "workshop_id": "WS3", "token_count": 1062, "position": 10, "timestamp": "Chunk 11", "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"workshop_id": "WS1", "position": 10, "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "token_count": 1050, "timestamp": "Chunk 11"}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "position": 10, "source": "sentence_chunking", "timestamp": "Chunk 11", "workshop_id": "WS1", "token_count": 1050, "speaker": "Unknown"}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"token_count": 1056, "workshop_id": "WS6", "position": 13, "timestamp": "Chunk 14", "speaker": "Unknown", "original_chunk_id": "303c95ae-1d27-4792-ab45-321e8d4778a5", "source": "sentence_chunking"}, "id": "WS6_303c95ae-1d27-4792-ab45-321e8d4778a5", "relevance": 1.0}, {"text": "I'm practicing key hygiene and we'll delete it afterwards. Please don't use my key. I've given you all an anthropic key as well in the course, Wiki, I think. 324 hugo bowne-anderson: But let's jump in and just see certain types of 325 hugo bowne-anderson: examples of these workflows particularly with the types of things we've been thinking about. So the ingestion of professional profiles. I'm trying to match people to particular jobs. So this is a prompt chaining workflow from anthropic when you'll use. This is when a task naturally breaks down into sequential steps when each step's output feeds into the next. And when you need clear intermediate results. Okay, also, when order of operation matters. And 326 hugo bowne-anderson: I just want to say once again, we saw that like 327 hugo bowne-anderson: building out like a spec of your software can be useful, of course. Who who would have thought 328 hugo bowne-anderson: But when we build out a spec we can. 329 hugo bowne-anderson: We can see like we can try to break it down into atomic units and see what would be useful, right. So this isn't something that happens in the abstract. It's something that happens with a particular business problem or something you're trying to build. So try to break it down into atomic units and see what's up. So 330 hugo bowne-anderson: this example, 331 hugo bowne-anderson: remember, one thing we were thinking about is extracting structured data from a Linkedin profile. We weren't only thinking about that. We've done a lot of that. Then using that to generate a personalized email. Okay, so 332 hugo bowne-anderson: how do we do that. So this 1st example, we can create a chain function. Now, I actually don't use this in this case. But I'm giving you this because it's a template that you can use to chain things together, because I've only got 2 Llm. Calls. I'm doing it manually, but I wanted to give that to you. So 333 hugo bowne-anderson: we have this extract extract, structured data. We've seen prompts like this before, you know, provided in Json and so on. Then we have another function which is generate the outreach email and return and then we return the result right. 334 hugo bowne-anderson: And it says it should address the recipient by name reference their current position, company, etc. So I'm giving the example of Linkedin profile of Elliot Alderson, who's a cybersecurity engineer at Allsafe security, and so on. Anyone who's seen Mr. Robot may know him as the main character, or one of the main characters in Mr. Robot 335 hugo bowne-anderson: and 336 hugo bowne-anderson: step one is extract the structured data. And I'm printing that and then step 2 is generate the email. Okay, so we've got the structured data name current position skills, previous positions. Then we have the recipient's email. This is a beautiful example, unintended, of why we should look at our data. Now you may go. Oh, this is cool. But notice that 337 hugo bowne-anderson: the email generation has messed it up completely. It's writing the email from Elliot to the recipient. Okay, so immediately, looking at our data. We can see we need to go and do a bit of prompt iteration. And this is something that can be solved by prompt iteration. So that's why I've left that left that there. But this is a simple example of chaining, and in a way that we can introspect what happens in the middle. So you can imagine the span, the trace, the whole trace with individual spans within it. And the 1st 338 hugo bowne-anderson: span would be the input, 339 hugo bowne-anderson: the linkedin input and the prompt and then this as the output. And then the second one would be this is input and the prompt and the email as output. Okay. And immediately looking at those traces and spans, we know what to do next in our system. So we've built a little prototype of an augmented Llm. Just chaining 2 Llm. Calls together. But we can already see how to how to improve on the system by by inspecting 340 hugo bowne-anderson: I'm not going to dive too deep into the parallelization. One. What I do want to say is, parallelization is cool. 341 hugo bowne-anderson: Now, one thing I like about it is, it can be expensive, though. 342 hugo bowne-anderson: I've mentioned this before, but we've seen that with structured data extraction. If you try to do it all in one Llm.", "metadata": {"position": 13, "timestamp": "Chunk 14", "source": "sentence_chunking", "workshop_id": "WS6", "speaker": "Unknown", "original_chunk_id": "e3b959c4-0d2f-4659-bc95-7dd30946df77", "token_count": 1056}, "id": "WS6_e3b959c4-0d2f-4659-bc95-7dd30946df77", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"source": "sentence_chunking", "workshop_id": "WS3", "timestamp": "Chunk 15", "position": 14, "token_count": 1068, "speaker": "Unknown", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"timestamp": "Chunk 15", "position": 14, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "speaker": "Unknown", "workshop_id": "WS3", "token_count": 1068, "source": "sentence_chunking"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"source": "sentence_chunking", "token_count": 1048, "position": 14, "speaker": "Unknown", "original_chunk_id": "c9a18485-8306-4216-9342-b906b362f26b", "workshop_id": "WS4", "timestamp": "Chunk 15"}, "id": "WS4_c9a18485-8306-4216-9342-b906b362f26b", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS4", "position": 14, "token_count": 1048, "original_chunk_id": "6c6748ec-6e50-49e9-83a8-6600e01683d8", "timestamp": "Chunk 15"}, "id": "WS4_6c6748ec-6e50-49e9-83a8-6600e01683d8", "relevance": 1.0}]}}{"question": "What are some best practices for prompt engineering to ensure that the responses from an LLM are both relevant and accurate for my application?", "response": "To ensure that responses from a language model (LLM) are relevant and accurate, consider the following best practices for prompt engineering:\n\n1. **Clarity and Specificity**: Craft prompts that are clear and specific. Avoid ambiguity by directly stating what information you need. For example, instead of asking, \"Tell me about the Eiffel Tower,\" specify, \"What year was the Eiffel Tower completed, and for what event?\"\n\n2. **Contextual Information**: Provide relevant context within the prompt. This helps the LLM understand the background and tailor its response accordingly. For instance, if you're asking about a product, include details about the product's features or intended use.\n\n3. **Iterative Testing**: Experiment with different prompt structures and evaluate the outputs. Use a systematic approach, such as logging inputs and outputs, to identify which prompts yield the best results.\n\n4. **Evaluation Criteria**: Establish clear criteria for acceptable responses, such as factual accuracy, relevance, and completeness. This can help in assessing the quality of the LLM's outputs.\n\n5. **Incorporate Feedback**: Involve domain experts to review the outputs and provide feedback. This can enhance the quality of the responses and ensure they meet the specific needs of your application.\n\nBy implementing these practices, you can significantly improve the relevance and accuracy of the responses generated by the LLM for your application.", "context_info": {"num_chunks": 10, "context_tokens": 10316, "completion_tokens": 277, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS1"], "chunks": [{"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"workshop_id": "WS3", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "token_count": 1062, "timestamp": "Chunk 11", "source": "sentence_chunking", "position": 10, "speaker": "Unknown"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"position": 10, "speaker": "Unknown", "timestamp": "Chunk 11", "workshop_id": "WS3", "source": "sentence_chunking", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "token_count": 1062}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 15", "token_count": 1068, "position": 14, "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"token_count": 1068, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "timestamp": "Chunk 15", "position": 14, "workshop_id": "WS3", "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"token_count": 1050, "workshop_id": "WS1", "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "position": 10, "timestamp": "Chunk 11", "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 11", "position": 10, "token_count": 1050, "original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "workshop_id": "WS1"}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}, {"text": "This response. And it actually says the response does not directly answer the question with specific details from workshop one. Once again, I don't think this has access to workshop one. So I want to then The problem is with the judge there, not the response. And just to be clear, it isn't judges all the way down. We use human evaluation to align the judge. With ourselves. You can see that the Gemini model It passed. And it was quite good. So this is somewhere maybe we want to improve on the judge. Now here we can see what is the difference between LM fine-tuning prompt engineering. Pretty nice response here. Directly answers the question, great. The Gemini response, I messed up with my rate limiting. I didn't do it. Quite right. And so I got an error and I actually love that I did this. This is a teachable moment um So I'm glad that I messed this up. You can see the judge said this response does not address the question at all. Instead, it provides an error message related to API rate limits, which is irrelevant to the question. It's totally right. Perhaps I'd like my judge there to say, hey, yo, it looks like you erred out. You need to fix this. Um like it doesn't seem to be interpreting this as an issue with the system really like an an era, but it says enough there. So this is out of scope for now. But it's something that's incredibly important. I'll link with resources to it and perhaps we can have a session Later on, but I don't want to put the cart before the horse. Aligning LLM judge with your own evaluations is important. And you can even see as a figure like plotting over time, you know, alignment between LLN Judge and human. After a few iterations of this type of thing You get relatively close a lot of the time. And I'll actually link to one of Eugene Yad. If somebody wants to find Eugene Yan's blog post a line eval. It's A-L-I-G-n- E-V-A-L and put in the workshop three chat, he actually built um so he's this dude at Amazon. And he, oh, sorry, I'm just looking at workshop three channel now. So I haven't been looking at it. So people who are tagging me If there are serious issues, someone can turn their mic on and chat. But Eugene has built a small thing which gamifies aligning evals, essentially, which is pretty cool. So then to wrap up this section, I just want to talk about observability and instrumentation in Devon prod. So we've talked about the core loop of evaluation of cost, accuracy, latency user business metrics and using automated evaluation to do these scalable evaluations. And how does this work. We want to log it like a system, right. We want to have logging on the user side. So system prompts, user prompts, tool calls, function calls. There was a great question. That hopefully we get to talk about with Hamill, that Vishrut asked in workshop three in my Hamill thread with his post about What if you can't log user prompts. I actually wonder\u2026 William, because you work in a highly regulated space if you've had any products you've had to build where you can't log some user prompts and that type of stuff. Yeah, I mean, I have a couple of thoughts on this. I think like I have had use cases where we absolutely weren't Actually, no, that's not true. There was a use case for internal. I built an internal like chat system that I owned and people didn't want that data getting exported to the data warehouse because you can imagine like managers using it to write reviews and then like That ends up in the data warehouse and all of a sudden someone's going through data and sees like their draft review. I've had to handle company sensitive stuff. What I was going to say to the question about healthcare is that In my experience, the company has already had to be handling sensitive data. Like at the company I worked at. Most recently, we already had the user's healthcare claims So like we already needed infrastructure in place to be able to handle things according to the requirements of hipa and so it wasn't really an issue to then be able to Also like log chats because we already had like the infrastructure in place It's slightly more challenging because it's unstructured data. Like if it's a claim you know what fields to mask in order to make it not like THI because it's structured data structured data Whereas if it's text. How exactly to redact the text to meet the requirements of Privacy, that is, I think, a very complex topic and we definitely ran into challenges there Fascinating. Well, hopefully we get to discuss that a bit more with Hamill. As well. But in the limit where you're able to log things, which is a lot of the case. You log things on the user side, including tool calls, function calls, if any. And that's with the LM as well.", "metadata": {"workshop_id": "WS3", "speaker": "Unknown", "timestamp": "Chunk 12", "token_count": 1043, "original_chunk_id": "f0863d5a-dbf0-4447-9ca4-1c1bda125b3e", "source": "sentence_chunking", "position": 11}, "id": "WS3_f0863d5a-dbf0-4447-9ca4-1c1bda125b3e", "relevance": 1.0}, {"text": "This response. And it actually says the response does not directly answer the question with specific details from workshop one. Once again, I don't think this has access to workshop one. So I want to then The problem is with the judge there, not the response. And just to be clear, it isn't judges all the way down. We use human evaluation to align the judge. With ourselves. You can see that the Gemini model It passed. And it was quite good. So this is somewhere maybe we want to improve on the judge. Now here we can see what is the difference between LM fine-tuning prompt engineering. Pretty nice response here. Directly answers the question, great. The Gemini response, I messed up with my rate limiting. I didn't do it. Quite right. And so I got an error and I actually love that I did this. This is a teachable moment um So I'm glad that I messed this up. You can see the judge said this response does not address the question at all. Instead, it provides an error message related to API rate limits, which is irrelevant to the question. It's totally right. Perhaps I'd like my judge there to say, hey, yo, it looks like you erred out. You need to fix this. Um like it doesn't seem to be interpreting this as an issue with the system really like an an era, but it says enough there. So this is out of scope for now. But it's something that's incredibly important. I'll link with resources to it and perhaps we can have a session Later on, but I don't want to put the cart before the horse. Aligning LLM judge with your own evaluations is important. And you can even see as a figure like plotting over time, you know, alignment between LLN Judge and human. After a few iterations of this type of thing You get relatively close a lot of the time. And I'll actually link to one of Eugene Yad. If somebody wants to find Eugene Yan's blog post a line eval. It's A-L-I-G-n- E-V-A-L and put in the workshop three chat, he actually built um so he's this dude at Amazon. And he, oh, sorry, I'm just looking at workshop three channel now. So I haven't been looking at it. So people who are tagging me If there are serious issues, someone can turn their mic on and chat. But Eugene has built a small thing which gamifies aligning evals, essentially, which is pretty cool. So then to wrap up this section, I just want to talk about observability and instrumentation in Devon prod. So we've talked about the core loop of evaluation of cost, accuracy, latency user business metrics and using automated evaluation to do these scalable evaluations. And how does this work. We want to log it like a system, right. We want to have logging on the user side. So system prompts, user prompts, tool calls, function calls. There was a great question. That hopefully we get to talk about with Hamill, that Vishrut asked in workshop three in my Hamill thread with his post about What if you can't log user prompts. I actually wonder\u2026 William, because you work in a highly regulated space if you've had any products you've had to build where you can't log some user prompts and that type of stuff. Yeah, I mean, I have a couple of thoughts on this. I think like I have had use cases where we absolutely weren't Actually, no, that's not true. There was a use case for internal. I built an internal like chat system that I owned and people didn't want that data getting exported to the data warehouse because you can imagine like managers using it to write reviews and then like That ends up in the data warehouse and all of a sudden someone's going through data and sees like their draft review. I've had to handle company sensitive stuff. What I was going to say to the question about healthcare is that In my experience, the company has already had to be handling sensitive data. Like at the company I worked at. Most recently, we already had the user's healthcare claims So like we already needed infrastructure in place to be able to handle things according to the requirements of hipa and so it wasn't really an issue to then be able to Also like log chats because we already had like the infrastructure in place It's slightly more challenging because it's unstructured data. Like if it's a claim you know what fields to mask in order to make it not like THI because it's structured data structured data Whereas if it's text. How exactly to redact the text to meet the requirements of Privacy, that is, I think, a very complex topic and we definitely ran into challenges there Fascinating. Well, hopefully we get to discuss that a bit more with Hamill. As well. But in the limit where you're able to log things, which is a lot of the case. You log things on the user side, including tool calls, function calls, if any. And that's with the LM as well.", "metadata": {"token_count": 1043, "source": "sentence_chunking", "position": 11, "original_chunk_id": "32750c97-ce60-4ae4-a753-dd7c768ef6d9", "speaker": "Unknown", "workshop_id": "WS3", "timestamp": "Chunk 12"}, "id": "WS3_32750c97-ce60-4ae4-a753-dd7c768ef6d9", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 14", "source": "sentence_chunking", "speaker": "Unknown", "token_count": 1021, "original_chunk_id": "a75cb503-824d-4264-8ba3-c9b897398367", "position": 13}, "id": "WS3_a75cb503-824d-4264-8ba3-c9b897398367", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"position": 13, "speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS3", "token_count": 1021, "original_chunk_id": "6a43538e-cd1b-4b39-a447-314078bf7d81", "timestamp": "Chunk 14"}, "id": "WS3_6a43538e-cd1b-4b39-a447-314078bf7d81", "relevance": 1.0}]}}{"question": "What are the best practices for evaluating the performance of LLM applications in a production environment?", "response": "To effectively evaluate the performance of LLM applications in a production environment, consider the following best practices:\n\n1. **Establish Clear Evaluation Criteria**: Define what constitutes a successful output based on your specific use case. This could include accuracy, relevance, completeness, and adherence to style and tone.\n\n2. **Start Small**: Begin with human evaluations to establish baselines. Use a simple plus-one/minus-one scoring system to ensure consistency among evaluators.\n\n3. **Monitor and Log Performance**: Implement robust logging and monitoring to capture data in a human-readable format. This helps in tracing issues and understanding user interactions.\n\n4. **Iterate and Calibrate**: Regularly check your system against human judgments and update your evaluation criteria as needed. This ensures that your evaluation remains relevant and effective.\n\n5. **Track Metrics Over Time**: Monitor for drift, biases, and performance changes. Use metrics like accuracy and user satisfaction to gauge effectiveness.\n\n6. **Incorporate Feedback Loops**: Use feedback from domain experts and users to refine your models and evaluation processes continuously.\n\n7. **Document Changes**: Keep a record of any adjustments made to the evaluation criteria or system to maintain transparency and facilitate future improvements.\n\nBy following these practices, you can build trust in your LLM applications and ensure they meet the desired performance standards.", "context_info": {"num_chunks": 10, "context_tokens": 10331, "completion_tokens": 269, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS1", "WS4"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"workshop_id": "WS3", "speaker": "Unknown", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "timestamp": "Chunk 15", "token_count": 1068, "position": 14, "source": "sentence_chunking"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"position": 14, "timestamp": "Chunk 15", "source": "sentence_chunking", "workshop_id": "WS3", "speaker": "Unknown", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "token_count": 1068}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"timestamp": "Chunk 11", "source": "sentence_chunking", "token_count": 1062, "position": 10, "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "speaker": "Unknown", "workshop_id": "WS3"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"position": 10, "workshop_id": "WS3", "token_count": 1062, "speaker": "Unknown", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "timestamp": "Chunk 11", "source": "sentence_chunking"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "ef57fa87-2177-44f5-bab8-24ccb6a587f7", "timestamp": "Chunk 16", "speaker": "Unknown", "workshop_id": "WS3", "token_count": 1049, "position": 15}, "id": "WS3_ef57fa87-2177-44f5-bab8-24ccb6a587f7", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"speaker": "Unknown", "workshop_id": "WS3", "token_count": 1049, "original_chunk_id": "62ef5b55-d219-45ff-a564-75ead9264997", "position": 15, "timestamp": "Chunk 16", "source": "sentence_chunking"}, "id": "WS3_62ef5b55-d219-45ff-a564-75ead9264997", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"timestamp": "Chunk 11", "workshop_id": "WS1", "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "speaker": "Unknown", "token_count": 1050, "position": 10, "source": "sentence_chunking"}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"source": "sentence_chunking", "token_count": 1050, "original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "workshop_id": "WS1", "speaker": "Unknown", "position": 10, "timestamp": "Chunk 11"}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"source": "sentence_chunking", "token_count": 1012, "workshop_id": "WS4", "speaker": "Unknown", "original_chunk_id": "79e3ed2f-8447-44f2-a41f-7288f51c8da7", "timestamp": "Chunk 9", "position": 8}, "id": "WS4_79e3ed2f-8447-44f2-a41f-7288f51c8da7", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"position": 8, "timestamp": "Chunk 9", "workshop_id": "WS4", "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "d492c886-fee2-48a6-b289-95214ec3d247", "token_count": 1012}, "id": "WS4_d492c886-fee2-48a6-b289-95214ec3d247", "relevance": 1.0}]}}{"question": "What are the best practices for evaluating the performance of LLM applications, and how can I effectively apply these in my data analysis projects?", "response": "To effectively evaluate the performance of LLM applications, consider these best practices:\n\n1. **Define Clear Metrics**: Establish what \"good\" means for your specific use case. This could include correctness, completeness, relevance, and clarity. For instance, in summarization, ensure the output captures key points concisely without unnecessary length.\n\n2. **Use Domain Expertise**: Involve domain experts to assess the outputs. Their insights can help identify nuances that automated metrics might miss.\n\n3. **Log and Monitor**: Capture inputs and outputs systematically. This allows for tracking performance over time and identifying patterns or issues.\n\n4. **Iterate on Prompts**: Test and refine your prompts based on the outputs. Run the same prompt multiple times to observe variability and adjust accordingly.\n\n5. **Employ Evaluation Frameworks**: Utilize established metrics like ROUGE for summarization or BLEU for translation tasks. These can provide quantitative assessments of output quality.\n\n6. **Conduct User Feedback Sessions**: Gather feedback from potential users to understand the practical effectiveness of the outputs in real-world scenarios.\n\n7. **Analyze Failures**: Use pivot tables to categorize errors (e.g., hallucinations, incomplete responses) and prioritize addressing high-impact issues first.\n\nBy applying these practices, you can enhance the evaluation process in your data analysis projects, ensuring that your LLM applications meet user needs effectively.", "context_info": {"num_chunks": 10, "context_tokens": 10351, "completion_tokens": 277, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS1"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "token_count": 1068, "timestamp": "Chunk 15", "position": 14, "workshop_id": "WS3", "source": "sentence_chunking", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "timestamp": "Chunk 15", "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS3", "position": 14, "token_count": 1068}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"source": "sentence_chunking", "position": 10, "speaker": "Unknown", "token_count": 1062, "timestamp": "Chunk 11", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "workshop_id": "WS3"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"timestamp": "Chunk 11", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "position": 10, "speaker": "Unknown", "source": "sentence_chunking", "token_count": 1062, "workshop_id": "WS3"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"position": 4, "timestamp": "Chunk 5", "workshop_id": "WS3", "original_chunk_id": "f4c88392-cba7-4006-910b-c8336207f0c1", "source": "sentence_chunking", "token_count": 1063, "speaker": "Unknown"}, "id": "WS3_f4c88392-cba7-4006-910b-c8336207f0c1", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"token_count": 1063, "position": 4, "original_chunk_id": "38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "workshop_id": "WS3", "source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 5"}, "id": "WS3_38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "position": 13, "workshop_id": "WS3", "token_count": 1021, "original_chunk_id": "a75cb503-824d-4264-8ba3-c9b897398367", "timestamp": "Chunk 14"}, "id": "WS3_a75cb503-824d-4264-8ba3-c9b897398367", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"position": 13, "workshop_id": "WS3", "speaker": "Unknown", "original_chunk_id": "6a43538e-cd1b-4b39-a447-314078bf7d81", "token_count": 1021, "timestamp": "Chunk 14", "source": "sentence_chunking"}, "id": "WS3_6a43538e-cd1b-4b39-a447-314078bf7d81", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "timestamp": "Chunk 11", "position": 10, "workshop_id": "WS1", "speaker": "Unknown", "token_count": 1050, "source": "sentence_chunking"}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"workshop_id": "WS1", "source": "sentence_chunking", "original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "position": 10, "token_count": 1050, "timestamp": "Chunk 11", "speaker": "Unknown"}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}]}}{"question": "What are the best practices for evaluating the performance of LLM applications, and how can I effectively interpret the evaluation metrics?", "response": "To effectively evaluate the performance of LLM applications, follow these best practices:\n\n1. **Define Clear Evaluation Criteria**: Establish what \"good\" means for your specific use case. This could include correctness, completeness, conciseness, and relevance of the output.\n\n2. **Use Hand-Labeled Data**: Start with a small, hand-labeled dataset to assess model performance. This allows for a clear understanding of errors, such as hallucinations or incomplete responses.\n\n3. **Quantify Performance**: Utilize metrics like ROUGE for summarization tasks or BLEU for translation. These metrics help quantify how well the model's output aligns with expected results.\n\n4. **Iterate and Refine**: Continuously refine your prompts and evaluation criteria based on feedback and performance metrics. This iterative process helps improve model accuracy and user satisfaction.\n\n5. **Incorporate Domain Expertise**: Engage domain experts to evaluate outputs, ensuring that the evaluation aligns with real-world expectations and requirements.\n\n6. **Analyze Errors**: Conduct error analysis to identify common issues and prioritize fixes based on their impact on overall performance.\n\nBy interpreting these metrics in the context of your defined criteria, you can make informed decisions about model adjustments and improvements. Engage with your user base for feedback to further refine your evaluation process.", "context_info": {"num_chunks": 10, "context_tokens": 10345, "completion_tokens": 259, "embedding_tokens": 15360, "workshops_used": ["WS3"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "workshop_id": "WS3", "timestamp": "Chunk 15", "token_count": 1068, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "source": "sentence_chunking", "position": 14}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 15", "workshop_id": "WS3", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "token_count": 1068, "position": 14, "source": "sentence_chunking"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"token_count": 1062, "source": "sentence_chunking", "workshop_id": "WS3", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "speaker": "Unknown", "timestamp": "Chunk 11", "position": 10}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"position": 10, "workshop_id": "WS3", "speaker": "Unknown", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "timestamp": "Chunk 11", "token_count": 1062, "source": "sentence_chunking"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"original_chunk_id": "a75cb503-824d-4264-8ba3-c9b897398367", "source": "sentence_chunking", "position": 13, "timestamp": "Chunk 14", "token_count": 1021, "speaker": "Unknown", "workshop_id": "WS3"}, "id": "WS3_a75cb503-824d-4264-8ba3-c9b897398367", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"speaker": "Unknown", "workshop_id": "WS3", "timestamp": "Chunk 14", "source": "sentence_chunking", "position": 13, "token_count": 1021, "original_chunk_id": "6a43538e-cd1b-4b39-a447-314078bf7d81"}, "id": "WS3_6a43538e-cd1b-4b39-a447-314078bf7d81", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"token_count": 1063, "original_chunk_id": "38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "timestamp": "Chunk 5", "position": 4, "source": "sentence_chunking", "workshop_id": "WS3", "speaker": "Unknown"}, "id": "WS3_38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "timestamp": "Chunk 5", "workshop_id": "WS3", "position": 4, "token_count": 1063, "original_chunk_id": "f4c88392-cba7-4006-910b-c8336207f0c1"}, "id": "WS3_f4c88392-cba7-4006-910b-c8336207f0c1", "relevance": 1.0}, {"text": "Okay, let me see. Let me think of this. Yeah, like more difficult evals would be like It's not about difficulty in the sense like Is difficulty like more than like you're not sure how to fix it like okay so like right off the bat it's not like some obvious engineering error that you're like, oh, like, I definitely know why that's happening and like it's so stupid. Like I should fix it. So, and a lot of these are like tend to be more like LLM as a judge based, but they don't have to be so like you know one so let me just give you some examples like Hey, like the user wants to go to the next step But then the model then the you know the lm is like repeating parodying back to the user unnecessarily, would you like to okay so would you like to go to the next step. Or, you know, like things from the system prompt are being repeated into the output that aren't supposed to like user ids Things like that. And you're like, well, why is the, you know, I wish the model wasn't like putting that into the answer. It shouldn't be. You know the latter one, you could test with code. You don't need LM as a judge. So it's not necessarily about either LM as a judge is a code base But it's like, hey, the model is doing something. It's not really a syntax or engineering error. And I'm not 100% sure how would just like get rid of it right off the bat. I need to do some experiments. Then that's a good sign that it might be an eval like that you might want to have an eval so you can iterate on it. Awesome. Natalia has an interesting question. I fundamentally disagree with her premise. She says evaluation is easy. And I just don't agree with that. But she asked, how do you make it the LLM better. I'll generalize the question slightly and say, how do you make your AI system. Better what type of, and we've talked about this briefly but And I do think you've said, let the error analysis guide you, essentially. But do you have any more Firstly, do you think evals are easy. No, it's not about, okay. It's really funny that this is a great comment. How do you make it better. What is better. Can you describe what is better. The struggle is Can you\u2026 articulate what is better. The process of you articulating what is better is the main focus. Of AI engineering. And like the uh it's pretty clear that humans aren't great especially like for non-trivial products describing exactly what they want. Without seeing LLM output and responding to it. And that helps external sort of what sort of what what you want in the process of going through evals is just kind of a\u2026 a way that we describe it, but really it's like There's evals, but there's looking at data. And it's like you have to give the LLM your requirements somehow and you know that like knowing what you want is hard. Discovering what you want takes work. Discovering what your users want That takes work. So evals in that is wrapped up so tightly together I just don't have a better word. Than evals. It's not exactly eval driven development. It's just development. I just, you know. Don't have the terminology, maybe someone will coin something or coin something one day i'm not in the business of coining terminology but sort of drive home it's like Yeah, it's the process of looking at data and sort of refining You know, your sort of Require like your vision of what the AI should do. And making it happen. You know, that's the hard part. And so, you know, you can't really make the LLM better if you don't give it, if you don't have a precise definition of what that is. And, you know, evaluation is like okay So that's like the top level. Now, if you go a little bit beneath that into like less you know more tactical You do need metrics that you trust. To iterate and make it better. Otherwise, you're just guessing. And the more complex your system becomes like you know you might change a prompt to fix one thing, but you'll break something else. And without evals like you won't know. And so it's that constant process of you need metrics, but the metrics have to be aligned very closely with what you want. With what like what your users want. And it's really, it's impossible to do that by going through this process. Does that help. Is anyone not commenced. I want to hear if someone doesn't feel like that is true. That helps a lot. And I do want to say Oh, great. Like, because I want to show you something. I want\u2026 Show us, please. Because I can show you something that can help you convince you. Yeah, I was about to say no, just so it would show us it. All right, have fun.", "metadata": {"position": 21, "source": "sentence_chunking", "original_chunk_id": "03aeddbc-2bc8-4630-885c-36dc7244ef2d", "timestamp": "Chunk 22", "token_count": 1056, "speaker": "Unknown", "workshop_id": "WS3"}, "id": "WS3_03aeddbc-2bc8-4630-885c-36dc7244ef2d", "relevance": 1.0}, {"text": "Okay, let me see. Let me think of this. Yeah, like more difficult evals would be like It's not about difficulty in the sense like Is difficulty like more than like you're not sure how to fix it like okay so like right off the bat it's not like some obvious engineering error that you're like, oh, like, I definitely know why that's happening and like it's so stupid. Like I should fix it. So, and a lot of these are like tend to be more like LLM as a judge based, but they don't have to be so like you know one so let me just give you some examples like Hey, like the user wants to go to the next step But then the model then the you know the lm is like repeating parodying back to the user unnecessarily, would you like to okay so would you like to go to the next step. Or, you know, like things from the system prompt are being repeated into the output that aren't supposed to like user ids Things like that. And you're like, well, why is the, you know, I wish the model wasn't like putting that into the answer. It shouldn't be. You know the latter one, you could test with code. You don't need LM as a judge. So it's not necessarily about either LM as a judge is a code base But it's like, hey, the model is doing something. It's not really a syntax or engineering error. And I'm not 100% sure how would just like get rid of it right off the bat. I need to do some experiments. Then that's a good sign that it might be an eval like that you might want to have an eval so you can iterate on it. Awesome. Natalia has an interesting question. I fundamentally disagree with her premise. She says evaluation is easy. And I just don't agree with that. But she asked, how do you make it the LLM better. I'll generalize the question slightly and say, how do you make your AI system. Better what type of, and we've talked about this briefly but And I do think you've said, let the error analysis guide you, essentially. But do you have any more Firstly, do you think evals are easy. No, it's not about, okay. It's really funny that this is a great comment. How do you make it better. What is better. Can you describe what is better. The struggle is Can you\u2026 articulate what is better. The process of you articulating what is better is the main focus. Of AI engineering. And like the uh it's pretty clear that humans aren't great especially like for non-trivial products describing exactly what they want. Without seeing LLM output and responding to it. And that helps external sort of what sort of what what you want in the process of going through evals is just kind of a\u2026 a way that we describe it, but really it's like There's evals, but there's looking at data. And it's like you have to give the LLM your requirements somehow and you know that like knowing what you want is hard. Discovering what you want takes work. Discovering what your users want That takes work. So evals in that is wrapped up so tightly together I just don't have a better word. Than evals. It's not exactly eval driven development. It's just development. I just, you know. Don't have the terminology, maybe someone will coin something or coin something one day i'm not in the business of coining terminology but sort of drive home it's like Yeah, it's the process of looking at data and sort of refining You know, your sort of Require like your vision of what the AI should do. And making it happen. You know, that's the hard part. And so, you know, you can't really make the LLM better if you don't give it, if you don't have a precise definition of what that is. And, you know, evaluation is like okay So that's like the top level. Now, if you go a little bit beneath that into like less you know more tactical You do need metrics that you trust. To iterate and make it better. Otherwise, you're just guessing. And the more complex your system becomes like you know you might change a prompt to fix one thing, but you'll break something else. And without evals like you won't know. And so it's that constant process of you need metrics, but the metrics have to be aligned very closely with what you want. With what like what your users want. And it's really, it's impossible to do that by going through this process. Does that help. Is anyone not commenced. I want to hear if someone doesn't feel like that is true. That helps a lot. And I do want to say Oh, great. Like, because I want to show you something. I want\u2026 Show us, please. Because I can show you something that can help you convince you. Yeah, I was about to say no, just so it would show us it. All right, have fun.", "metadata": {"original_chunk_id": "163db890-00a9-47b5-8c19-8f33dcd9fb4b", "token_count": 1056, "source": "sentence_chunking", "workshop_id": "WS3", "speaker": "Unknown", "timestamp": "Chunk 22", "position": 21}, "id": "WS3_163db890-00a9-47b5-8c19-8f33dcd9fb4b", "relevance": 1.0}]}}{"question": "What are the best practices for implementing RAG (retrieval-augmented generation) in LLM applications to ensure optimal performance and accuracy?", "response": "To implement Retrieval-Augmented Generation (RAG) effectively in LLM applications, consider the following best practices:\n\n1. **Information Retrieval**: Focus on how to extract relevant information from your document corpus. Techniques like embedding-based retrieval, keyword-based searches, and hybrid approaches can enhance accuracy. Ensure that your retrieval system can efficiently chunk documents to maintain context.\n\n2. **Context Management**: When passing retrieved information to the LLM, use structured templates or prepend context directly to prompts. This helps maintain clarity and relevance, avoiding overwhelming the model with excessive data.\n\n3. **Evaluation Harness**: Develop a robust evaluation framework. Use metrics like ROUGE for summarization tasks and ensure that responses are factually correct and grounded in the source documents. Regularly analyze outputs to identify common errors, such as hallucinations or incomplete responses.\n\n4. **Latency Considerations**: Balance the trade-off between retrieval speed and the depth of information. For consumer applications, consider delaying retrieval until necessary to minimize latency.\n\n5. **Iterative Improvement**: Continuously refine your RAG system based on user feedback and performance metrics. Document your processes and decisions to clarify your approach and facilitate future iterations.\n\nBy following these practices, you can enhance the performance and accuracy of your LLM applications using RAG.", "context_info": {"num_chunks": 10, "context_tokens": 10279, "completion_tokens": 258, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS5", "WS4"], "chunks": [{"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"workshop_id": "WS5", "position": 6, "source": "sentence_chunking", "timestamp": "Chunk 7", "token_count": 1023, "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "speaker": "Unknown"}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"position": 6, "workshop_id": "WS5", "token_count": 1023, "timestamp": "Chunk 7", "original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246", "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"source": "sentence_chunking", "workshop_id": "WS3", "position": 10, "token_count": 1062, "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "timestamp": "Chunk 11", "speaker": "Unknown"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"token_count": 1062, "speaker": "Unknown", "position": 10, "workshop_id": "WS3", "source": "sentence_chunking", "timestamp": "Chunk 11", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"token_count": 1068, "source": "sentence_chunking", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "speaker": "Unknown", "workshop_id": "WS3", "position": 14, "timestamp": "Chunk 15"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"workshop_id": "WS3", "timestamp": "Chunk 15", "source": "sentence_chunking", "position": 14, "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "speaker": "Unknown", "token_count": 1068}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"token_count": 1036, "source": "sentence_chunking", "position": 22, "workshop_id": "WS4", "timestamp": "Chunk 23", "original_chunk_id": "4eedb1cc-f056-44e3-9b97-0d4db328d700", "speaker": "Unknown"}, "id": "WS4_4eedb1cc-f056-44e3-9b97-0d4db328d700", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"original_chunk_id": "cd32a46d-1bc2-42cc-bf15-ddc5bb384a17", "position": 22, "workshop_id": "WS4", "timestamp": "Chunk 23", "token_count": 1036, "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS4_cd32a46d-1bc2-42cc-bf15-ddc5bb384a17", "relevance": 1.0}, {"text": "And once you do this it'll become clear. Oh, if I improve my information retrieval, I'll get 5% lift. 75 hugo bowne-anderson: If I improve the way I'm ingesting Pdfs, I'll actually get 20% lift right. So it's, you know, once again, use all the multi agentic multiturn, infinite rag, you know 76 hugo bowne-anderson: one 77 hugo bowne-anderson: trillion token context windows that that you want. Just know if you're looking to improve your business metrics. This this process will get you a long way. And then in workshop 4. Stefan took us all through the 2 iteration loops of testing in dev and and production, and even introduced the concept how to implement crcd, and these types of things. And we even gave you a blueprint for doing that with Github pull requests. 78 hugo bowne-anderson: So I'm excited to see if any of you want to play around with. 79 hugo bowne-anderson: So today, we're going to focus on context. 80 hugo bowne-anderson: So the basic idea here. And this is something William Horton made made a point of, and IA hundred 10% agree with one of the 81 hugo bowne-anderson: one of the 82 hugo bowne-anderson: most impactful use cases for Llms that I've seen in businesses and in personal use is people being able to query internal documents. 83 hugo bowne-anderson: Right. And so thinking about. Then how do we build systems which allow people to do this robustly, quickly, efficiently, right. So how do we get external context into the Llm system. Right. And how do we make sure it's not like just drawing information from its weights as well. So that's really what we're here to talk about today. 84 hugo bowne-anderson: To put it another way, and we'll be going through this blog post in detail in the next workshop. This is from. 85 hugo bowne-anderson: Let me just share the link in discord. And this is a blog post that I you know I'm 86 hugo bowne-anderson: I really. I mentioned most workshops to be honest. 87 hugo bowne-anderson: I just saw Jeff's yeah, totally. 88 hugo bowne-anderson: So this is a blog post called building effective agents. And the basic idea here is that, you know, if we're going to talk about agents, we probably should define it, and we should probably think about 89 hugo bowne-anderson: how do we move from a single Llm. Call 90 hugo bowne-anderson: to something more agentic and 91 hugo bowne-anderson: Barry and Eric at anthropic formulated as augmented Llms. Where we start building different types of workflows that we can think about as being on the agentic continuum. But even before that, knowing that Llms 92 hugo bowne-anderson: you give it tokens, it gives tokens out okay. And then along the continuum to building agentic type things, then we integrate other types of tooling and properties into the system. So the 1st one to think about really is, is memory right. Well, it may not be the first, st but it's an important one, and we've discussed that, in brief. And that's why I had Eddie from Fondue to come and talk to you about simple ways to think about memory last week. 93 hugo bowne-anderson: The next one is 94 hugo bowne-anderson: tool calls, okay, and this is something we're going to be talking about later later this week. And, in fact, for anyone who wants to join the the workshop with ravine next next week, doing a lot of 95 hugo bowne-anderson: tool calls, using Olama and Gradio and Gemma. 3 are all all locally, which will be super fun. But you could imagine, for example. 96 hugo bowne-anderson: you want to get the weather today somewhere and the cutoff date of Vlm you're working with was who knows when. And so you can implement a basic tool call to a weather Api. Similarly, you'll notice that chat Gpt. Perhaps these days you'll ask it something. You won't tell it to search the web, but it will recognize that perhaps 97 hugo bowne-anderson: searching the web will help it. Now, who knows whether that's true or not. To be honest. But you'll see it will dynamically search the web without even being told to, and that's more along the agentic side. But these types of tool uses are incredibly important. And then there's retrieval.", "metadata": {"timestamp": "Chunk 5", "token_count": 1050, "source": "sentence_chunking", "workshop_id": "WS5", "position": 4, "original_chunk_id": "4d2d3e29-8883-4bcd-9c1a-fe7bbe9df289", "speaker": "Unknown"}, "id": "WS5_4d2d3e29-8883-4bcd-9c1a-fe7bbe9df289", "relevance": 1.0}, {"text": "And once you do this it'll become clear. Oh, if I improve my information retrieval, I'll get 5% lift. 75 hugo bowne-anderson: If I improve the way I'm ingesting Pdfs, I'll actually get 20% lift right. So it's, you know, once again, use all the multi agentic multiturn, infinite rag, you know 76 hugo bowne-anderson: one 77 hugo bowne-anderson: trillion token context windows that that you want. Just know if you're looking to improve your business metrics. This this process will get you a long way. And then in workshop 4. Stefan took us all through the 2 iteration loops of testing in dev and and production, and even introduced the concept how to implement crcd, and these types of things. And we even gave you a blueprint for doing that with Github pull requests. 78 hugo bowne-anderson: So I'm excited to see if any of you want to play around with. 79 hugo bowne-anderson: So today, we're going to focus on context. 80 hugo bowne-anderson: So the basic idea here. And this is something William Horton made made a point of, and IA hundred 10% agree with one of the 81 hugo bowne-anderson: one of the 82 hugo bowne-anderson: most impactful use cases for Llms that I've seen in businesses and in personal use is people being able to query internal documents. 83 hugo bowne-anderson: Right. And so thinking about. Then how do we build systems which allow people to do this robustly, quickly, efficiently, right. So how do we get external context into the Llm system. Right. And how do we make sure it's not like just drawing information from its weights as well. So that's really what we're here to talk about today. 84 hugo bowne-anderson: To put it another way, and we'll be going through this blog post in detail in the next workshop. This is from. 85 hugo bowne-anderson: Let me just share the link in discord. And this is a blog post that I you know I'm 86 hugo bowne-anderson: I really. I mentioned most workshops to be honest. 87 hugo bowne-anderson: I just saw Jeff's yeah, totally. 88 hugo bowne-anderson: So this is a blog post called building effective agents. And the basic idea here is that, you know, if we're going to talk about agents, we probably should define it, and we should probably think about 89 hugo bowne-anderson: how do we move from a single Llm. Call 90 hugo bowne-anderson: to something more agentic and 91 hugo bowne-anderson: Barry and Eric at anthropic formulated as augmented Llms. Where we start building different types of workflows that we can think about as being on the agentic continuum. But even before that, knowing that Llms 92 hugo bowne-anderson: you give it tokens, it gives tokens out okay. And then along the continuum to building agentic type things, then we integrate other types of tooling and properties into the system. So the 1st one to think about really is, is memory right. Well, it may not be the first, st but it's an important one, and we've discussed that, in brief. And that's why I had Eddie from Fondue to come and talk to you about simple ways to think about memory last week. 93 hugo bowne-anderson: The next one is 94 hugo bowne-anderson: tool calls, okay, and this is something we're going to be talking about later later this week. And, in fact, for anyone who wants to join the the workshop with ravine next next week, doing a lot of 95 hugo bowne-anderson: tool calls, using Olama and Gradio and Gemma. 3 are all all locally, which will be super fun. But you could imagine, for example. 96 hugo bowne-anderson: you want to get the weather today somewhere and the cutoff date of Vlm you're working with was who knows when. And so you can implement a basic tool call to a weather Api. Similarly, you'll notice that chat Gpt. Perhaps these days you'll ask it something. You won't tell it to search the web, but it will recognize that perhaps 97 hugo bowne-anderson: searching the web will help it. Now, who knows whether that's true or not. To be honest. But you'll see it will dynamically search the web without even being told to, and that's more along the agentic side. But these types of tool uses are incredibly important. And then there's retrieval.", "metadata": {"timestamp": "Chunk 5", "original_chunk_id": "bf36823a-27cc-4f5e-a105-6e9d28292d3e", "position": 4, "workshop_id": "WS5", "speaker": "Unknown", "token_count": 1050, "source": "sentence_chunking"}, "id": "WS5_bf36823a-27cc-4f5e-a105-6e9d28292d3e", "relevance": 1.0}]}}{"question": "What are the best practices for implementing retrieval-augmented generation (RAG) in an LLM application, and how can I evaluate its performance effectively?", "response": "To implement retrieval-augmented generation (RAG) effectively in your LLM application, consider the following best practices:\n\n1. **Information Retrieval Techniques**: Utilize various retrieval methods such as keyword-based retrieval, exact matching, and embedding-based retrieval. Hybrid approaches can also enhance performance by combining different techniques.\n\n2. **Context Management**: Efficiently manage the context window by chunking documents into meaningful parts. This ensures that the information fed into the LLM is relevant and concise, preventing confusion.\n\n3. **Structured Input**: Format the retrieved context clearly, using structured templates or system messages to guide the LLM's behavior.\n\n4. **Evaluation Harness**: Build a minimal viable evaluation harness that includes good and bad examples with clear evaluation criteria. Focus on correctness, completeness, and relevance of responses.\n\n5. **Iterative Feedback**: Regularly analyze outputs using pivot tables to identify common errors (e.g., retrieval errors, incomplete responses) and prioritize fixing high-impact issues.\n\n6. **User Feedback**: Engage potential users for feedback to refine the system and ensure it meets real-world needs.\n\nFor performance evaluation, consider metrics like ROUGE for summarization tasks and ensure outputs are grounded in the source documents. Regularly iterate based on user interactions and feedback to enhance the system's effectiveness.", "context_info": {"num_chunks": 10, "context_tokens": 10272, "completion_tokens": 261, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS5", "WS4"], "chunks": [{"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"position": 6, "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "speaker": "Unknown", "workshop_id": "WS5", "token_count": 1023, "source": "sentence_chunking", "timestamp": "Chunk 7"}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"workshop_id": "WS5", "position": 6, "timestamp": "Chunk 7", "speaker": "Unknown", "token_count": 1023, "source": "sentence_chunking", "original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246"}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "workshop_id": "WS3", "token_count": 1062, "source": "sentence_chunking", "position": 10, "speaker": "Unknown", "timestamp": "Chunk 11"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"speaker": "Unknown", "source": "sentence_chunking", "token_count": 1062, "position": 10, "workshop_id": "WS3", "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "timestamp": "Chunk 11"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"timestamp": "Chunk 15", "speaker": "Unknown", "token_count": 1068, "position": 14, "workshop_id": "WS3", "source": "sentence_chunking", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"timestamp": "Chunk 15", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "speaker": "Unknown", "token_count": 1068, "position": 14, "workshop_id": "WS3", "source": "sentence_chunking"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"timestamp": "Chunk 14", "original_chunk_id": "a75cb503-824d-4264-8ba3-c9b897398367", "workshop_id": "WS3", "source": "sentence_chunking", "speaker": "Unknown", "position": 13, "token_count": 1021}, "id": "WS3_a75cb503-824d-4264-8ba3-c9b897398367", "relevance": 1.0}, {"text": "And maybe I could go in and hand label the empty ones, but in all honesty I want feedback from potential users and As participants in this course, this is incredible that you are like the users I want, right. Let me just sort short sort A to Z. So I had 47 and you can actually see I only had seven or eight fail. And one of them was me messing it up. I said fail, but then my note was correct. Not like some were empty, but have a quick look. We've got from row nine to\u2026 to row 38. We got like 38 passes, which I'm not\u2026 I'm kind of happy with, to be honest, as like a first first approximation. Now, what I'm going to do And of course, what you can do is you can, how do I filter White. Okay, I'm not going to waste any time on this, but you can filter to just look at the fails, do your pivot tables, all of that type of stuff. But what I'm going to do, I think, is set up a new repo with the transcripts and perhaps like for those who are doing the projects and have extra time after that or want to do it in parallel We can play around with building out this kind of workshop transcript query as well, which could be super fun, actually. Side note, anyone who wants to and this is This is almost an absurd thing to suggest right now. But, you know, we have this website that I built using Notion that's like a course wiki Where I'm like, we did this thing on prompt engineering. And then I put a link in workshop two to Let me actually bring it up. So you'll see in workshop two We've got these videos, we've got all our slides, et cetera. And then I say for extra credit, check out Sander Shulhoff's talk from cohort One on Prompt Engineering, the SDLC. Really cool talk. The reason I'm I'm speaking about this now is this is kind of embedded in this page, but I am interested. And if anyone's interested in building something like this with me or all of us, like. Different ways to interact with course material. I mean, one potential future music end game could be like Minority Report style like VR stuff where you're like bringing up videos and courses and transcripts and that type of stuff. And that may sound ridiculous, but it isn't actually as ridiculous as it would have sounded to me a year ago. So you could imagine we end up building like a rag system where It says, check out this clip from this workshop and instead of answering it, it gives you the clip of me and Nathan talking or, you know, someone asking a question and Hamill responding or that type of stuff so future music about potential product development, but these could be fun ways to interact with course content So\u2026 Oh, please. So I just want to, sorry to interrupt, but I want to call attention toward someone, I think Jeffrey made an excellent meme about the secret about Stefan and how our ragbot is not keeping secrets in the course so I just wanted to call that up. Might be a great eval if somebody wanted to Figure out a way to figure out a way Make sure that our rag bot is not spilling secrets out to whoever's hosting it. Just calling it out. Sorry for interrupting Hugo. We've got a very active Discord in workshop three three Yeah. Yeah. That's a great point. And that's another question i mean um So the workshop transcript is actually in a public GitHub repository. So it isn't only the rag bot, it's actually the data is there. Well, that's what I need to think through. So look. I'm trying to figure out, I've got these two notebooks That I want to go through. And we have around 15 minutes. I think in the interest of time. I'm going to go through one and then I'll record myself going through another to to share later. And this one doesn't involve any coding per se. The evaluating factual consistency in summarization, one does. And it also does some\u2026 We do some LLM as a judge stuff there as well so I will record a video of doing that one later because I think\u2026 We've gone into a lot of process stuff now, but I just want to step back And talk about how we think about evaluation and an iteration. And the goals are really to understanding approaches to evaluating LLMs, because we've talked about the process if you know that you want correctness and conciseness and that type of stuff. But how do we even think about these things. So firstly. We've talked about this before. How do you evaluate a system, right. You've got micro LLM calls is the information it extracts from LinkedIn correct. Do they e-mails. For example, the ones sent, but then the macro system calls. Do we actually hire good candidates.", "metadata": {"original_chunk_id": "6a43538e-cd1b-4b39-a447-314078bf7d81", "workshop_id": "WS3", "source": "sentence_chunking", "token_count": 1021, "speaker": "Unknown", "timestamp": "Chunk 14", "position": 13}, "id": "WS3_6a43538e-cd1b-4b39-a447-314078bf7d81", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"token_count": 1036, "source": "sentence_chunking", "original_chunk_id": "4eedb1cc-f056-44e3-9b97-0d4db328d700", "timestamp": "Chunk 23", "speaker": "Unknown", "workshop_id": "WS4", "position": 22}, "id": "WS4_4eedb1cc-f056-44e3-9b97-0d4db328d700", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"position": 22, "token_count": 1036, "workshop_id": "WS4", "speaker": "Unknown", "timestamp": "Chunk 23", "source": "sentence_chunking", "original_chunk_id": "cd32a46d-1bc2-42cc-bf15-ddc5bb384a17"}, "id": "WS4_cd32a46d-1bc2-42cc-bf15-ddc5bb384a17", "relevance": 1.0}]}}{"question": "What are the best practices for integrating synthetic data into the training pipeline of LLM applications to enhance performance and reliability?", "response": "To effectively integrate synthetic data into the training pipeline of LLM applications, follow these best practices:\n\n1. **Define User Personas**: Identify and create detailed personas representing your target users. This helps tailor synthetic queries to reflect realistic user interactions.\n\n2. **Generate Synthetic Queries**: Develop synthetic user queries based on the defined personas. This allows for a diverse range of inputs that can simulate real-world usage.\n\n3. **Label Responses**: Manually label the outputs of the LLM for these synthetic queries. This creates a hand-labeled dataset that serves as a foundation for evaluation.\n\n4. **Build an Evaluation Harness**: Use the labeled data to construct a basic evaluation system. This can include test sets, LLM as a judge, and techniques like fuzzy matching or regex for structured outputs.\n\n5. **Iterate and Improve**: Continuously refine your synthetic data generation and evaluation processes based on performance metrics. Monitor for drift between synthetic and real user queries post-launch.\n\n6. **Automate Evaluation**: Implement CI/CD practices to automate the evaluation harness, ensuring consistent performance tracking and improvement across model iterations.\n\nBy following these steps, you can create a robust training pipeline that leverages synthetic data to enhance the performance and reliability of your LLM applications.", "context_info": {"num_chunks": 10, "context_tokens": 10394, "completion_tokens": 253, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS5", "WS4"], "chunks": [{"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 8", "workshop_id": "WS3", "original_chunk_id": "97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "source": "sentence_chunking", "position": 7, "token_count": 1061}, "id": "WS3_97f83e5d-9e20-4a3a-aaf7-a9d0d8b382fd", "relevance": 1.0}, {"text": "Give me a one-page product doc. Who would have thought that product thinking could be helpful when shipping product. Because on top of that, when you answer these, you can actually start a data flywheel and build your NVE before you launch. So let's see what that looks like. So what we do is we build an MVP like the system I shipped to you all and generate some synthetic user queries. Then you label responses by hand in a spreadsheet or in a JSON view or whatever it is. Then you use the label data to build a basic evaluation harness, okay. Such as, and we'll get into what this looks like. A test set plus LLM as a judge. Maybe you have some fuzzy matching or stringing matching or testing for structured output as well. Regex, some classic techniques. Then you use this MVE to evaluate and improve your MVP. Now let's step back for a second. I want to make very clear, this is not you. We've been doing it in ML for years. Where you have an MVP and you collect data. This collecting data used to be a lot more expensive right But now we can synthetically generate it. Now, of course, we want to make sure As soon as we launch at that. You want to kind of monitor the drift from real user questions from your own synthetically generated questions and evolve the product as it goes on. But in ML, you'd have an MVP collect data, label responses by hand. Use labeled data to build a basic evaluation system. And that's a test set, right. And then you use this NVE to evaluate and improve your MVP. So when you try a new model or try different hyperparameters, you have a test set where you can Well, it should actually be your validation set. Probably. But you have a holdout set which you use to evaluate the performance of any changes to your model So what's different and familiar about evaluating LLM apps in an ML workflow, you collect real world examples. Llm, you synthesize realistic queries from personas pre-launch. And then once you launch, you can get feedback from users, right. In ML workflow, you label outputs by hand. Ml. In LLM, you label outputs by hand. Once again, I'm getting you all. To do some labeling for me, actually, which I think is really important. I'm not merely outsourcing the work to you, but as the users. Who may find the product useful, who I'm building it for. You know bringing you in the loop there is important. And finally, I actually just recorded a podcast VP of data science at Meta, at Instagram. And we were talking about one of the, you know Product innovations early on and a lot of these Companies was getting humans, getting users to little bit of labeling themselves and using that to bootstrap product development, right. That data flywheel early on. The best example And\u2026 aspects of morality and ethics aside, which I think is a very important conversation when talking about these things facebook I didn't push them. The VP of data science that matter too hard about this though, sadly but um Facebook early on got us to label ourselves in photos, right. And you may have noticed after six to 12 months, it was very good at saying, is this you. And then it gets you to verify. And then afterwards it just, it doesn't even need that, right. So in an ML workflow, then you train the model and label data. In an element workflow, you prompt the model. And tune and prompt the system setup. Ml workflow, you evaluate on the held out data set LMAP, you evaluate using your MVE, which may be a test set, auto eval some So that's illness judged, some hand labeling yourself. And the reason I wrote AutoEval, and this is thanks to Nathan actually who pushed me hard to just really recognize How much of the auto eval stuff can be just is this JSON. Is this valid. The evaluation can be around latency. It isn't just LLM as a judge stuff. Do these strings match. Those types of things. And then in an ML workflow. You iterate based on performance. Element workflow. You iterate based on performance. You use the eval harness to guide improvement. So both workflows are grounded in Three things. Test sets. Hand labeled data, iteration and failure analysis. And that's what we'll be getting into now. So let's build. Go through the building of such a loop. So you build an MVP. Then you do some evaluations such as vibes, error analysis, build a harness Then you iterate, keep going. At the start for this valuation to get this synthetic data flywheel going You want to say who your user is. You want to talk about the scenario. How do they use your app. You want to create as realistic a test set as possible, okay. So why do we want to do this. And I've talked about this before. When we do eval by vibes, it's subjective. It's like, this seems better. Inconsistent, difficult to repeat reliably. No historical baseline. Hard to communicate results and impossible to maintain consistency.", "metadata": {"source": "sentence_chunking", "token_count": 1061, "workshop_id": "WS3", "original_chunk_id": "acbab402-9567-43c9-b2a9-caa05bdd8999", "speaker": "Unknown", "position": 7, "timestamp": "Chunk 8"}, "id": "WS3_acbab402-9567-43c9-b2a9-caa05bdd8999", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"workshop_id": "WS3", "position": 8, "original_chunk_id": "6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "source": "sentence_chunking", "token_count": 1067, "timestamp": "Chunk 9", "speaker": "Unknown"}, "id": "WS3_6b9e78cb-cf86-45a1-a843-8a3c423b0a63", "relevance": 1.0}, {"text": "No historical baseline. Hard to communicate results and impossible to maintain consistency. Maintain consistency whereas once you have a basic harness, you can have objective, measurable results. And you may look at it and go, wait, these aren't the things we quite want to measure. And then you iterate on your harness. You have repeatable structured evaluation processes. On top of that, as Stefan will show you later this week, you can implement these in CICD. So when you open like a new branch. With a challenger system, a challenger app, a challenger model, you can then get the evaluation harness running automatically on that, right. So it can all be automated. You can clearly track improvements. Document results, easy to share and analyze, and it's standardized and reproducible across team members. And as I always say. Your most important team member. Is you on a Monday morning after you done a bunch of really dumb things on a Friday afternoon to get to the pub quickly. Clearly, I've moved back to Australia because we're going to the pub. After work or at lunchtime, aren't we, Jeff. No, half kidding. Um so The core loop really involves. Tracking things that are just bloody important to you right like cost and accuracy and latency time and money, people. Time and money and then user and business metrics, tying it to your business metrics so We have an AI system response. How do we compare it. To see whether we like it. You want ground truth. You want realistic samples. That have been hand labelled. And then you can use an automated system where you have an LLM as a judge, for example. Or you're also seeing, are they structured outputs You're doing string fuzzy matching. So if you know that you So actually. Great example, right. And I think this is one that Nathan\u2026 Reminded me of. For the example Who are the instructors in this course. Do I want to create an LLM as a judge to help me with that. Or do I want to make sure that it's a sentence that makes sense that contains the strings Hudo van Anderson and Stefan Krawchik. Okay, so maybe you want to use some generative stuff for that. But the really important thing is actually contained in string matching in that particular case, okay. So, and once again what i'm doing mentally there. And in terms of this process i Like it isn't highfalutin tech like it's really like just first principle thinking right So before I get into this, I do want to show you In the repository, and I'm sorry for jumping around. I appreciate that it can be um give a bit of whiplash. You can say, I have this directory called synthetic data EDD and all the files I'm showing you are in here. So let me, I think it's under data. I have a personas.json, right. And you can see I have the student persona, I have the data scientist persona, I have the ML engineer persona, okay. So that's all to say that all carbon generated code chunks and YAML stuff I show you here is in the repository. So the first thing I did was I was like, who do I want to use the app. Okay. For example, my mother is a semi-retired lawyer, okay. Now, I don't want the app to really be helpful to her. Okay, so I don't necessarily want non-technical people. I don't think it would be useful at the moment to have Is that developed for non-technical people. So I want to narrow down. I have a data scientist. I have a description and some of their goals here. And you can do this kind of at any level of granularity. I've done it at a medium level. You could just say a data scientist and describe them. I also have an ML engineer and they have slightly different goals, right. And different technical levels in, I mean. Far out i almost as someone who's more scientist and less engineer, I balk at the fact that I chose technical level intermediate, but really I think what I was speaking to was like more infrastructural, like software engineering, technical stuff there. Okay, so you define a persona And then I'm going to define some scenarios, right. So cohort students this is cohort a student in the second cohort trying to evaluate their own implementations and understand what makes a good response, right. Then we have different types of questions. We have general questions, we have technical questions, we have factual questions. Now, you don't need to do this to start. The reason I did this is\u2026 How you often see this play out is that If you factor your failure modes by types of questions, you'll often see that it's pretty good at factual ones. It isn't good at technical ones yet, that type of stuff. So it can help you analyze your failure modes. Once again, have a play around with the types of scenarios you can create. I've done this in one way but there's no way you know golden rule here full transparency. I worked on this with Claude, okay. I got it to help me. Leverage the tools you have.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "token_count": 1067, "position": 8, "workshop_id": "WS3", "original_chunk_id": "9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "timestamp": "Chunk 9"}, "id": "WS3_9cf9b25e-e9c6-4e55-9b3d-b9a4d6172d39", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"token_count": 1063, "timestamp": "Chunk 5", "source": "sentence_chunking", "position": 4, "original_chunk_id": "f4c88392-cba7-4006-910b-c8336207f0c1", "workshop_id": "WS3", "speaker": "Unknown"}, "id": "WS3_f4c88392-cba7-4006-910b-c8336207f0c1", "relevance": 1.0}, {"text": "So now I've got some hand labeled data. I analyze correctness as a function of model. And when you're choosing your first model, once again. Like you could build like a sophisticated eval harness now, but I don't suggest you do it. I'd be like okay Llama threes like not doing it. I could try to diagnose that, but if Gemini and OpenAI are working well and their latency is okay. Why not, huh. Then looking at JSON validity, we see Gemini and OpenAI, 100% JSON validity as well. Then latency analysis. For some reason, Claude was wigging out on me. Don't know why. Don't even care right now because Gemini and OpenAI were working pretty well, right. And then you want to think about all of these metrics holistically At a first approximation. You know, looking at them all, JSON and, sorry, Gemini and JSON must be JSON. Gemini and OpenAI. We're doing pretty well on all metrics um so And I said that i said that Why did I choose Gemini. Oh, yeah. Firstly, Gemini offers a free tier with 15 requests per minute, making it more cost-effective option for experimentation. I also, I have this weird That's totally nonsensical. But I have this weird view of like Gemini being the underdog against OpenAI. And of course, clearly it's Google, right. So clearly I'm I'm being absolutely silly. But part of me kind of Once Gemini to win. I actually think their product development and LM development is far more exciting as well personally. They have hired a lot of my friends as well, though, so I want to say that could be the case also. But once we choose Gemini, which we chose for high correctness, consistent validity. Json, low latency, cost effective The next steps are to refine the prompt test at scale, document results. And that's actually what I then continue to do. I start refining the prompt And then iterate on it to capture detailed dates of start dates and end dates for people's roles and these types of things. And you do that on small subsets of the data set and then kind of move to the full data set. And that's really how to start thinking about evaluation. And that wasn't like a huge part of the homework, but it was to start thinking through these things. And then you can start reviewing the results. And once again, hand labeling, thinking about potential flip-floppy behavior. And that's really the next section of the notebook, which I spoke to and I would encourage you all to do. But it's really this point that you start to run the same prompt several times and you start to see The different types of replies it can give you. I'm not going to talk a lot about testing today. Because we're here really to talk about evaluation today. But what I will say in Stefan's workshop in 48 hours, he's going to dive deep into testing In development and production and give you a sense of how you can even use basic Frameworks like PyTest, right. In order to test the output of responses before serving them to users and maybe have a fallback plan if they don't meet your requirements. You could imagine that we actually have a test which says. If the name in the output doesn't satisfy certain characteristics, we're going to have to do it again. Or if it's too inconsistent, we then need to iterate on the prompt. To ensure more consistency in these types of things. So that's really the way I've been thinking about a lot of what we've been doing so far. And I think that leads nicely. A soft landing into thinking about evaluation. We've all seen now that you can get a vibe, you can think about certain characteristics, but let's say I do switch out my model. From GPT 4.0 to Gemini, whatever. How do I even know besides getting a vibe, whether it's good or not. Which brings us to evaluation. And I'm going to say three things about evaluation We'll show you three slides. You need to look at your data and results and quantify performance. You need to do this before using multi-agentic frameworks. You need to do this before trying to build multi-turn systems. You look at your data and results, quantify performance, which is a combination of domain expertise and binary classification. Once again, who would have thought getting a domain expert involved would be a good idea. And you're logging inputs and outputs in Devon prod. You're using domain specific expertise to evaluate outputs in both. You're starting to build systems. Such as unit tests, data sets and product feedback hooks to help you in this process. Now, when I say data set, you may say, what data set, Hugo. The hand labeled annotations I just showed you actually are a wonderful place to start. And for those who build machine learning products, you might say. That kind of looks like a test set, right. And I'd say, you're absolutely right. And modulo certain details A lot of this process is eerily similar to what building machine learning power products has been like for years and data powered products.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "speaker": "Unknown", "timestamp": "Chunk 5", "token_count": 1063, "workshop_id": "WS3", "position": 4}, "id": "WS3_38ad3440-7a66-40d4-86c8-bbec8e1ee7e8", "relevance": 1.0}, {"text": "And then within it, you'll see kind of these kind of boxes that represent spans and so There could be an overarching span that underneath is calling various different things. So you can then see the breakdown of the span as it kind of Who responds, what type of cuisine do you prefer would be, you know, this kind of represent one trace and then the next input would then represented by another trace, right. And so this is where, depending if it's agents or other things, you know, different call paths or code and logic can be taken. And so, uh. So therefore, I mentioned spans This is the other word you'll hear if you hear about tracing, it spans right and so In a LLM app or something like this will be different these bands will correspond to Extraction going for to an API for retrieval the vector database for retrieval coming back you know that the LLM call, how long did it take. And so spans, you can think of it, if you're familiar with logging, it's kind of like a structured log where There are IDs that help it capture the relationship between like, so what was my parents span so what created me But then also some sort of, you know, it does have a payload that you can kind of, that is fairly flexible. There's a lot of UIs here. There's a lot of vendors here. This is an example of UI, but if you think about for LLM side. So Datadog has now built a product here but This does come from distributed tracing, which is between distributed systems. The only difference LLM providers here in the space have done is they've just provided a different UI treatment. To uh two traces. And so very, very important pretty much all of them have this kind of interface. So if you can kind of see this kind of tree type structure of relationship between like spans and things and then When you cook on them, you can kind of see more information, and this is what was logged along with that span. Right. How do traces fit in right it is you know unless you can use them in development, right. So as you're running your app locally this is you can actually leverage this as a way to do data collection but then When it goes to production, you can also do the same thing. So this is the interesting difference between regular software engineering If you are familiar with distributed tracing and Datadog, you would only use that in production But here with Gen AI tools, you can actually use the tracing tools locally in development for yourself as well. And so\u2026 what do those tools look like. How do you integrate them. So I'm just picking Langfuse just because it's a popular open source project. The main thing that you'll kind of see is like there is, you know, you have to install some libraries You then potentially, if you're using their cloud service or you basically have to put in some sort of endpoint And then you're instrumenting so you're instrumenting Using that particular library Importing those modules and then say in this case here, decorating functions that then When this function runs, it is capturing certain aspects about it. And then this doesn't have to be LLM calls. It could be any internal function right and so That's just kind of one example. I think, you know, Arise Phoenix, just to give you another example, right, is like you install something, import things to set things up and then wherever you want to kind of track or do more stuff right you Basically using that SDK, so to speak, to kind of instrument to create traces. Yeah. So nothing too complex, but But I want to say in the last year and a half, there's like a lot of these tracing providers initially when they came out, they were like very they were proprietary right meaning that the data format they sent was Something that they came up. But OpenTelemetry, which is an open standard, we can see a lot of convergence on vendors using this. And so one of the things you want to avoid is vendor lock-in. And so OpenTelemetry allows you to kind of uncheck yourself from that. And so specifically. As I mentioned, spans are a bit like a structured log. Opentelemetry gives a very specific vendor agnostic standard that a lot of vendors buy into. That then allows you to specifies how things should be formatted and how things should be logged. They even have a standard that's kind of, you know, in terms of the standard, they have a standard around what attributes and what they're called to be logged. So this is where if you use OpenTelemetry and a provider that supports ingesting it. They then very easily can then, this is where you can attach metadata like the LLM token usage, the LLM, how many tokens were input, how many tokens were output What were the inputs. What was the type of model. And so they're providing a structured schema that then any downstream consumer, as long as they support it, can ingest it.", "metadata": {"token_count": 1029, "timestamp": "Chunk 11", "source": "sentence_chunking", "workshop_id": "WS4", "original_chunk_id": "70265733-6a58-4bff-a8db-edbb94471961", "position": 10, "speaker": "Unknown"}, "id": "WS4_70265733-6a58-4bff-a8db-edbb94471961", "relevance": 1.0}, {"text": "And then within it, you'll see kind of these kind of boxes that represent spans and so There could be an overarching span that underneath is calling various different things. So you can then see the breakdown of the span as it kind of Who responds, what type of cuisine do you prefer would be, you know, this kind of represent one trace and then the next input would then represented by another trace, right. And so this is where, depending if it's agents or other things, you know, different call paths or code and logic can be taken. And so, uh. So therefore, I mentioned spans This is the other word you'll hear if you hear about tracing, it spans right and so In a LLM app or something like this will be different these bands will correspond to Extraction going for to an API for retrieval the vector database for retrieval coming back you know that the LLM call, how long did it take. And so spans, you can think of it, if you're familiar with logging, it's kind of like a structured log where There are IDs that help it capture the relationship between like, so what was my parents span so what created me But then also some sort of, you know, it does have a payload that you can kind of, that is fairly flexible. There's a lot of UIs here. There's a lot of vendors here. This is an example of UI, but if you think about for LLM side. So Datadog has now built a product here but This does come from distributed tracing, which is between distributed systems. The only difference LLM providers here in the space have done is they've just provided a different UI treatment. To uh two traces. And so very, very important pretty much all of them have this kind of interface. So if you can kind of see this kind of tree type structure of relationship between like spans and things and then When you cook on them, you can kind of see more information, and this is what was logged along with that span. Right. How do traces fit in right it is you know unless you can use them in development, right. So as you're running your app locally this is you can actually leverage this as a way to do data collection but then When it goes to production, you can also do the same thing. So this is the interesting difference between regular software engineering If you are familiar with distributed tracing and Datadog, you would only use that in production But here with Gen AI tools, you can actually use the tracing tools locally in development for yourself as well. And so\u2026 what do those tools look like. How do you integrate them. So I'm just picking Langfuse just because it's a popular open source project. The main thing that you'll kind of see is like there is, you know, you have to install some libraries You then potentially, if you're using their cloud service or you basically have to put in some sort of endpoint And then you're instrumenting so you're instrumenting Using that particular library Importing those modules and then say in this case here, decorating functions that then When this function runs, it is capturing certain aspects about it. And then this doesn't have to be LLM calls. It could be any internal function right and so That's just kind of one example. I think, you know, Arise Phoenix, just to give you another example, right, is like you install something, import things to set things up and then wherever you want to kind of track or do more stuff right you Basically using that SDK, so to speak, to kind of instrument to create traces. Yeah. So nothing too complex, but But I want to say in the last year and a half, there's like a lot of these tracing providers initially when they came out, they were like very they were proprietary right meaning that the data format they sent was Something that they came up. But OpenTelemetry, which is an open standard, we can see a lot of convergence on vendors using this. And so one of the things you want to avoid is vendor lock-in. And so OpenTelemetry allows you to kind of uncheck yourself from that. And so specifically. As I mentioned, spans are a bit like a structured log. Opentelemetry gives a very specific vendor agnostic standard that a lot of vendors buy into. That then allows you to specifies how things should be formatted and how things should be logged. They even have a standard that's kind of, you know, in terms of the standard, they have a standard around what attributes and what they're called to be logged. So this is where if you use OpenTelemetry and a provider that supports ingesting it. They then very easily can then, this is where you can attach metadata like the LLM token usage, the LLM, how many tokens were input, how many tokens were output What were the inputs. What was the type of model. And so they're providing a structured schema that then any downstream consumer, as long as they support it, can ingest it.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "token_count": 1029, "workshop_id": "WS4", "original_chunk_id": "a6d99531-2ae0-4f8d-a810-cb420f7cc61e", "timestamp": "Chunk 11", "position": 10}, "id": "WS4_a6d99531-2ae0-4f8d-a810-cb420f7cc61e", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"speaker": "Unknown", "timestamp": "Chunk 7", "position": 6, "workshop_id": "WS5", "source": "sentence_chunking", "token_count": 1023, "original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246"}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"timestamp": "Chunk 7", "token_count": 1023, "position": 6, "workshop_id": "WS5", "source": "sentence_chunking", "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "speaker": "Unknown"}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}]}}{"question": "What are the key differences between RAG and traditional LLM approaches when building applications?", "response": "RAG (Retrieval-Augmented Generation) and traditional LLM (Large Language Model) approaches differ significantly in how they handle information retrieval and context integration.\n\n1. **Information Retrieval**: RAG combines retrieval mechanisms with LLMs, allowing the model to access external documents or databases to enhance its responses. Traditional LLMs rely solely on their pre-trained knowledge, which is static and limited to the data they were trained on.\n\n2. **Dynamic Context**: RAG enables the incorporation of up-to-date and domain-specific information, filling knowledge gaps that traditional LLMs cannot address. This is crucial for applications requiring real-time data or proprietary knowledge.\n\n3. **Accuracy and Grounding**: RAG improves accuracy by grounding responses in actual documents, reducing the risk of hallucinations. Traditional LLMs may generate plausible-sounding but incorrect information due to their reliance on fixed training data.\n\n4. **Complexity and Implementation**: RAG systems often require more complex setups, including embedding techniques and document chunking, while traditional LLMs can be simpler to implement but less flexible in terms of context.\n\nIn summary, RAG enhances LLM capabilities by integrating real-time information retrieval, improving accuracy, and allowing for more dynamic and relevant responses.", "context_info": {"num_chunks": 10, "context_tokens": 10175, "completion_tokens": 252, "embedding_tokens": 15360, "workshops_used": ["WS5", "WS4"], "chunks": [{"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"speaker": "Unknown", "workshop_id": "WS5", "token_count": 1023, "position": 6, "timestamp": "Chunk 7", "source": "sentence_chunking", "original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246"}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"workshop_id": "WS5", "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "token_count": 1023, "timestamp": "Chunk 7", "position": 6, "speaker": "Unknown", "source": "sentence_chunking"}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}, {"text": "But these types of tool uses are incredibly important. And then there's retrieval. 98 hugo bowne-anderson: which is what we're here to talk talk about today. But the idea is, if you have a corpus of documents, how do you. How do you give that information to your Llm. And how do you. How do you query it. So these are the 3, the 3 pillars of thinking about building agents and thinking about 99 hugo bowne-anderson: like giving your Llm systems more capabilities. Essentially. We're at the early stages of working with this type of quote unquote software and figuring out how to give it a bit more that it can do to help us. In the work we want to do. Okay. So 100 hugo bowne-anderson: firstly, why do we even need to add context. Okay, so I think that a lot of this should be obvious. But we want to fill knowledge gaps. 101 hugo bowne-anderson: Llms are trained on fixed data sets. Can't quote unquote, know everything we can give it external information for domain, specific proprietary, time-sensitive knowledge. 102 hugo bowne-anderson: it can help ground and increase accuracy. So we've seen systems. And this is something in llama indexes favor. And for you, all of you who played around with the app I've built. I've seen some of you have asked the question that you knew wasn't in the Transcript, and it replied, this isn't in the Transcript, whatever, and you gave it a pass. You're like that's good. It didn't hallucinate something there, right. So grounding in the actual documents can work to constrain the models outputs now still know we do this through prompting, like 103 hugo bowne-anderson: we literally in the prompt say, hey. If you can't find it, please do not. Pretty. Please do not hallucinate my Llm, you know. So, and that's also the page it like we've got people at Deepmind who are like, just put it in the prompt right. So. And I don't mean to be flippant about that. But it 104 hugo bowne-anderson: it's a pretty interesting way to build software as far as I'm concerned, and it it works better than other ways. But it is a pretty wild thing to be doing. We get task specific relevance. 105 hugo bowne-anderson: We can get dynamic and up to date information. 106 hugo bowne-anderson: and this is actually one of my favorite uses. It can enable explainability. And I know William and I discussed this in in January, and there's some nuance to this, and I'm not actually only talking about rag here. I'll share these links. Actually let me. I've got them all 107 hugo bowne-anderson: or 2 of them here. 108 hugo bowne-anderson: And I'm just gonna oh, 3 of them. I'll paste them in discord. 109 hugo bowne-anderson: I love this title. It's classic. Tim O'reilly. The r and Rag stands for stands for royalties. And what O'reilly has done is they've used rag systems 110 hugo bowne-anderson: on their books. Sorry they've used the Reg system on their catalog of books so that I can ask a question when I'm in their platform, and it returns 111 hugo bowne-anderson: sources from particular books, and then I can click through to them, and if the source is helpful for me, then 112 hugo bowne-anderson: the rag system allows O'reilly to pay royalties to that that author. And I think this is an incredible use of this technology. To be perfectly honest, I think, William, and and correct me if I'm wrong, William. But William made a point that this doesn't mean it has to be rag right. There are other information retrieval systems that we can. 113 hugo bowne-anderson: you know, used for attribution and that type of stuff. And I totally agree with that that critique 114 hugo bowne-anderson: But it is something which allows us to think more more robustly through attribution and in classic Tim O'reilly style. He's talked about it as one way to think about fixing AI's original sin. And of course we do live in a space which we find incredibly useful. 115 hugo bowne-anderson: but where the big labs have essentially sucked up the world's information without paying anything 116 hugo bowne-anderson: for it. Of course, this isn't new. I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this.", "metadata": {"original_chunk_id": "514ae663-d4d7-4116-820c-2566e481c591", "timestamp": "Chunk 6", "position": 5, "source": "sentence_chunking", "speaker": "Unknown", "token_count": 1035, "workshop_id": "WS5"}, "id": "WS5_514ae663-d4d7-4116-820c-2566e481c591", "relevance": 1.0}, {"text": "But these types of tool uses are incredibly important. And then there's retrieval. 98 hugo bowne-anderson: which is what we're here to talk talk about today. But the idea is, if you have a corpus of documents, how do you. How do you give that information to your Llm. And how do you. How do you query it. So these are the 3, the 3 pillars of thinking about building agents and thinking about 99 hugo bowne-anderson: like giving your Llm systems more capabilities. Essentially. We're at the early stages of working with this type of quote unquote software and figuring out how to give it a bit more that it can do to help us. In the work we want to do. Okay. So 100 hugo bowne-anderson: firstly, why do we even need to add context. Okay, so I think that a lot of this should be obvious. But we want to fill knowledge gaps. 101 hugo bowne-anderson: Llms are trained on fixed data sets. Can't quote unquote, know everything we can give it external information for domain, specific proprietary, time-sensitive knowledge. 102 hugo bowne-anderson: it can help ground and increase accuracy. So we've seen systems. And this is something in llama indexes favor. And for you, all of you who played around with the app I've built. I've seen some of you have asked the question that you knew wasn't in the Transcript, and it replied, this isn't in the Transcript, whatever, and you gave it a pass. You're like that's good. It didn't hallucinate something there, right. So grounding in the actual documents can work to constrain the models outputs now still know we do this through prompting, like 103 hugo bowne-anderson: we literally in the prompt say, hey. If you can't find it, please do not. Pretty. Please do not hallucinate my Llm, you know. So, and that's also the page it like we've got people at Deepmind who are like, just put it in the prompt right. So. And I don't mean to be flippant about that. But it 104 hugo bowne-anderson: it's a pretty interesting way to build software as far as I'm concerned, and it it works better than other ways. But it is a pretty wild thing to be doing. We get task specific relevance. 105 hugo bowne-anderson: We can get dynamic and up to date information. 106 hugo bowne-anderson: and this is actually one of my favorite uses. It can enable explainability. And I know William and I discussed this in in January, and there's some nuance to this, and I'm not actually only talking about rag here. I'll share these links. Actually let me. I've got them all 107 hugo bowne-anderson: or 2 of them here. 108 hugo bowne-anderson: And I'm just gonna oh, 3 of them. I'll paste them in discord. 109 hugo bowne-anderson: I love this title. It's classic. Tim O'reilly. The r and Rag stands for stands for royalties. And what O'reilly has done is they've used rag systems 110 hugo bowne-anderson: on their books. Sorry they've used the Reg system on their catalog of books so that I can ask a question when I'm in their platform, and it returns 111 hugo bowne-anderson: sources from particular books, and then I can click through to them, and if the source is helpful for me, then 112 hugo bowne-anderson: the rag system allows O'reilly to pay royalties to that that author. And I think this is an incredible use of this technology. To be perfectly honest, I think, William, and and correct me if I'm wrong, William. But William made a point that this doesn't mean it has to be rag right. There are other information retrieval systems that we can. 113 hugo bowne-anderson: you know, used for attribution and that type of stuff. And I totally agree with that that critique 114 hugo bowne-anderson: But it is something which allows us to think more more robustly through attribution and in classic Tim O'reilly style. He's talked about it as one way to think about fixing AI's original sin. And of course we do live in a space which we find incredibly useful. 115 hugo bowne-anderson: but where the big labs have essentially sucked up the world's information without paying anything 116 hugo bowne-anderson: for it. Of course, this isn't new. I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this.", "metadata": {"timestamp": "Chunk 6", "workshop_id": "WS5", "token_count": 1035, "original_chunk_id": "4c8cc0f3-45ae-49a4-bc9c-be2ec3e5019e", "position": 5, "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS5_4c8cc0f3-45ae-49a4-bc9c-be2ec3e5019e", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"original_chunk_id": "c9a18485-8306-4216-9342-b906b362f26b", "timestamp": "Chunk 15", "position": 14, "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS4", "token_count": 1048}, "id": "WS4_c9a18485-8306-4216-9342-b906b362f26b", "relevance": 1.0}, {"text": "So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that. But then if you think about the one to n iteration, this is where it's, well. This verbose should actually make some things a little clearer so if i want to really change how the query is done using the input user data from the client like this is where i can there's a very clear place for me to kind of go do that. Whereas with llama Slam or index, if I want to change something, I really now need to understand Lambda Dix framework. I really now need to go understand, okay, like if I really want to change how this is queried or what the prompt is like, you know, this is where I really need to do, you know, a lot more work. So, um. Hopefully that gives you you know Just a bit of a sense of like, if you're going to look at frameworks and you're going to evaluate them, maybe some lenses to kind of look at. Um but uh peel back the onion there a little bit. All right. And so, you know, is every framework it's right for every situation. Hopefully this is a rhetorical question. But, you know. Short answer is it depends. So this is where like Lama index could be very good for you when you when you need to kind of use it and you really want that quick POC. But then, you know, this is where for the one to n iteration, you really understand like, well. Or what parts of the vacancy retrieval process do I really not want to deal with. That I really want Lamarindex to kind of hand, right. Whereas with Burr, it's like, yeah, it's more bring your own. But then I get those benefits that I can stitch together llama index and, you know. Haystack together because I can do that because that's, you know, Burke makes that easy for you to kind of do so Well, cool. So in this last 15 minutes, I probably don't quite as much. I'm going to quickly go over guardrails. Otherwise, Priya, good point. Yes, frameworks are as opinionated as models are. So what are GAD reps. Nothing too complex. We have some input, right. We want to send to an LLM and then we want to get some output. God rows, you can think of it as visually things that are guiding and trying to make sure like things don't go off the rails so to speak We have actually two places where guardrails come in or how people think about them, how people use them, right. So it's either you know. We get some input and we're doing some check here before we send it to the LM or afterwards we you know get the output from the LLM, do some check before we send it back onto the user. And so mechanically, what that actually means is that Rather than having the input directly go to the LM, you're processing it through some sort of code. So, you know. This is where you might have like different ways of doing this, but it could be if else statements on like what questions or input the user has put in, right. You might have regular expressions. You might have machine learning models that are specifically trained on detecting a particular type of either injection prompt injection attack. It could actually be another LLM call. If it's another LM call, you run into this kind of, you know, how do you trust the LM call that it's doing its thing right but In both of these places. This is where uh yeah You can put some arbitrary Python code that is looking at the inputs or the output of what it's passed and then it's deciding either, you know, whether to flag this raise an exception This is where, depending on how your system does, it could actually you know uh Retry things. So in the case of hallucinations or other things like there's different strategies but In terms of guardrails right they're not anything too complex. All it is is Before you send input to an LLM. You have some if-al statements, regexes, machine learning models or another LLM call to check the quality or the things you want to check or guard against in the input And then similarly in the output, some sort of checks to to see What's going on. Anyone have questions here. Happy to dig into more. I think there's more Yes, good question. So can we paralyze the guardrails if you care about latency. So\u2026 it kind of depends. So yes, if you add in guardrails right naively like this will actually add potentially add to your latency cost. So if it's any one of these top three. Like the it's going to be pretty marginal, but if it's going to be another LM call or series of LLM calls. Then yes, then this can add to your latency. Can you paralyze some of these things. It kind of depends on the question.", "metadata": {"workshop_id": "WS4", "position": 14, "token_count": 1048, "source": "sentence_chunking", "timestamp": "Chunk 15", "speaker": "Unknown", "original_chunk_id": "6c6748ec-6e50-49e9-83a8-6600e01683d8"}, "id": "WS4_6c6748ec-6e50-49e9-83a8-6600e01683d8", "relevance": 1.0}, {"text": "It doesn't dictate The logic in the service, it just wants to help you stitch together HTTP to business logic and back. And I want to say the best ways to understand is to understand a bit more about the project origins or understand who created it, right. So this is where Samuel Colvin, Pydantic creator and Pydantic AI. We'll give you, I guess, a bit more of a sense as to what they're optimizing for. And then you should not bring in a framework just because you think you need a framework. Like you really should like you know, when you bring it in, it should be clear that like, hey, this will actually make some part of iteration faster. Iteration is going to be different for you, different on your context. So asterisk there. And then you also have to partition it into the zero to one case. So I have nothing and I want to create something very quickly to, oh, look, this is going to be running in production There's going to be many iterations. It's going to be living for a long time. Like you might make different choices between these two things, right. And so I want to say this is probably the where things I think, bifurcate the most in terms of frameworks or choosing stuff, at least in the gen ai space is like Do you optimize something that gets you from zero to one first or do you optimize to something that helps you iterate successively and progressively as a team versus say an individual. And then importantly, right. This should replace code that you don't care about. Because that is what you want to delegate to the framework. If you have to dive into the framework. Too often to change something, to augment or to implement. I want to say that's a smell. And so just Not to say that it's bad, but this is where, you know, if you're diving and changing things too much, then maybe it's like, well. What value is that framework kind of providing. You have to be clear about it. So, yeah. Sorry, Seven. One question on the last slide. People were wondering why like you included FastAPI as an example, along with the other two. Yeah, I mean, because to build a Gen AI app, I could just do everything vanilla, right. But it's like, this is where like my problem is if I'm serving something, I mean, this is where I just wanted to kind of give you an example of like frameworks and not just focus on generic ones but like Hmm. There isn't anything You shouldn't read too much into that versus you're saying like they optimize for different things, right. And so this is the the This does not optimize for any LLM stuff at all. Yeah. Good question. Cool. So just to give you, so I actually, you know, I've created two open source projects, right. And so just to give you a quick sense of in a couple of minutes, two example design differences just to like hopefully crystallize an example of kind of what I was talking to about What we were optimizing for and of course spaces back up cool so um uh In the first kind of workshop, we used kind of lamarind dates right and so um uh the um there's a lot that kind of happens in here right and so You can, this is where you can this is we, you know, you uploaded some pretty pdf Right. And then you say, given the query, give me a response. Super cute like super fast, right. But this is where If I want to know what's happening, like I really now have to dig into understanding the framework and then all the frameworks and direction, right. So this is where you know, they're not optimizing for that perspective right and so To juxtaposition burr, which is, if you're familiar with Landgraph, it is something pretty similar to that. But part of the idea was that we actually don't want to hide anything any logic, but we want to make it really easy to glue things together and show how things kind of relate. And so with respect to how something is actually kind of generated right we You can actually see like this LLM client is actually a, you know, you pass in the LLM client to kind of use. Doesn't do anything LLM specific, right. So in terms of one thing you can kind of tell is when you're looking at someone that's using a framework how much code from the framework or modules that they're importing. And so I think in Lamarindex, actually, it's not too much but um uh As your code base gets more complex, this is where you can kind of really see this kind of come out. But the design decision is that this is a little more verbose. In terms of description. So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that.", "metadata": {"position": 13, "workshop_id": "WS4", "token_count": 1032, "speaker": "Unknown", "timestamp": "Chunk 14", "source": "sentence_chunking", "original_chunk_id": "20fce790-a4b6-4f38-a1ef-c0d282961e04"}, "id": "WS4_20fce790-a4b6-4f38-a1ef-c0d282961e04", "relevance": 1.0}, {"text": "It doesn't dictate The logic in the service, it just wants to help you stitch together HTTP to business logic and back. And I want to say the best ways to understand is to understand a bit more about the project origins or understand who created it, right. So this is where Samuel Colvin, Pydantic creator and Pydantic AI. We'll give you, I guess, a bit more of a sense as to what they're optimizing for. And then you should not bring in a framework just because you think you need a framework. Like you really should like you know, when you bring it in, it should be clear that like, hey, this will actually make some part of iteration faster. Iteration is going to be different for you, different on your context. So asterisk there. And then you also have to partition it into the zero to one case. So I have nothing and I want to create something very quickly to, oh, look, this is going to be running in production There's going to be many iterations. It's going to be living for a long time. Like you might make different choices between these two things, right. And so I want to say this is probably the where things I think, bifurcate the most in terms of frameworks or choosing stuff, at least in the gen ai space is like Do you optimize something that gets you from zero to one first or do you optimize to something that helps you iterate successively and progressively as a team versus say an individual. And then importantly, right. This should replace code that you don't care about. Because that is what you want to delegate to the framework. If you have to dive into the framework. Too often to change something, to augment or to implement. I want to say that's a smell. And so just Not to say that it's bad, but this is where, you know, if you're diving and changing things too much, then maybe it's like, well. What value is that framework kind of providing. You have to be clear about it. So, yeah. Sorry, Seven. One question on the last slide. People were wondering why like you included FastAPI as an example, along with the other two. Yeah, I mean, because to build a Gen AI app, I could just do everything vanilla, right. But it's like, this is where like my problem is if I'm serving something, I mean, this is where I just wanted to kind of give you an example of like frameworks and not just focus on generic ones but like Hmm. There isn't anything You shouldn't read too much into that versus you're saying like they optimize for different things, right. And so this is the the This does not optimize for any LLM stuff at all. Yeah. Good question. Cool. So just to give you, so I actually, you know, I've created two open source projects, right. And so just to give you a quick sense of in a couple of minutes, two example design differences just to like hopefully crystallize an example of kind of what I was talking to about What we were optimizing for and of course spaces back up cool so um uh In the first kind of workshop, we used kind of lamarind dates right and so um uh the um there's a lot that kind of happens in here right and so You can, this is where you can this is we, you know, you uploaded some pretty pdf Right. And then you say, given the query, give me a response. Super cute like super fast, right. But this is where If I want to know what's happening, like I really now have to dig into understanding the framework and then all the frameworks and direction, right. So this is where you know, they're not optimizing for that perspective right and so To juxtaposition burr, which is, if you're familiar with Landgraph, it is something pretty similar to that. But part of the idea was that we actually don't want to hide anything any logic, but we want to make it really easy to glue things together and show how things kind of relate. And so with respect to how something is actually kind of generated right we You can actually see like this LLM client is actually a, you know, you pass in the LLM client to kind of use. Doesn't do anything LLM specific, right. So in terms of one thing you can kind of tell is when you're looking at someone that's using a framework how much code from the framework or modules that they're importing. And so I think in Lamarindex, actually, it's not too much but um uh As your code base gets more complex, this is where you can kind of really see this kind of come out. But the design decision is that this is a little more verbose. In terms of description. So specifically, how do you construct and do something like you need to like construct like a graph and say how things kind of are connected, right. So it's a little more kind of verbose than that.", "metadata": {"token_count": 1032, "position": 13, "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "7af12ccf-7c71-4e6d-82f0-60d6589ebb78", "timestamp": "Chunk 14", "workshop_id": "WS4"}, "id": "WS4_7af12ccf-7c71-4e6d-82f0-60d6589ebb78", "relevance": 1.0}, {"text": "And once you do this it'll become clear. Oh, if I improve my information retrieval, I'll get 5% lift. 75 hugo bowne-anderson: If I improve the way I'm ingesting Pdfs, I'll actually get 20% lift right. So it's, you know, once again, use all the multi agentic multiturn, infinite rag, you know 76 hugo bowne-anderson: one 77 hugo bowne-anderson: trillion token context windows that that you want. Just know if you're looking to improve your business metrics. This this process will get you a long way. And then in workshop 4. Stefan took us all through the 2 iteration loops of testing in dev and and production, and even introduced the concept how to implement crcd, and these types of things. And we even gave you a blueprint for doing that with Github pull requests. 78 hugo bowne-anderson: So I'm excited to see if any of you want to play around with. 79 hugo bowne-anderson: So today, we're going to focus on context. 80 hugo bowne-anderson: So the basic idea here. And this is something William Horton made made a point of, and IA hundred 10% agree with one of the 81 hugo bowne-anderson: one of the 82 hugo bowne-anderson: most impactful use cases for Llms that I've seen in businesses and in personal use is people being able to query internal documents. 83 hugo bowne-anderson: Right. And so thinking about. Then how do we build systems which allow people to do this robustly, quickly, efficiently, right. So how do we get external context into the Llm system. Right. And how do we make sure it's not like just drawing information from its weights as well. So that's really what we're here to talk about today. 84 hugo bowne-anderson: To put it another way, and we'll be going through this blog post in detail in the next workshop. This is from. 85 hugo bowne-anderson: Let me just share the link in discord. And this is a blog post that I you know I'm 86 hugo bowne-anderson: I really. I mentioned most workshops to be honest. 87 hugo bowne-anderson: I just saw Jeff's yeah, totally. 88 hugo bowne-anderson: So this is a blog post called building effective agents. And the basic idea here is that, you know, if we're going to talk about agents, we probably should define it, and we should probably think about 89 hugo bowne-anderson: how do we move from a single Llm. Call 90 hugo bowne-anderson: to something more agentic and 91 hugo bowne-anderson: Barry and Eric at anthropic formulated as augmented Llms. Where we start building different types of workflows that we can think about as being on the agentic continuum. But even before that, knowing that Llms 92 hugo bowne-anderson: you give it tokens, it gives tokens out okay. And then along the continuum to building agentic type things, then we integrate other types of tooling and properties into the system. So the 1st one to think about really is, is memory right. Well, it may not be the first, st but it's an important one, and we've discussed that, in brief. And that's why I had Eddie from Fondue to come and talk to you about simple ways to think about memory last week. 93 hugo bowne-anderson: The next one is 94 hugo bowne-anderson: tool calls, okay, and this is something we're going to be talking about later later this week. And, in fact, for anyone who wants to join the the workshop with ravine next next week, doing a lot of 95 hugo bowne-anderson: tool calls, using Olama and Gradio and Gemma. 3 are all all locally, which will be super fun. But you could imagine, for example. 96 hugo bowne-anderson: you want to get the weather today somewhere and the cutoff date of Vlm you're working with was who knows when. And so you can implement a basic tool call to a weather Api. Similarly, you'll notice that chat Gpt. Perhaps these days you'll ask it something. You won't tell it to search the web, but it will recognize that perhaps 97 hugo bowne-anderson: searching the web will help it. Now, who knows whether that's true or not. To be honest. But you'll see it will dynamically search the web without even being told to, and that's more along the agentic side. But these types of tool uses are incredibly important. And then there's retrieval.", "metadata": {"source": "sentence_chunking", "position": 4, "original_chunk_id": "4d2d3e29-8883-4bcd-9c1a-fe7bbe9df289", "token_count": 1050, "timestamp": "Chunk 5", "workshop_id": "WS5", "speaker": "Unknown"}, "id": "WS5_4d2d3e29-8883-4bcd-9c1a-fe7bbe9df289", "relevance": 1.0}, {"text": "And once you do this it'll become clear. Oh, if I improve my information retrieval, I'll get 5% lift. 75 hugo bowne-anderson: If I improve the way I'm ingesting Pdfs, I'll actually get 20% lift right. So it's, you know, once again, use all the multi agentic multiturn, infinite rag, you know 76 hugo bowne-anderson: one 77 hugo bowne-anderson: trillion token context windows that that you want. Just know if you're looking to improve your business metrics. This this process will get you a long way. And then in workshop 4. Stefan took us all through the 2 iteration loops of testing in dev and and production, and even introduced the concept how to implement crcd, and these types of things. And we even gave you a blueprint for doing that with Github pull requests. 78 hugo bowne-anderson: So I'm excited to see if any of you want to play around with. 79 hugo bowne-anderson: So today, we're going to focus on context. 80 hugo bowne-anderson: So the basic idea here. And this is something William Horton made made a point of, and IA hundred 10% agree with one of the 81 hugo bowne-anderson: one of the 82 hugo bowne-anderson: most impactful use cases for Llms that I've seen in businesses and in personal use is people being able to query internal documents. 83 hugo bowne-anderson: Right. And so thinking about. Then how do we build systems which allow people to do this robustly, quickly, efficiently, right. So how do we get external context into the Llm system. Right. And how do we make sure it's not like just drawing information from its weights as well. So that's really what we're here to talk about today. 84 hugo bowne-anderson: To put it another way, and we'll be going through this blog post in detail in the next workshop. This is from. 85 hugo bowne-anderson: Let me just share the link in discord. And this is a blog post that I you know I'm 86 hugo bowne-anderson: I really. I mentioned most workshops to be honest. 87 hugo bowne-anderson: I just saw Jeff's yeah, totally. 88 hugo bowne-anderson: So this is a blog post called building effective agents. And the basic idea here is that, you know, if we're going to talk about agents, we probably should define it, and we should probably think about 89 hugo bowne-anderson: how do we move from a single Llm. Call 90 hugo bowne-anderson: to something more agentic and 91 hugo bowne-anderson: Barry and Eric at anthropic formulated as augmented Llms. Where we start building different types of workflows that we can think about as being on the agentic continuum. But even before that, knowing that Llms 92 hugo bowne-anderson: you give it tokens, it gives tokens out okay. And then along the continuum to building agentic type things, then we integrate other types of tooling and properties into the system. So the 1st one to think about really is, is memory right. Well, it may not be the first, st but it's an important one, and we've discussed that, in brief. And that's why I had Eddie from Fondue to come and talk to you about simple ways to think about memory last week. 93 hugo bowne-anderson: The next one is 94 hugo bowne-anderson: tool calls, okay, and this is something we're going to be talking about later later this week. And, in fact, for anyone who wants to join the the workshop with ravine next next week, doing a lot of 95 hugo bowne-anderson: tool calls, using Olama and Gradio and Gemma. 3 are all all locally, which will be super fun. But you could imagine, for example. 96 hugo bowne-anderson: you want to get the weather today somewhere and the cutoff date of Vlm you're working with was who knows when. And so you can implement a basic tool call to a weather Api. Similarly, you'll notice that chat Gpt. Perhaps these days you'll ask it something. You won't tell it to search the web, but it will recognize that perhaps 97 hugo bowne-anderson: searching the web will help it. Now, who knows whether that's true or not. To be honest. But you'll see it will dynamically search the web without even being told to, and that's more along the agentic side. But these types of tool uses are incredibly important. And then there's retrieval.", "metadata": {"original_chunk_id": "bf36823a-27cc-4f5e-a105-6e9d28292d3e", "token_count": 1050, "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS5", "position": 4, "timestamp": "Chunk 5"}, "id": "WS5_bf36823a-27cc-4f5e-a105-6e9d28292d3e", "relevance": 1.0}]}}{"question": "What are the key differences between retrieval-augmented generation (RAG) and traditional LLM methods, and why would I choose one over the other for my application?", "response": "Retrieval-augmented generation (RAG) differs from traditional LLM methods primarily in how it integrates external information. Traditional LLMs rely on fixed datasets and their internal knowledge, which can lead to outdated or incomplete responses. In contrast, RAG dynamically retrieves relevant information from a corpus of documents, enhancing the model's ability to provide accurate, up-to-date, and contextually relevant answers.\n\nChoosing RAG is beneficial when your application requires real-time information, domain-specific knowledge, or the ability to ground responses in specific documents. RAG can improve accuracy and reduce hallucinations by ensuring the model references actual data. However, it may introduce higher latency due to the retrieval process, making it less suitable for applications needing immediate responses.\n\nOn the other hand, traditional LLMs are simpler and faster, making them ideal for applications where speed is critical and the information is relatively static. Ultimately, the choice depends on your application's needs for accuracy, context, and response time. Consider your specific use case to determine the best approach.", "context_info": {"num_chunks": 10, "context_tokens": 10164, "completion_tokens": 206, "embedding_tokens": 15360, "workshops_used": ["WS6", "WS3", "WS5", "WS4"], "chunks": [{"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"position": 6, "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "dd8071dd-88a3-4d57-b9fe-477c28a9b246", "timestamp": "Chunk 7", "workshop_id": "WS5", "token_count": 1023}, "id": "WS5_dd8071dd-88a3-4d57-b9fe-477c28a9b246", "relevance": 1.0}, {"text": "I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this. But I think Tim's thinking around this being an original sin of AI is, is probably worth thinking about a bit more 117 hugo bowne-anderson: so. 118 hugo bowne-anderson: The the stage having been set the 2 major questions, and these are 119 hugo bowne-anderson: in the wrong order for a right reason. So the questions are, how do we give information. We extract to the Llm. And the other question is, how do we get information out of our corpus of documents. And of course you want to answer the second 1 1st in order to give it to the Llm. But the 1st one isn't 120 hugo bowne-anderson: really that challenging right. And the second one is is one of the big challenges in the space. So just thinking through. And there are different ways to do this. But modulo minor details, most of the effects are the same. So the question is, how do we format and pass context. Let's say we've retrieved a bunch from our documents. Most common techniques 121 hugo bowne-anderson: prepending to the prompt. So literally, just add the retrieval context directly to the input, saying, here is the relevant information. XYZ. Then ask the question, okay, surprisingly effective. 122 hugo bowne-anderson: You can also have structured templates where you organize the retrieve context into well-defined sections like context, query for clarity and consistency. You can also use our system messages for chat based models, past context via system level instructions. You're an assistant. Use the following context to control behavior. 123 hugo bowne-anderson: Now, there are challenges which we'll get to managing the context window efficiently, such as how we chunk things and prioritizing where to put different pieces of information and then ensuring the context is relevant and concise, so it doesn't overwhelm or confuse the Llm. And I would argue that this second challenge, and partially the first, st is more about how we retrieve information than how we feed it, feed it to it. But there is overlap here. 124 hugo bowne-anderson: So 125 hugo bowne-anderson: how do we get information out of our corpus of documents. Okay, so I think you're all like, have heard the term rag. 126 hugo bowne-anderson: if not having built with it, which is retrieval, augmented generation, and that essentially, as we'll see today we do. We take our texts and do some form of embedding which essentially that just embeddings turn 127 hugo bowne-anderson: strings into vectors into numbers essentially. And then, when we have a query, as a, as a vector, or a set of numbers, we can see how close it is to different things in in the corpus. Essentially. So, that's 128 hugo bowne-anderson: kind of state of the art in a lot of ways. And I think Rag has sounded like intricate. And of course there are a lot of important details and optimizations, but the principle, I think, is more straightforward than one may believe when 1st encountering the term, same as like Llm. As a judge, and all this jargon stuff where it's like once you break it down relatively straightforward to explain, and Grock on the other end, not so much to to implement all the time. 129 hugo bowne-anderson: But in all honesty, the 1st system we built with lama Index. It did all the rag stuff and embeddings behind the scene for us. Having said all that I do want to mention, there are a lot of other techniques, right. You can do exact matching right. You can use grep and regular expressions for precise patterns. There's keyword-based retrieval which we'll talk about a bit and Tfidf for term importance. Heuristic searching. There are statistical methods. Bm, 25 has worked incredibly well 130 hugo bowne-anderson: for relevance ranking and recommender systems. And there's an increasing body of literature around the fact that rag is really recommender systems in disguise. And if you use classical techniques that will get you a long way and actually be somewhat more efficient. Then we have embedding based retrieval, so semantic similarity via vector spaces, which is kind of state, of the art in a lot of ways. 131 hugo bowne-anderson: And then we have hybrid approaches. Now the challenges. 132 hugo bowne-anderson: The big ones are chunking documents for effective retrieval. So how do we break a Pdf. Or long text into meaningful. 133 hugo bowne-anderson: meaningful parts.", "metadata": {"workshop_id": "WS5", "timestamp": "Chunk 7", "speaker": "Unknown", "original_chunk_id": "8ea669af-ed9c-472d-9636-d14270b3b54d", "token_count": 1023, "source": "sentence_chunking", "position": 6}, "id": "WS5_8ea669af-ed9c-472d-9636-d14270b3b54d", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"timestamp": "Chunk 23", "workshop_id": "WS4", "source": "sentence_chunking", "original_chunk_id": "4eedb1cc-f056-44e3-9b97-0d4db328d700", "token_count": 1036, "position": 22, "speaker": "Unknown"}, "id": "WS4_4eedb1cc-f056-44e3-9b97-0d4db328d700", "relevance": 1.0}, {"text": "And just kind of from what I've seen in practice, it actually works surprisingly And so, yeah, you know, just kind of like a fun Fun analogy, I come from a data science background. So, you know, this is basically very similar to just a recurrent neural network architecture where Instead of basically the hidden state is just kind of like that string Which with every period of time, you're getting kind of like a new data plan and just updating that same that same string to provide outputs. So speaking to kind of the trade-offs that Allah, sorry if I didn't pronounce your name correctly, but I think you accurately preempted is that This is not very scalable. It's not very token efficient. It does have great latency. I would say this is a form of retrieval augmented generation. It doesn't use a vector database. It doesn't use any fancy machinery, but it's extremely fast. It's extremely simple and easy to understand and kind of intuitive to see what's going on and to develop intuition. And then, yeah, the last thing is that because it's such low latency. It can easily be kind of available all the time. You can easily just provide it kind of supplement every language model call with it. So now getting into getting into kind of like addressing some of the weaknesses of that approach and starting to make this more scalable And so, yeah, this is where kind of like retrieval starts to play a bigger role And so in this case, you're not Just um you're not just retrieving and updating a single string you're updating and retrieving a bunch of strings that live in a vector database Or you could use lexical search, which I think works very well, or some hybrid approach as well. But, you know, kind of whatever your approach is there. But yeah, the main difference here is so, you know, so here we have the same kind of like workflow, except you'll notice that we're no longer injecting this into every single prompt because we're paying a latency cost And so depending on your application, you may want to continue injecting that into every language model call. But from my experience. Once you kind of add the retrieval component, at least in kind of a consumer application. You're going to be hit with enough latency that you're probably going to want to wait to kind of retrieve the deeper knowledge Until you actually really need it. So yeah, you can still kind of have This is the same kind of consistent pool use Deciding to use the same memory tool. You have the language model call, which is basically instead of just retrieving that single string. It's able to retrieve, you know, imagine you have a vector database, you have, you know, a bunch of strings, each that represent maybe like an atomic unit of knowledge, however you decide to define that. There's a lot of kind of design decisions and kind of prompting decisions that go into that. But the main idea here is that you're retrieving the relevant strings within this collection of strings that live in your vector database or collection of documents. You're using the language model to understand, to basically make decisions about how to update them. So if you have something that If one of your preferences changes or you have a new kid or, you know, kind of a date for a meeting changes or something like that, right. You're going to want to be able to retrieve that existing information and update it. There are other times where you're just creating something that's entirely new. But there is always this really important problem of wanting to make sure that you're your memory, your knowledge store is kind of internally consistent and doesn't contradict itself And so, yeah, instead of the relational database, we're just kind of swapping this out with a vector DB instead of a single string We have a bunch of different strings. Instead of injecting this into every prompt. We have a tool that the agent can optionally reference if it's reference if it's if it's trying to answer a question or kind of perform a task that it thinks would benefit from some memory that exists within the vector database. So, you know, also trade-offs here, right. So this is a much more scalable approach it's much more token efficient. It's only efficient retrieving the strings and in context that you know according to your retrieval engine is relevant to the task that the language model is trying to solve. That said, it's It's much higher latency. What we do on in fondue is, you know, we have hybrid retrieval and then we have a re-ranker on top and so that that works really well. The quality is extremely good in terms of relevance, but it could take Two seconds to provide a response. In some cases, that latency is very well worth it, but it's definitely not something that would want to inject into every single language model call. The update logic is significantly more complicated. It's a lot of It's just kind of everything that comes with more complexity. It's harder to debug. It's harder to get kind of intuition in terms of exactly what it's doing.", "metadata": {"position": 22, "source": "sentence_chunking", "original_chunk_id": "cd32a46d-1bc2-42cc-bf15-ddc5bb384a17", "timestamp": "Chunk 23", "workshop_id": "WS4", "speaker": "Unknown", "token_count": 1036}, "id": "WS4_cd32a46d-1bc2-42cc-bf15-ddc5bb384a17", "relevance": 1.0}, {"text": "But these types of tool uses are incredibly important. And then there's retrieval. 98 hugo bowne-anderson: which is what we're here to talk talk about today. But the idea is, if you have a corpus of documents, how do you. How do you give that information to your Llm. And how do you. How do you query it. So these are the 3, the 3 pillars of thinking about building agents and thinking about 99 hugo bowne-anderson: like giving your Llm systems more capabilities. Essentially. We're at the early stages of working with this type of quote unquote software and figuring out how to give it a bit more that it can do to help us. In the work we want to do. Okay. So 100 hugo bowne-anderson: firstly, why do we even need to add context. Okay, so I think that a lot of this should be obvious. But we want to fill knowledge gaps. 101 hugo bowne-anderson: Llms are trained on fixed data sets. Can't quote unquote, know everything we can give it external information for domain, specific proprietary, time-sensitive knowledge. 102 hugo bowne-anderson: it can help ground and increase accuracy. So we've seen systems. And this is something in llama indexes favor. And for you, all of you who played around with the app I've built. I've seen some of you have asked the question that you knew wasn't in the Transcript, and it replied, this isn't in the Transcript, whatever, and you gave it a pass. You're like that's good. It didn't hallucinate something there, right. So grounding in the actual documents can work to constrain the models outputs now still know we do this through prompting, like 103 hugo bowne-anderson: we literally in the prompt say, hey. If you can't find it, please do not. Pretty. Please do not hallucinate my Llm, you know. So, and that's also the page it like we've got people at Deepmind who are like, just put it in the prompt right. So. And I don't mean to be flippant about that. But it 104 hugo bowne-anderson: it's a pretty interesting way to build software as far as I'm concerned, and it it works better than other ways. But it is a pretty wild thing to be doing. We get task specific relevance. 105 hugo bowne-anderson: We can get dynamic and up to date information. 106 hugo bowne-anderson: and this is actually one of my favorite uses. It can enable explainability. And I know William and I discussed this in in January, and there's some nuance to this, and I'm not actually only talking about rag here. I'll share these links. Actually let me. I've got them all 107 hugo bowne-anderson: or 2 of them here. 108 hugo bowne-anderson: And I'm just gonna oh, 3 of them. I'll paste them in discord. 109 hugo bowne-anderson: I love this title. It's classic. Tim O'reilly. The r and Rag stands for stands for royalties. And what O'reilly has done is they've used rag systems 110 hugo bowne-anderson: on their books. Sorry they've used the Reg system on their catalog of books so that I can ask a question when I'm in their platform, and it returns 111 hugo bowne-anderson: sources from particular books, and then I can click through to them, and if the source is helpful for me, then 112 hugo bowne-anderson: the rag system allows O'reilly to pay royalties to that that author. And I think this is an incredible use of this technology. To be perfectly honest, I think, William, and and correct me if I'm wrong, William. But William made a point that this doesn't mean it has to be rag right. There are other information retrieval systems that we can. 113 hugo bowne-anderson: you know, used for attribution and that type of stuff. And I totally agree with that that critique 114 hugo bowne-anderson: But it is something which allows us to think more more robustly through attribution and in classic Tim O'reilly style. He's talked about it as one way to think about fixing AI's original sin. And of course we do live in a space which we find incredibly useful. 115 hugo bowne-anderson: but where the big labs have essentially sucked up the world's information without paying anything 116 hugo bowne-anderson: for it. Of course, this isn't new. I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this.", "metadata": {"source": "sentence_chunking", "token_count": 1035, "position": 5, "workshop_id": "WS5", "original_chunk_id": "514ae663-d4d7-4116-820c-2566e481c591", "timestamp": "Chunk 6", "speaker": "Unknown"}, "id": "WS5_514ae663-d4d7-4116-820c-2566e481c591", "relevance": 1.0}, {"text": "But these types of tool uses are incredibly important. And then there's retrieval. 98 hugo bowne-anderson: which is what we're here to talk talk about today. But the idea is, if you have a corpus of documents, how do you. How do you give that information to your Llm. And how do you. How do you query it. So these are the 3, the 3 pillars of thinking about building agents and thinking about 99 hugo bowne-anderson: like giving your Llm systems more capabilities. Essentially. We're at the early stages of working with this type of quote unquote software and figuring out how to give it a bit more that it can do to help us. In the work we want to do. Okay. So 100 hugo bowne-anderson: firstly, why do we even need to add context. Okay, so I think that a lot of this should be obvious. But we want to fill knowledge gaps. 101 hugo bowne-anderson: Llms are trained on fixed data sets. Can't quote unquote, know everything we can give it external information for domain, specific proprietary, time-sensitive knowledge. 102 hugo bowne-anderson: it can help ground and increase accuracy. So we've seen systems. And this is something in llama indexes favor. And for you, all of you who played around with the app I've built. I've seen some of you have asked the question that you knew wasn't in the Transcript, and it replied, this isn't in the Transcript, whatever, and you gave it a pass. You're like that's good. It didn't hallucinate something there, right. So grounding in the actual documents can work to constrain the models outputs now still know we do this through prompting, like 103 hugo bowne-anderson: we literally in the prompt say, hey. If you can't find it, please do not. Pretty. Please do not hallucinate my Llm, you know. So, and that's also the page it like we've got people at Deepmind who are like, just put it in the prompt right. So. And I don't mean to be flippant about that. But it 104 hugo bowne-anderson: it's a pretty interesting way to build software as far as I'm concerned, and it it works better than other ways. But it is a pretty wild thing to be doing. We get task specific relevance. 105 hugo bowne-anderson: We can get dynamic and up to date information. 106 hugo bowne-anderson: and this is actually one of my favorite uses. It can enable explainability. And I know William and I discussed this in in January, and there's some nuance to this, and I'm not actually only talking about rag here. I'll share these links. Actually let me. I've got them all 107 hugo bowne-anderson: or 2 of them here. 108 hugo bowne-anderson: And I'm just gonna oh, 3 of them. I'll paste them in discord. 109 hugo bowne-anderson: I love this title. It's classic. Tim O'reilly. The r and Rag stands for stands for royalties. And what O'reilly has done is they've used rag systems 110 hugo bowne-anderson: on their books. Sorry they've used the Reg system on their catalog of books so that I can ask a question when I'm in their platform, and it returns 111 hugo bowne-anderson: sources from particular books, and then I can click through to them, and if the source is helpful for me, then 112 hugo bowne-anderson: the rag system allows O'reilly to pay royalties to that that author. And I think this is an incredible use of this technology. To be perfectly honest, I think, William, and and correct me if I'm wrong, William. But William made a point that this doesn't mean it has to be rag right. There are other information retrieval systems that we can. 113 hugo bowne-anderson: you know, used for attribution and that type of stuff. And I totally agree with that that critique 114 hugo bowne-anderson: But it is something which allows us to think more more robustly through attribution and in classic Tim O'reilly style. He's talked about it as one way to think about fixing AI's original sin. And of course we do live in a space which we find incredibly useful. 115 hugo bowne-anderson: but where the big labs have essentially sucked up the world's information without paying anything 116 hugo bowne-anderson: for it. Of course, this isn't new. I mean Google and Facebook. And we do, you know, these these types of technologies have pre-existed this.", "metadata": {"original_chunk_id": "4c8cc0f3-45ae-49a4-bc9c-be2ec3e5019e", "source": "sentence_chunking", "speaker": "Unknown", "workshop_id": "WS5", "token_count": 1035, "position": 5, "timestamp": "Chunk 6"}, "id": "WS5_4c8cc0f3-45ae-49a4-bc9c-be2ec3e5019e", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"token_count": 1068, "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "timestamp": "Chunk 15", "speaker": "Unknown", "source": "sentence_chunking", "workshop_id": "WS3", "position": 14}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "workshop_id": "WS3", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "token_count": 1068, "source": "sentence_chunking", "position": 14, "timestamp": "Chunk 15"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "58 hugo bowne-anderson: and then to trigger actions. So activate appropriate strategies, tools or next steps based on context and inputs. So a lot of people say, make decisions or dynamically, choose Llms aren't making decisions, and they're not choosing anything in terms of having like 59 hugo bowne-anderson: what we assume is agency, at least with ourselves and and other humans. But they do trigger actions based on context and inputs. And actually, when we 60 hugo bowne-anderson: let me bring up this Whoa, bring up this blog post. 61 hugo bowne-anderson: We discussed this yesterday with Marva from hugging face. But we'll talk about that's not it. 62 hugo bowne-anderson: We'll talk about how anthropic has has approached this, which is super useful, but I do want to say this. Beautiful blog post from 63 hugo bowne-anderson: hugging face. I'm just gonna paste it in our workshop channel. 64 hugo bowne-anderson: Let's see. Where did they say, sorry about this. Oh, yeah. 65 hugo bowne-anderson: AI agents are programs where Llm outputs control the workflow. Okay. 66 hugo bowne-anderson: that's a really nice, I think. 67 hugo bowne-anderson: definition or heuristic for thinking about what these, what these systems do. Okay, so 68 hugo bowne-anderson: and just to be clear, chat Gpt is agentic. So I can't even remember what example 69 hugo bowne-anderson: this is, let's see, what did I do here. 70 hugo bowne-anderson: hey, Chatgpt. I would like you to develop a marketing campaign for a product that yes, I'm getting to develop. I'm speaking to develop a marketing campaign. And 71 hugo bowne-anderson: I'm actually just okay. 72 hugo bowne-anderson: Let me see if I can. 73 hugo bowne-anderson: Sorry about this people who would have thought embedding Youtube videos in wouldn't work. 74 hugo bowne-anderson: So I got it to develop a marketing campaign for a product that inserts advertising in people's dreams. 75 hugo bowne-anderson: okay. And you may recall the Nicolas Cage Movie Dream Scenario. I came up with this before that movie, though, although Nicolas Cage does seem to steal all my good ideas, 76 hugo bowne-anderson: And I asked for visuals. Right. And I didn't say, use these tools or whatever. And it quote unquote decided, or its output triggered using Dali 3. Okay. And it's been doing that for some time. Similarly, if I ask for something that maybe outside, maybe outside its weights. 77 hugo bowne-anderson: it will 78 hugo bowne-anderson: it will automatically search the web right without me telling it to. So there is a dynamic response to Llm output that results in it. Using a tool. Okay. And you can try it. For example, you can say, what's the weather today in Sydney to an Llm. Or wherever you are. And it will often use a web search tool without asking asking you whether you want it or not. And without you telling it, right. So a lot of these systems are already agentic. 79 hugo bowne-anderson: But okay, I do want to clarify. Agent is mostly a marketing term. And that's why we were all 80 hugo bowne-anderson: very pleased when anthropic put out this blog post 81 hugo bowne-anderson: called building effective Llms. And actually, could someone paste that in in the chat. I thought I had the link here. But in the interest of time. I won't. 82 hugo bowne-anderson: You're on finding it. I mean, it's 1 that I suppose I mentioned most workshops. Actually, but it's essential. And it's essential to how I currently think about augmenting these systems. So we've talked about 83 hugo bowne-anderson: the importance of memory which a lot of people really don't talk about. And I asked Eddie from Fondue to kind of come and give a talk about like best practices for starting, working, working with memory, and and 2 patterns in particular. Then we can augment Llms by 84 hugo bowne-anderson: having retrieval right. And that's often in the form of rag as we went through earlier this week. But we want to be able to ground it in the context of documents and not just its weights.", "metadata": {"workshop_id": "WS6", "timestamp": "Chunk 4", "position": 3, "speaker": "Unknown", "original_chunk_id": "66b88cac-129a-470a-8b42-67de4ab93364", "source": "sentence_chunking", "token_count": 1035}, "id": "WS6_66b88cac-129a-470a-8b42-67de4ab93364", "relevance": 1.0}, {"text": "58 hugo bowne-anderson: and then to trigger actions. So activate appropriate strategies, tools or next steps based on context and inputs. So a lot of people say, make decisions or dynamically, choose Llms aren't making decisions, and they're not choosing anything in terms of having like 59 hugo bowne-anderson: what we assume is agency, at least with ourselves and and other humans. But they do trigger actions based on context and inputs. And actually, when we 60 hugo bowne-anderson: let me bring up this Whoa, bring up this blog post. 61 hugo bowne-anderson: We discussed this yesterday with Marva from hugging face. But we'll talk about that's not it. 62 hugo bowne-anderson: We'll talk about how anthropic has has approached this, which is super useful, but I do want to say this. Beautiful blog post from 63 hugo bowne-anderson: hugging face. I'm just gonna paste it in our workshop channel. 64 hugo bowne-anderson: Let's see. Where did they say, sorry about this. Oh, yeah. 65 hugo bowne-anderson: AI agents are programs where Llm outputs control the workflow. Okay. 66 hugo bowne-anderson: that's a really nice, I think. 67 hugo bowne-anderson: definition or heuristic for thinking about what these, what these systems do. Okay, so 68 hugo bowne-anderson: and just to be clear, chat Gpt is agentic. So I can't even remember what example 69 hugo bowne-anderson: this is, let's see, what did I do here. 70 hugo bowne-anderson: hey, Chatgpt. I would like you to develop a marketing campaign for a product that yes, I'm getting to develop. I'm speaking to develop a marketing campaign. And 71 hugo bowne-anderson: I'm actually just okay. 72 hugo bowne-anderson: Let me see if I can. 73 hugo bowne-anderson: Sorry about this people who would have thought embedding Youtube videos in wouldn't work. 74 hugo bowne-anderson: So I got it to develop a marketing campaign for a product that inserts advertising in people's dreams. 75 hugo bowne-anderson: okay. And you may recall the Nicolas Cage Movie Dream Scenario. I came up with this before that movie, though, although Nicolas Cage does seem to steal all my good ideas, 76 hugo bowne-anderson: And I asked for visuals. Right. And I didn't say, use these tools or whatever. And it quote unquote decided, or its output triggered using Dali 3. Okay. And it's been doing that for some time. Similarly, if I ask for something that maybe outside, maybe outside its weights. 77 hugo bowne-anderson: it will 78 hugo bowne-anderson: it will automatically search the web right without me telling it to. So there is a dynamic response to Llm output that results in it. Using a tool. Okay. And you can try it. For example, you can say, what's the weather today in Sydney to an Llm. Or wherever you are. And it will often use a web search tool without asking asking you whether you want it or not. And without you telling it, right. So a lot of these systems are already agentic. 79 hugo bowne-anderson: But okay, I do want to clarify. Agent is mostly a marketing term. And that's why we were all 80 hugo bowne-anderson: very pleased when anthropic put out this blog post 81 hugo bowne-anderson: called building effective Llms. And actually, could someone paste that in in the chat. I thought I had the link here. But in the interest of time. I won't. 82 hugo bowne-anderson: You're on finding it. I mean, it's 1 that I suppose I mentioned most workshops. Actually, but it's essential. And it's essential to how I currently think about augmenting these systems. So we've talked about 83 hugo bowne-anderson: the importance of memory which a lot of people really don't talk about. And I asked Eddie from Fondue to kind of come and give a talk about like best practices for starting, working, working with memory, and and 2 patterns in particular. Then we can augment Llms by 84 hugo bowne-anderson: having retrieval right. And that's often in the form of rag as we went through earlier this week. But we want to be able to ground it in the context of documents and not just its weights.", "metadata": {"workshop_id": "WS6", "position": 3, "token_count": 1035, "speaker": "Unknown", "source": "sentence_chunking", "original_chunk_id": "4baa84b2-b8c2-47a4-8b22-0ba866ab1cbd", "timestamp": "Chunk 4"}, "id": "WS6_4baa84b2-b8c2-47a4-8b22-0ba866ab1cbd", "relevance": 1.0}]}}{"question": "What are the key metrics I should focus on when evaluating the performance of my LLM applications to ensure they provide reliable responses?", "response": "To effectively evaluate the performance of your LLM applications, focus on several key metrics that ensure reliability and relevance:\n\n1. **Accuracy**: Measure how well the LLM's responses align with expected outputs. This includes checking if the information is factually correct and grounded in the relevant documents.\n\n2. **Completeness**: Assess whether the responses provide all necessary details. Incomplete answers can lead to misunderstandings or misinformed decisions.\n\n3. **Relevance**: Ensure that the outputs are pertinent to the queries posed. This involves checking if the responses directly address the questions asked.\n\n4. **Conciseness**: Evaluate if the responses are clear and to the point, avoiding unnecessary verbosity while still conveying essential information.\n\n5. **User Satisfaction**: Gather feedback from users regarding the helpfulness and clarity of the responses. This qualitative data can provide insights into user experience.\n\n6. **Error Classification**: Track types of errors (e.g., hallucinations, retrieval errors) to prioritize improvements based on their impact on user experience.\n\n7. **Monitoring and Logging**: Implement robust logging to trace inputs and outputs, allowing for ongoing evaluation and adjustments based on real-world usage.\n\nBy focusing on these metrics, you can ensure that your LLM applications deliver reliable and effective responses.", "context_info": {"num_chunks": 10, "context_tokens": 10338, "completion_tokens": 258, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS1", "WS4"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "position": 14, "speaker": "Unknown", "token_count": 1068, "workshop_id": "WS3", "timestamp": "Chunk 15"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"source": "sentence_chunking", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "token_count": 1068, "workshop_id": "WS3", "timestamp": "Chunk 15", "position": 14, "speaker": "Unknown"}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"timestamp": "Chunk 11", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "speaker": "Unknown", "source": "sentence_chunking", "position": 10, "token_count": 1062, "workshop_id": "WS3"}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"workshop_id": "WS3", "speaker": "Unknown", "position": 10, "original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "source": "sentence_chunking", "token_count": 1062, "timestamp": "Chunk 11"}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"timestamp": "Chunk 16", "original_chunk_id": "62ef5b55-d219-45ff-a564-75ead9264997", "token_count": 1049, "workshop_id": "WS3", "speaker": "Unknown", "position": 15, "source": "sentence_chunking"}, "id": "WS3_62ef5b55-d219-45ff-a564-75ead9264997", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"timestamp": "Chunk 16", "token_count": 1049, "speaker": "Unknown", "position": 15, "workshop_id": "WS3", "source": "sentence_chunking", "original_chunk_id": "ef57fa87-2177-44f5-bab8-24ccb6a587f7"}, "id": "WS3_ef57fa87-2177-44f5-bab8-24ccb6a587f7", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"timestamp": "Chunk 11", "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "source": "sentence_chunking", "workshop_id": "WS1", "speaker": "Unknown", "token_count": 1050, "position": 10}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"token_count": 1050, "workshop_id": "WS1", "position": 10, "timestamp": "Chunk 11", "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3"}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"source": "sentence_chunking", "timestamp": "Chunk 9", "token_count": 1012, "position": 8, "workshop_id": "WS4", "speaker": "Unknown", "original_chunk_id": "79e3ed2f-8447-44f2-a41f-7288f51c8da7"}, "id": "WS4_79e3ed2f-8447-44f2-a41f-7288f51c8da7", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"workshop_id": "WS4", "token_count": 1012, "timestamp": "Chunk 9", "source": "sentence_chunking", "position": 8, "original_chunk_id": "d492c886-fee2-48a6-b289-95214ec3d247", "speaker": "Unknown"}, "id": "WS4_d492c886-fee2-48a6-b289-95214ec3d247", "relevance": 1.0}]}}{"question": "What metrics should I focus on when evaluating the performance of my LLM applications to ensure they meet production standards?", "response": "To effectively evaluate the performance of your LLM applications, focus on several key metrics that align with production standards:\n\n1. **Accuracy and Completeness**: Ensure the output accurately answers queries and captures all required information. For example, in summarization tasks, the output should concisely convey key points without unnecessary length.\n\n2. **Relevance**: The responses should be contextually relevant and grounded in the provided documents. This is crucial for information retrieval tasks.\n\n3. **Clarity and Helpfulness**: Evaluate whether the generated content is clear, polite, and genuinely helpful, rather than just appearing so.\n\n4. **Error Classification**: Track types of errors, such as hallucinations, retrieval errors, and formatting issues. This helps prioritize which areas to improve.\n\n5. **User Feedback**: Incorporate human evaluations to calibrate the model's performance against real-world expectations. Regularly update evaluation criteria based on this feedback.\n\n6. **Monitoring and Logging**: Implement robust logging to capture input-output pairs and system performance over time. This aids in identifying drift and biases.\n\n7. **Business Impact**: Ultimately, assess whether the LLM contributes to hiring good candidates or improves efficiency compared to previous systems.\n\nBy focusing on these metrics, you can ensure your LLM applications meet the necessary production standards and continuously improve their performance.", "context_info": {"num_chunks": 10, "context_tokens": 10335, "completion_tokens": 269, "embedding_tokens": 15360, "workshops_used": ["WS3", "WS1", "WS4"], "chunks": [{"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"speaker": "Unknown", "original_chunk_id": "a3854f37-c968-45a1-9537-915ee70740e0", "position": 14, "workshop_id": "WS3", "timestamp": "Chunk 15", "source": "sentence_chunking", "token_count": 1068}, "id": "WS3_a3854f37-c968-45a1-9537-915ee70740e0", "relevance": 1.0}, {"text": "For example, the ones sent, but then the macro system calls. Do we actually hire good candidates. So we want to think about tying these two things together So in terms of evaluating output, and all I want to do is a zoom, zoom, zoom. And what we're going\u2026 What's good. Seriously, I don't know. And in fact, I want to link to William. William gave a guest lecture in The first cohort of this course in January about how tough it is to even There's a paradox that you don't even know quite what you want to measure until you start launching something as well, which the synthetic data generation gets us around slightly. But once you have real users, you iterate pretty quickly, right. So what is good. In summarization. Sorry, in extraction, what is good. Does the output accurately extract all required information In summarization. Does it concisely capture the key points. You could have it be comprehensive and it's longer than the original text. That seems like bad summarization In generative tasks, perhaps we want it to be clear, polite and helpful. I'd love it if LLMs were optimized for being helpful as opposed to appearing to be helpful. That would actually help me a lot. In information retrieval. Tasks does he output provide accurate, relevant and complete answers to the query. And I should add that a grounded the document, the docs as well, right. So something to think about and a discussion we can have on our Discord is. Choose one of the above and define what good means. Consider what characteristics would matter most to you. Okay. So let's just look at an example of summarization. So the original text, I need this. I'm bored looking at the first three words of this, to be honest. The annual financial report details the company's performance over the past year. Highlighting a 20% increase. In terms of potential outputs, the annual report highlights growth in sales revenue and customer acquisition. Includes several financial details. It discusses profit margins and revenue growth. The company experienced a successful year, but the details are vague in the report. So I think one is\u2026 is better and less vague, to be honest. But once again, these are tasks that you need to think about with respect to your particular use cases. So feel free to discuss that in Discord as well. With rag if the document was the Eiffel Tower was completed in 1889 for the World's Fair in Paris. It stands 324 meters high. The query is, when was the Eiffel Tower built. And why. And you'll notice, like, even when I built this first app to share with you all. Like, I tried to give it simple questions. I try not to. So I interview a lot of people for podcasts and stuff, right. And I always break the interviewer's number one rule with Which is don't ask two questions at once. Right. Also, like when you're emailing someone for someone and you've got two requests. Send different emails because people love to reply to one request. When there are several. But so asking two questions may not be the smartest idea, but the outputs, the Eiffel Tower was built in 1889, correct, as part of the world's Fair. I think that is concise and comprehensive, actually. Eiffel Tower was constructed for the world's Fair extraneous information. Number three doesn't even answer the question, okay. So we're really navigating the different dimensions possible here. These types of responses you may want to give to an LLM as a judge as well. Then thinking about generative tasks. I want to return a product I bought last week, but I lost the receipt. Thank you for reaching out. Unfortunately, we can't process. Thank you for contacting us. Please provide more details. We cannot help you with the return, okay. So it's actually interesting to think through which of these is the best. And I actually think the best without any more information, I would go with number two Having said that, number three might be the best if that's the truth, right. What I would prefer is we cannot help you with your return. Any further questions. Give us your email address or customer service can reach out to you okay These are just general kind of ways of recognizing what may be important to you and then what you want to prioritize when you're building out your products. Now thinking through different types of evaluation methods. There are academic ones which can be useful and are used a lot in industry that I just wanted to Bring your attention to if you haven't seen them. Their nop metrics such as blur for bilingual evaluation understudy which was originally designed for translation but is used for translation for when you have Australia hardship outputs and when you have clear reference texts. Rouge is something that's used a lot for summarization and it measures overlap between output. An input, and there are multiple variants And it's best for summarization tasks, content comparison, and when you really need to preserve key information. So those are academic. How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones.", "metadata": {"original_chunk_id": "6d14bc2a-b730-4b3c-966e-f25733f8eeed", "timestamp": "Chunk 15", "workshop_id": "WS3", "position": 14, "token_count": 1068, "source": "sentence_chunking", "speaker": "Unknown"}, "id": "WS3_6d14bc2a-b730-4b3c-966e-f25733f8eeed", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"source": "sentence_chunking", "workshop_id": "WS3", "token_count": 1049, "position": 15, "timestamp": "Chunk 16", "speaker": "Unknown", "original_chunk_id": "ef57fa87-2177-44f5-bab8-24ccb6a587f7"}, "id": "WS3_ef57fa87-2177-44f5-bab8-24ccb6a587f7", "relevance": 1.0}, {"text": "How does this play out in practice okay Thinking about correctness and thinking about Accuracy. Consider this product description for new wireless headphones. So generating this product description. Our wireless headphones feature 20 hour battery life, Bluetooth connectivity and active noise cancellation compatible with iOS and Android. Then, which seems like a good product description to me. Output 2 seems like a marketing. Product description. So our revolutionary headphones use quantum technology to provide infinite battery life and telepathic. Clearly, I'm taking\u2026 taking the quote unquote mickey Mickey there. But these are the types of things you can think about giving to your LLM as judge for positive and a few shot examples as well. And similarly, I think. We don't need to go through all of these, but thinking through what task completion would look like and what would be important to you there. Thinking about format and structure. And once again, when you scale this to an evaluation harness. With format and structure, maybe you don't want the sophistication of an LLM as a judge and the costs you incur. With LLM as a judge, but you actually want to make sure your structured output, that you actually get the structured output you wanted using you know some some form of testing. Style and tone can be a lot tougher. And as we know. You can prompt in a system prompt or a user prompt the type of style and tone you would want. But that doesn't necessarily catch everything. So perhaps you do want to have guardrails after the fact to make sure no cuffs or swear words were used, right. As we talked about at the end of Last workshop. And as Stefan will go further into later this week. And then as you build out more sophisticated products and pipelines, you perhaps really want to think about it across several dimensions. And there are trade-offs there. But make sure you have like your North Star and your North metric. But keep in mind what's happening across all the other different aspects. That may be of interest. So\u2026 Yeah, there are a lot more things to keep in mind and I won't go through all of these now, but this notebook, please do keep on hand. But yeah, length analysis. You want to monitor present usage of required terminology, forbidden words. And when we go into guardrails and testing later this week, we'll see a bit more of that. There are business requirements. So as we've been talking about the whole time, once you have a human judgment What I'd love is for\u2026 me and Jeff to have the same rubric So that if we're human annotating the same responses that we give the same response, right. And if we don't, then we can align on what and it'll be fascinating to see why we have different responses there because there's a mismatch between Communicate then, right. And that will help us when starting um to scale to LM as a judge. What type of criteria we want it to give. Once again, feel free to create like these types of granular evaluations. But in the end, you want a plus one minus one. You don't want a dashboard full of, you know. One to five responses, okay. You want to calibrate with human evaluations And then you want to monitor performance. So track agreement with humans, watch for drift. Check for biases and then start to do validation on on edge cases, okay. So\u2026 Yeah, I do want to just quickly before we, Hamill joins to to go to the next notebook, which, as I said, I'll record a video on but When thinking about scaling evaluation. Don't do it at the start. Start small. Begin with human evaluation, establish baselines, create clear criteria that if Jeff implemented and I did. Almost get the same result. Then you scale gradually. Start with low risk evaluations. Maintain human oversight, monitor performance. When my former colleagues rolled out new recommendation systems at Netflix. I'm not saying you shouldn't sleep for days. They didn't, right. They were also getting paid Netflix salaries. So they were arguably paid to not sleep. But when you first launch something You need to build trust with your system, right. And how do you do that. You look at it, you observe it, you maintain oversight and monitor performance. And then you have regular calibration. So you check your system against human judgments. Update the criteria as needed. And you document the changes, okay. Like in the first days of launching a product, your eyes are glued to the dashboard or whatever, the traces the whole time. Then perhaps it's once every few hours. Then weeks later, it's once a day and so on as you build trust with this system. Now, remember, this isn't classic software. This is software that takes in the entropy of the real world and real world data. It's actually, I know we think of computation and models as cold and computational but there is something somewhat organic about these systems as well in that they take in a lot of kind of Organic information about the world and incorporated within them. So we do need to keep an eye on them, right.", "metadata": {"original_chunk_id": "62ef5b55-d219-45ff-a564-75ead9264997", "speaker": "Unknown", "workshop_id": "WS3", "timestamp": "Chunk 16", "source": "sentence_chunking", "position": 15, "token_count": 1049}, "id": "WS3_62ef5b55-d219-45ff-a564-75ead9264997", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"speaker": "Unknown", "token_count": 1062, "timestamp": "Chunk 11", "source": "sentence_chunking", "workshop_id": "WS3", "original_chunk_id": "f6b9c1a9-8aab-43ad-bb21-151344a2defc", "position": 10}, "id": "WS3_f6b9c1a9-8aab-43ad-bb21-151344a2defc", "relevance": 1.0}, {"text": "It was supposed to be john it said john doe supposed to be jane smith file. In this case, maybe you'd classify it as a hallucination, but if you're doing information retrieval. You might want to classify it as a retrieval error because these will have different solutions, right. These two were accepted. These were correct. This one is summarized doc. The report discusses sales. This is a great example actually of something I mentioned earlier, which is this is correct. But it's not complete. It's concise and correct and not complete. Big fail for an incomplete response. Now, let's say I do 20 to 50 of these. Then I can do a pivot table. And I'm so sorry for the amount of times I've said pivot table today, but it is so important. Suddenly. I see that hallucinations are 30% retrieval errors 25% incomplete response 20, formatting issues, 15. So funnily. Some formatting issues are sometimes the easiest to see, right. And you may go, oh, I'll try to fix those But now we've done this in like a few hours, whatever it is, we actually see that formatting issues will only give us 15 fixing them will only give us 15% lift on our model. I mean, we do want to prioritize them at some point. But perhaps working on these higher impact ones we should be doing first, okay. Then. Yes, then, Hugo. How do we build our evaluation harness. Leveraging the idea of LLM as a judge So\u2026 This is a minimal example. This is a minimum viable LLM as judge, okay. And this is the type of thing I want you to get a lot of practice with is just doing As you iterate, building small, you've got your minimum viable product You've got your minimum viable test set with hand annotations. Now it's your minimum viable evaluation harness. What I'm telling it here is I'm giving it good examples and saying why they're good. And once again, this is in the GitHub repository. Do I need to zoom in at all or is that We go\u2026 Okay, great. So I'm giving you some good examples, giving it some bad examples, right. Now, sometimes it will over index on the good examples and the bad examples. So you want to iterate and align your elements judge. But essentially what I'm saying it to here, saying to it here. I give it the good examples and bad examples and now i say I give it evaluation criteria. A response is acceptable if it directly answers the question with specific details. It is factually correct based on the workshop content. It avoids hallucinations or made up information. A response is unacceptable if. Now, as I said before. I was giving plus one and minus one like a random do, you know. And in fact, and this is something I mentioned when I shipped the app to you um The first evaluation, we weren't even checking whether it's grounded in In the workshop it's like, did this feel good or bad, right. And that's fine at the start but in the end. You want it to be grounded in the docs if it's an information retrieval system. Funnily, this is a really bad idea in this case. I think this second point here Because I don't think I've given this LLM as a judge, the workshop transcript. So it's going to hallucinate things around that. So once again, I'm discovering things out about my MVP as an MV, as I run it and use it. But I think one point remains here, and this is something that I find so exciting is It's kind of like when you hire an intern or something and you start telling them like how you do a task you do and you're like, oh, wait, that's actually how I do it. Let's write it down. And you're like, oh, that's my process. And this is the criteria I use. Writing things down, communication helps you clarify what you're actually doing yourself. So even if you don't use like a large scale LM as a judge, writing down this type of thing can be incredibly useful. So when I did the LM as a judge, you can see the JSON viewer messed up a bit for me, but And I'll explain that in a second. But this is a question from the synthetic data set. This isn't an open AI evaluation and a Gemini evaluation. That's where the JSON the vibe coding JSON viewer messed up. What it is, is it's a model, it's an AI system, a rag system with open ai and a rag system with Gemini and I'm getting OpenAI to evaluate both of them, okay. So this was a fail. According to workshop one, prompt engineering can be approached by logging inputs and outputs, evaluating logs and so on. It is important to leverage domain expertise to evaluate additionally involving the person who used to write the emails in the case of generating emails can help in evaluating the quality of the output. I love this. This response. And it actually says the response does not directly answer the question with specific details from workshop one.", "metadata": {"original_chunk_id": "31a5b21e-aee1-4483-9460-02f7c0e75795", "source": "sentence_chunking", "speaker": "Unknown", "token_count": 1062, "timestamp": "Chunk 11", "workshop_id": "WS3", "position": 10}, "id": "WS3_31a5b21e-aee1-4483-9460-02f7c0e75795", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"timestamp": "Chunk 9", "token_count": 1012, "source": "sentence_chunking", "speaker": "Unknown", "original_chunk_id": "79e3ed2f-8447-44f2-a41f-7288f51c8da7", "position": 8, "workshop_id": "WS4"}, "id": "WS4_79e3ed2f-8447-44f2-a41f-7288f51c8da7", "relevance": 1.0}, {"text": "Does that answer the question. I think so, yeah. So I just need to read about PyTest Harvest. It kind of does these things. Behind the scenes. Yeah, yeah. And specifically, I would say PyTest plugins because that's kind of They do a little bit of magic to just auto load stuff If it's in the environment, else it will kind of complain. I was just going to say like, I've been using PyTest for a while at my job and like Sometimes the fact that it automatically inserts like params to the function name like It can be really counterintuitive and sometimes it still surprises me if it's especially if it's a new plugin that you don't know about and all of a sudden you're like, well, why is This arg in my function and you have to make sure it's named the thing that the plugin expects it to be so Yeah, it's worth ringing me up on but it is also just something you kind of sometimes it breaks and you're like, why. And you're like, oh, it's because um The plugin loaded something under this name. Yep. But yeah, but yeah, I guess\u2026 Meta point, shouldn't be too fixated on PyTest, but hopefully the conceptual structure of like, this is what you can do to set up something systematic that takes workshop three and you know puts it into something that is a little more software engineering and kind of shape. Hey, Stefan. Just a heads up that we're nearly halfway through. Yeah, cool. Yep. Cool. All right. With that, I don't think I have\u2026 Any more to share on this. Let me flip back to Don't touch on. All right. Here, so\u2026 Let's see. Sorry. Discussions and questions. I think some of the questions that you guys kind of brought up, I think, touched on this. So in the repo I did give an example of like, you know, a CI implementation. So GitHub, if you don't know, allows you to write workflows. These can be run on, say, every pull request. And so I wrote an example one that exercises High test logic. A logic.py has been changed. Obviously, you can make this run when any files in the repository have changed, but here I've just done being very specific. And so\u2026 This is an example of a CI pipeline. Fairly basic. All it's doing is just running by test logic now Depending on what you're doing, like this does output a CSV file like either in the test logic pi itself or outside of it, like this is where you can kind of connect it to like, you know. Uploading results to Google Sheets or um if you really want this pipeline to fail, like you could have it ping something and wait for a callback to say some human looked at things, yes or no, this is where your imagination and regular software engineering skills can kind of help you come up with the right process for you that works internally. I think we covered this question as well. So should you aim to never have failures Right. And Hamill, I think, in his wisdom, the way that he kind of frames it is like, if your tests are passing 100%, like what are they testing. Right. And so to me, there is a bit of a, the things you want 100% are the things like you always want your application to behave and do. But then there's just going to be this kind of gray area of things right which is kind of like you don't want to stop your push to production But you do want to longitudinally potentially measure or track or have some sort of data point so that you can go like, hey, something looks weird. And then you can look back, hey, look at our last releases. This particular accuracy measure has actually been fluctuating a little bit or now it's been dipping down, right. And so this is where This is where a bit of the data sciencey type aspect comes into measuring and figuring out where things are going but But then with this podcast set up, right, at least you can set up a way that you can measure and track these artifacts with these CSV files. If you need to dig into more, you do have the data. Okay. Question for you all. Llms cost money and potentially depending if your eval set is a thousand things you really want to do A thousand calls to an LLM, And then if you think about, you know, it's not just one LM call it's multiple Does anyone have any ideas or suggestions as to what the the best way potentially to keep track of costs associated with testing because i'm sure uh you know that's going to be whoever is in finance or whoever's enabling you It's probably going to ask you know Who's using all the credits. Like its own keys and you can track it down to the At the key level. Yep. Yeah.", "metadata": {"token_count": 1012, "timestamp": "Chunk 9", "speaker": "Unknown", "position": 8, "workshop_id": "WS4", "original_chunk_id": "d492c886-fee2-48a6-b289-95214ec3d247", "source": "sentence_chunking"}, "id": "WS4_d492c886-fee2-48a6-b289-95214ec3d247", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"position": 10, "speaker": "Unknown", "token_count": 1050, "original_chunk_id": "85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "timestamp": "Chunk 11", "workshop_id": "WS1", "source": "sentence_chunking"}, "id": "WS1_85a2abcf-264b-478f-b9c8-4a4eeb84cc2c", "relevance": 1.0}, {"text": "Okay, so what's cool, you can ask, is the information extracts from Linkedin. Correct. That's a cool question to ask and important, that needs to be satisfied. But remember, we're trying to scale human behavior so, and we want it to result in the same outcomes. But better right. So the next question you might ask, do the emails generated resemble the emails domain experts sent previously. 172 hugo bowne-anderson: also a good question to ask, and who would have thought trying to get a domain expert involved would be so so useful. But it turns out it is now 173 hugo bowne-anderson: at a very macro level, though 174 hugo bowne-anderson: you really want to know. Do the emails result in hiring good candidates. And is it more efficient and more cost effective than our previous system right. And it's actually not even clear how long these cycles are. So making sure that we in the end tie these macro level 175 hugo bowne-anderson: business evaluations to the micro level Llm calls is super important. 176 hugo bowne-anderson: So that brings us back to this. How do we. How do we evaluate a system like this. I don't know what the actual prompt sent to the Llm. Was. How do I give feedback on on system performance. How can I see data and traces and conversations. How can we iterate quickly on this. Mvp. 177 hugo bowne-anderson: And 178 hugo bowne-anderson: the answer, the answers come in the in the process that we're here to teach you about in this entire course. And one thing to understand is kind of the flow of the system. So 179 hugo bowne-anderson: we have our upstream prompt and context being sent to an Llm. Then we have the Llm output, and this being consumed downstream in a variety of ways. 180 hugo bowne-anderson: So what type of principles do we need to really think about then. And another way to frame what we're doing here is, of course, we're using tools the entire time. But we're not tool focused. We're, we're principle focused. And 181 hugo bowne-anderson: one of the reasons that's the case is 182 hugo bowne-anderson: we wanna help you develop skills and adopt tools that we'll be around in 183 hugo bowne-anderson: several years. And I honestly can't guarantee you whether Langchain or Llama Index or crew, AI or any of these things will be around to be honest. But what Stefan and myself and a lot of our community are taking a bet on is that the principles that we learned building machine learning software for years and have started to use building Llm powered software, they will stick around. And so 184 hugo bowne-anderson: the 1st principle to recognize is upstream. You have these Api calls you have input and output. And you have knobs. You can tweak there. Right. You have system and user prompts. You have context, right. You have structured outputs, and you have a variety of knobs, such as temperature and top P, which we'll get to later this week. 185 hugo bowne-anderson: Another principle and this is a foundational principle. All of these are is that Llms are non-deterministic. Same inputs 186 hugo bowne-anderson: can result in different outputs. This is an issue for software. Not only that, of course, is rarely. Do you even have the same inputs when you deploy to to users. The types of things you'll see are absolutely wild. You could, I talked about generating synthetic data to mimic users hands down. If you've ever built a conversational software product, you know the types of wild, wild stuff. That people will say that you could never, never even think of 187 hugo bowne-anderson: I do love. I haven't done this, but I do know people who who essentially found 188 hugo bowne-anderson: what they call free Gpt. Maybe you all have done this as well. But you can, you know. 189 hugo bowne-anderson: find a product that has a chat bot, or something that actually allows you to just chat with Chat Gbt within it. Essentially, because I haven't put up proper guardrails right. But people will put input in to do all types of things. Now. 190 hugo bowne-anderson: one of the most important principles is. 191 hugo bowne-anderson: and I almost feel silly having to say this, but you'd be amazed how few people think about this robustly and do it. You need to log and monitor and trace. You need to capture your data in a human readable form as well. And then you need to evaluate, evaluate early evaluate.", "metadata": {"source": "sentence_chunking", "speaker": "Unknown", "position": 10, "timestamp": "Chunk 11", "workshop_id": "WS1", "original_chunk_id": "e36c674b-62a6-4f10-8bab-781baf5547c3", "token_count": 1050}, "id": "WS1_e36c674b-62a6-4f10-8bab-781baf5547c3", "relevance": 1.0}]}}