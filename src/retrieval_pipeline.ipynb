{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaa9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import glob\n",
    "from process_transcript import chunk_workshop_transcript, count_tokens, robust_chunk_workshop\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# Support both local and Modal paths - notebook compatible version\n",
    "# Check for Modal environment first, then use relative paths for local notebook\n",
    "if os.path.exists(\"/root/data\"):\n",
    "    DATA_DIR = \"/root/data\"\n",
    "    CHROMA_DB_PATH = \"/root/chroma_db\"\n",
    "else:\n",
    "    # For notebook environment, go up one directory from src to project root\n",
    "    current_dir = os.getcwd()\n",
    "    if current_dir.endswith('/src'):\n",
    "        project_root = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    DATA_DIR = os.path.join(project_root, \"data\")\n",
    "    CHROMA_DB_PATH = os.path.join(project_root, \"chroma_db\")\n",
    "\n",
    "COLLECTION_NAME = \"workshop_chunks_all\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "DEFAULT_MAX_TOKENS = 12000\n",
    "DEFAULT_MAX_CHUNKS = 10\n",
    "COMPLETION_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MAX_TOKENS = 7000\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful workshop assistant.\n",
    "Answer questions based only on the workshop transcript sections provided.\n",
    "If you don't know the answer or can't find it in the provided sections, say so.\n",
    "When referencing information, mention which workshop(s) the information comes from.\"\"\"\n",
    "\n",
    "def discover_workshops(data_dir=DATA_DIR):\n",
    "    \"\"\"Discover all workshop VTT files in the data directory\"\"\"\n",
    "    try:\n",
    "        pattern = os.path.join(data_dir, \"*.vtt\")\n",
    "        vtt_files = glob.glob(pattern)\n",
    "        \n",
    "        workshops = {}\n",
    "        for vtt_file in vtt_files:\n",
    "            filename = os.path.basename(vtt_file)\n",
    "            workshop_id = filename.split('-')[0] if '-' in filename else filename.split('.')[0]\n",
    "            \n",
    "            workshops[workshop_id] = {\n",
    "                'id': workshop_id,\n",
    "                'filename': filename,\n",
    "                'path': vtt_file\n",
    "            }\n",
    "        \n",
    "        return workshops\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering workshops: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d11a03-090e-46a3-b625-e09cb96a355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d298eaa0-990b-4d6b-a326-370f587c2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/home/pastor/projects/discord-chat-bot/eval/test_llm_judge.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5013802c-34d9-4114-9528-9d51816a81c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Are there any guest lectures?',\n",
       "       'Did anyone talk about builders in residence?',\n",
       "       'Does anybody mention the Builders in Residence Program?',\n",
       "       'Does anyone talk about a builders in residence program here?',\n",
       "       'How long is the course?',\n",
       "       'How many attended this workshop? I believe Hugo mentioned that at the beginning of the workshop. ',\n",
       "       'How many speakers this workshop has?',\n",
       "       'I mean synthetic data generated questions',\n",
       "       'I mean synthetically generated questions',\n",
       "       'Is evaluation driven development mentioned in this workshop at all? If so, how many times?'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(df['question'].values)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e07dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_vtt = discover_workshops(data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ed7412e-7f0b-4736-b026-7a5a43530a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pastor/projects/discord-chat-bot/data'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c719eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pastor/projects/discord-chat-bot/data/WS5-C2.vtt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_vtt['WS5']['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfc019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vtt_content(file_path):\n",
    "    \"\"\"Load VTT file and extract clean text content\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    content_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if (not line or \n",
    "            line == 'WEBVTT' or \n",
    "            '-->' in line or \n",
    "            re.match(r'^\\d+:\\d+:\\d+', line) or\n",
    "            re.match(r'^[A-Z]+(\\s*:.*)?$', line)):\n",
    "            continue\n",
    "        content_lines.append(line)\n",
    "    \n",
    "    return \" \".join(content_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6614546b-286a-4eb6-93fb-f339d0493730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(encoding.encode(text))\n",
    "    except:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d7da19-b0c2-4ee5-89e0-3b5c036c605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_chunk(text: str, max_tokens: int = EMBEDDING_MAX_TOKENS) -> List[str]:\n",
    "    \"\"\"Split a large chunk into smaller pieces that fit within token limits\"\"\"\n",
    "    if count_tokens(text) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    sentences = re.split(r'[.!?]+\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        if sentence_tokens > max_tokens:\n",
    "            words = sentence.split()\n",
    "            word_chunk = \"\"\n",
    "            word_tokens = 0\n",
    "            \n",
    "            for word in words:\n",
    "                word_token_count = count_tokens(word)\n",
    "                if word_tokens + word_token_count > max_tokens and word_chunk:\n",
    "                    chunks.append(word_chunk.strip())\n",
    "                    word_chunk = word\n",
    "                    word_tokens = word_token_count\n",
    "                else:\n",
    "                    word_chunk += \" \" + word if word_chunk else word\n",
    "                    word_tokens += word_token_count\n",
    "            \n",
    "            if word_chunk:\n",
    "                chunks.append(word_chunk.strip())\n",
    "            continue\n",
    "        \n",
    "        if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk += \". \" + sentence if current_chunk else sentence\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66e9782-096b-4450-b9d4-a1133752746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding for text with safe token splitting\"\"\"\n",
    "    client = get_openai_client()\n",
    "    \n",
    "    token_count = count_tokens(text)\n",
    "    \n",
    "    if token_count <= EMBEDDING_MAX_TOKENS:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=EMBEDDING_MODEL\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    else:\n",
    "        split_texts = split_large_chunk(text, EMBEDDING_MAX_TOKENS)\n",
    "        \n",
    "        embeddings = []\n",
    "        for split_text in split_texts:\n",
    "            response = client.embeddings.create(\n",
    "                input=split_text,\n",
    "                model=EMBEDDING_MODEL\n",
    "            )\n",
    "            embeddings.append(response.data[0].embedding)\n",
    "        \n",
    "        avg_embedding = np.mean(embeddings, axis=0).tolist()\n",
    "        return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3879f29c-c1c6-4d4b-ab4b-c281901cb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_client():\n",
    "    \"\"\"Initialize OpenAI client with API key\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "    return OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "199fe580-e1c1-41e8-bd44-2a4474659836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50345/4281037847.py:36: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "/tmp/ipykernel_50345/4281037847.py:43: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI()\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Step 1: Load and preprocess VTT documents without external dependencies\n",
    "def load_vtt_files(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".vtt\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                # Remove timestamps, numbering, and metadata\n",
    "                content = re.sub(r'(\\d+:\\d+:\\d+\\.\\d+ --> \\d+:\\d+:\\d+\\.\\d+|WEBVTT|\\d+)\\n?', '', content)\n",
    "                content = re.sub(r'\\n+', ' ', content).strip()\n",
    "                documents.append(content)\n",
    "    return documents\n",
    "\n",
    "# Step 2: Split documents into chunks\n",
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(splitter.split_text(doc))\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Embedding and indexing with ChromaDB\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxx\"\n",
    "\n",
    "def create_chroma_db(chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    chroma_db = Chroma.from_texts(chunks, embeddings)\n",
    "    return chroma_db\n",
    "\n",
    "# Step 4: Create the RAG retrieval QA chain\n",
    "def create_rag_system(chroma_db):\n",
    "    retriever = chroma_db.as_retriever()\n",
    "    llm = OpenAI()\n",
    "    rag = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "    return rag\n",
    "\n",
    "# Example Usage\n",
    "vtt_directory = DATA_DIR  # Update with your path\n",
    "\n",
    "# Load and process documents\n",
    "documents = load_vtt_files(vtt_directory)\n",
    "chunks = chunk_documents(documents)\n",
    "\n",
    "# Create and populate ChromaDB\n",
    "chroma_db = create_chroma_db(chunks)\n",
    "\n",
    "# Create RAG system\n",
    "rag_system = create_rag_system(chroma_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f5864c1-0352-486a-8c68-d03e93afe0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workshop 8 is about building a minimum viable evaluation framework and using prompt engineering in the software development life cycle. It also covers topics such as format and structure, style, tone, length appropriateness, and safety in the context of building a product. There will also be guest speakers discussing their work with LLMs and human-in-the-loop development.\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system\n",
    "query = \"what workshop 8 is about?\"\n",
    "response = rag_system.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9a94a-dca1-4db1-bbb8-f3883f28447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the RAG system\n",
    "query = \"Your question here?\"\n",
    "response = rag_system.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c729cf-38e5-42cf-b5e5-ac1d4847421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "DEFAULT_MAX_TOKENS = 12000\n",
    "DEFAULT_MAX_CHUNKS = 5\n",
    "COMPLETION_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MAX_TOKENS = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e7020e-9e0a-48b0-beb6-fe80a571220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_client():\n",
    "    \"\"\"Initialize and return a ChromaDB client with persistence\"\"\"\n",
    "    os.makedirs(CHROMA_DB_PATH, exist_ok=True)\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "    return client\n",
    "\n",
    "def get_or_create_collection(client, collection_name=COLLECTION_NAME):\n",
    "    \"\"\"Get or create a collection in ChromaDB\"\"\"\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "    except:\n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"Workshop transcript chunks\"}\n",
    "        )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0a1170b-6527-4d32-8be0-45610b5ac885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk(text: str, position: int, workshop_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create a chunk dictionary with metadata\"\"\"\n",
    "    return {\n",
    "        \"chunk_id\": str(uuid.uuid4()),\n",
    "        \"text\": text.strip(),\n",
    "        \"position\": position,\n",
    "        \"token_count\": count_tokens(text),\n",
    "        \"source\": \"sentence_chunking\",\n",
    "        \"timestamp\": f\"Chunk {position + 1}\",\n",
    "        \"speaker\": \"Unknown\",\n",
    "        \"workshop_id\": workshop_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30005558-c92d-4854-84e1-25970536a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_workshop(file_path: str, workshop_id: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Simple, reliable chunking strategy with sentence-based splitting\"\"\"\n",
    "    text = load_vtt_content(file_path)\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+\\s+', text) if s.strip()]\n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "    position = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        if current_tokens + sentence_tokens > CHUNK_SIZE and current_chunk_sentences:\n",
    "            chunk_text = \". \".join(current_chunk_sentences) + \".\"\n",
    "            chunk = create_chunk(chunk_text, position, workshop_id)\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            overlap_sentences = current_chunk_sentences[-2:] if len(current_chunk_sentences) >= 2 else current_chunk_sentences\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = count_tokens(\". \".join(current_chunk_sentences))\n",
    "            position += 1\n",
    "        else:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = \". \".join(current_chunk_sentences) + \".\"\n",
    "        chunk = create_chunk(chunk_text, position, workshop_id)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    valid_chunks = [chunk for chunk in chunks if chunk['token_count'] >= MIN_CHUNK_SIZE]\n",
    "    return valid_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a91bcae3-ff6c-40b5-b94c-2ca72002c66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKSHOP 1\n",
      "Created 30 chunks for workshop 1\n",
      "Added 30 chunks to database\n",
      "--------------------------------------------------\n",
      "WORKSHOP 2\n",
      "Created 14 chunks for workshop 2\n",
      "Added 14 chunks to database\n",
      "--------------------------------------------------\n",
      "WORKSHOP 3\n",
      "Created 24 chunks for workshop 3\n",
      "Added 24 chunks to database\n",
      "--------------------------------------------------\n",
      "WORKSHOP 4\n",
      "Created 26 chunks for workshop 4\n",
      "Added 26 chunks to database\n",
      "--------------------------------------------------\n",
      "WORKSHOP 5\n",
      "Created 27 chunks for workshop 5\n",
      "Added 27 chunks to database\n",
      "--------------------------------------------------\n",
      "WORKSHOP 6\n",
      "Created 29 chunks for workshop 6\n",
      "Added 29 chunks to database\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB\n",
    "CHUNK_SIZE = 1000\n",
    "MIN_CHUNK_SIZE = 200\n",
    "\n",
    "client = get_chroma_client()\n",
    "collection = get_or_create_collection(client)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    print(f\"WORKSHOP {i}\")\n",
    "    workshop_id = f\"WS{i}\"\n",
    "    workshop_path = files_vtt[f'WS{i}']['path']\n",
    "    \n",
    "    # Instead of loading raw VTT, use proper chunking\n",
    "    chunks = chunk_workshop(workshop_path, workshop_id)\n",
    "    print(f\"Created {len(chunks)} chunks for workshop {i}\")\n",
    "    \n",
    "    # Add chunks to ChromaDB (this handles embedding generation)\n",
    "    num_added = add_chunks_to_collection(collection, chunks, workshop_id)\n",
    "    print(f\"Added {num_added} chunks to database\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be0ddcc3-0f34-4290-9d15-c6e939f8c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(chunks, max_tokens=DEFAULT_MAX_TOKENS):\n",
    "    \"\"\"Combine multiple chunks into a single context\"\"\"\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    \n",
    "    sorted_chunks = sorted(chunks, key=lambda x: int(x['metadata'].get('position', 0)))\n",
    "    \n",
    "    combined_text = \"\"\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for chunk in sorted_chunks:\n",
    "        chunk_text = chunk['text']\n",
    "        chunk_tokens = int(chunk['metadata'].get('token_count', 0))\n",
    "        \n",
    "        if chunk_tokens == 0:\n",
    "            chunk_tokens = count_tokens(chunk_text)\n",
    "        \n",
    "        if total_tokens + chunk_tokens > max_tokens:\n",
    "            break\n",
    "        \n",
    "        if combined_text:\n",
    "            combined_text += \"\\n\\n--- Next Section ---\\n\\n\"\n",
    "        \n",
    "        combined_text += chunk_text\n",
    "        total_tokens += chunk_tokens\n",
    "    \n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "163210b4-f034-489c-b305-87c9cd297783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(collection, query_text, n_results=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Query the collection for relevant documents\"\"\"\n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    \n",
    "    query_params = {\n",
    "        \"query_embeddings\": [query_embedding],\n",
    "        \"n_results\": n_results\n",
    "    }\n",
    "    \n",
    "    if workshop_filter:\n",
    "        if isinstance(workshop_filter, str):\n",
    "            query_params[\"where\"] = {\"workshop_id\": workshop_filter}\n",
    "        elif isinstance(workshop_filter, list):\n",
    "            query_params[\"where\"] = {\"workshop_id\": {\"$in\": workshop_filter}}\n",
    "    \n",
    "    results = collection.query(**query_params)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c89ef79b-7afa-47af-81fd-b164dbc05b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(question, collection_name=COLLECTION_NAME, n_results=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Retrieve chunks from vector database for a given question\"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, collection_name)\n",
    "    \n",
    "    results = query_collection(collection, question, n_results=n_results, workshop_filter=workshop_filter)\n",
    "    \n",
    "    chunks = []\n",
    "    if results and 'documents' in results and results['documents'] and len(results['documents'][0]) > 0:\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            chunk = {\n",
    "                'text': results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'id': results['ids'][0][i],\n",
    "                'relevance': 1.0\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fc3305f-e97e-47e0-b0ef-8345fa3985fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_for_question(question, collection_name=COLLECTION_NAME, max_chunks=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Get relevant context from the vector database for a question\"\"\"\n",
    "    chunks = retrieve_relevant_chunks(question, collection_name, max_chunks, workshop_filter)\n",
    "    \n",
    "    sources = []\n",
    "    for chunk in chunks:\n",
    "        metadata = chunk['metadata']\n",
    "        text = chunk['text']\n",
    "        \n",
    "        source = {\n",
    "            'position': metadata.get('position', 'Unknown'),\n",
    "            'timestamp': metadata.get('timestamp', \"Unknown\"),\n",
    "            'speaker': metadata.get('speaker', \"Unknown\"),\n",
    "            'workshop_id': metadata.get('workshop_id', \"Unknown\"),\n",
    "            'text': text,\n",
    "            'relevance': chunk.get('relevance')\n",
    "        }\n",
    "        sources.append(source)\n",
    "    \n",
    "    context = combine_chunks(chunks)\n",
    "    return context, sources, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dac84997-11c2-4d1f-a4eb-cf9fda2fbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, workshop_filter=None):\n",
    "    \"\"\"Answer a question based on workshop transcripts\"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, COLLECTION_NAME)\n",
    "    \n",
    "    # Check if collection is empty and populate if needed\n",
    "    count = collection.count()\n",
    "    if count == 0:\n",
    "        process_all_workshops(COLLECTION_NAME)\n",
    "    \n",
    "    context, sources, chunks = get_context_for_question(\n",
    "        question=question,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        max_chunks=DEFAULT_MAX_CHUNKS,\n",
    "        workshop_filter=workshop_filter\n",
    "    )\n",
    "    \n",
    "    return context, sources, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af1a7c24-2321-48ac-b6fa-4150b90b064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_answer_question(client, context, sources, chunks, question):\n",
    "    \"\"\"Generate LLM answer with workshop awareness\"\"\"\n",
    "    num_chunks = len(sources)\n",
    "    workshops_used = list(set([source.get('workshop_id', 'Unknown') for source in sources]))\n",
    "    \n",
    "    client = get_openai_client()\n",
    "    try:\n",
    "        enhanced_system_prompt = SYSTEM_PROMPT + f\"\\n\\nThe information provided comes from workshops: {', '.join(workshops_used)}.\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=COMPLETION_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": enhanced_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Workshop Transcript Sections:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message.content\n",
    "        \n",
    "        completion_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 0\n",
    "        prompt_tokens = response.usage.prompt_tokens if hasattr(response, 'usage') else count_tokens(context)\n",
    "        \n",
    "        context_info = {\n",
    "             \"num_chunks\": num_chunks,\n",
    "             \"context_tokens\": prompt_tokens,\n",
    "             \"completion_tokens\": completion_tokens,\n",
    "             \"embedding_tokens\": num_chunks * 1536,\n",
    "             \"workshops_used\": workshops_used,\n",
    "             \"chunks\": chunks\n",
    "         }\n",
    "        \n",
    "        return message, context_info\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Sorry, an error occurred: {str(e)}\"\n",
    "        return error_message, {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e593e9bb-0404-4e10-b111-29587b5d4687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant sources\n",
      "Question: what workshop 2 is about\n",
      "Answer: Workshop 2 focused on deploying the MVP (Minimum Viable Product) on Modal. It explored prompt engineering, API tuning, and multi-vendor APIs, including Gemini, OpenAI, and Anthropy. Additionally, participants were reminded to claim their credits if they hadn't already. This information comes from WS6.\n",
      "Workshops used: ['WS6', 'WS3']\n"
     ]
    }
   ],
   "source": [
    "question = \"what workshop 2 is about\"\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "context, sources, chunks = answer_question(question)\n",
    "print(f\"Found {len(sources)} relevant sources\")\n",
    "\n",
    "# Generate LLM response\n",
    "client = get_openai_client()\n",
    "response, context_info = llm_answer_question(client, context, sources, chunks, question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", response)\n",
    "print(\"Workshops used:\", context_info['workshops_used'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d22a92-79d2-40b3-8f34-734387141895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system\n",
    "question = \"How do I implement authentication?\"\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "context, sources, chunks = answer_question(question)\n",
    "print(f\"Found {len(sources)} relevant sources\")\n",
    "\n",
    "# Generate LLM response\n",
    "client = get_openai_client()\n",
    "response, context_info = llm_answer_question(client, context, sources, chunks, question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", response)\n",
    "print(\"Workshops used:\", context_info['workshops_used'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436c4bf-0efb-4530-aee2-ad128c90ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "for i in range(1, 7):\n",
    "    print(f\"WORKSHOP {i}\")\n",
    "    transcript_vtt = load_vtt_content(files_vtt[f'WS{i}']['path'])\n",
    "    print(len(transcript_vtt))\n",
    "    print(count_tokens(transcript_vtt))\n",
    "    chunk = split_large_chunk(transcript_vtt)\n",
    "    print(chunk[-1][-100::])\n",
    "    print(f\"transcript {i} {transcript_vtt[-100:]}\")\n",
    "    print(\"embedding\")\n",
    "    emb_trans = generate_embedding(transcript_vtt)\n",
    "    print(len(emb_trans))\n",
    "    print(emb_trans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d46113-483d-4813-8357-cbdb8e9ef4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunks_to_collection(collection, chunks, workshop_id):\n",
    "    \"\"\"Add multiple chunks to the collection with workshop metadata\"\"\"\n",
    "    ids = []\n",
    "    documents = []\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            embedding = generate_embedding(chunk['text'])\n",
    "            chunk_id = f\"{workshop_id}_{chunk['chunk_id']}\"\n",
    "            \n",
    "            ids.append(chunk_id)\n",
    "            documents.append(chunk['text'])\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "            metadata = {\n",
    "                'workshop_id': workshop_id,\n",
    "                'position': chunk['position'],\n",
    "                'token_count': chunk['token_count'],\n",
    "                'source': chunk['source'],\n",
    "                'timestamp': chunk.get('timestamp', 'Unknown'),\n",
    "                'speaker': chunk.get('speaker', 'Unknown'),\n",
    "                'original_chunk_id': chunk['chunk_id']\n",
    "            }\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    if ids:\n",
    "        try:\n",
    "            collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            return len(ids)\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1be75-e055-4b91-9e11-53d946498877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chunk_transcript(file_path: str, workshop_id: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Simple, reliable chunking strategy with sentence-based splitting\"\"\"\n",
    "    text = load_vtt_content(file_path)\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+\\s+', text) if s.strip()]\n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "    position = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        if current_tokens + sentence_tokens > CHUNK_SIZE and current_chunk_sentences:\n",
    "            chunk_text = \". \".join(current_chunk_sentences) + \".\"\n",
    "            chunk = create_chunk(chunk_text, position, workshop_id)\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            overlap_sentences = current_chunk_sentences[-2:] if len(current_chunk_sentences) >= 2 else current_chunk_sentences\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = count_tokens(\". \".join(current_chunk_sentences))\n",
    "            position += 1\n",
    "        else:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = \". \".join(current_chunk_sentences) + \".\"\n",
    "        chunk = create_chunk(chunk_text, position, workshop_id)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    valid_chunks = [chunk for chunk in chunks if chunk['token_count'] >= MIN_CHUNK_SIZE]\n",
    "    return valid_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4284baf8-2e18-4500-a83c-2c5af09d6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workshop(workshop_data: Dict[str, str], collection_name: str = COLLECTION_NAME) -> int:\n",
    "    \"\"\"Process a single workshop\"\"\"\n",
    "    try:\n",
    "        workshop_id = workshop_data['id']\n",
    "        workshop_path = workshop_data['path']\n",
    "        \n",
    "        client = get_chroma_client()\n",
    "        collection = get_or_create_collection(client, collection_name)\n",
    "        \n",
    "        # Check if already processed\n",
    "        try:\n",
    "            existing_results = collection.query(\n",
    "                query_embeddings=[[0.0] * 1536],\n",
    "                n_results=1,\n",
    "                where={\"workshop_id\": workshop_id}\n",
    "            )\n",
    "            if existing_results and existing_results['ids'] and len(existing_results['ids'][0]) > 0:\n",
    "                return 0\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        chunks = simple_chunk_transcript(workshop_path, workshop_id)\n",
    "        \n",
    "        if not chunks:\n",
    "            return 0\n",
    "        \n",
    "        num_added = add_chunks_to_collection(collection, chunks, workshop_id)\n",
    "        return num_added\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing workshop {workshop_data.get('id', 'Unknown')}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def process_all_workshops(collection_name=COLLECTION_NAME):\n",
    "    \"\"\"Process all discovered workshops\"\"\"\n",
    "    workshops = discover_workshops()\n",
    "    \n",
    "    if not workshops:\n",
    "        return []\n",
    "    \n",
    "    processed_workshops = []\n",
    "    \n",
    "    for workshop_id, workshop_info in workshops.items():\n",
    "        print(workshop_id, workshop_info)\n",
    "        try:\n",
    "            num_chunks = process_workshop(workshop_info, collection_name)\n",
    "            print(num_chunks)\n",
    "            if num_chunks > 0:\n",
    "                processed_workshops.append(workshop_id)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return processed_workshops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "799fa9f6-59b4-4899-9273-5d1b67ba2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS5 {'id': 'WS5', 'filename': 'WS5-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS5-C2.vtt'}\n",
      "27\n",
      "WS2 {'id': 'WS2', 'filename': 'WS2-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS2-C2.vtt'}\n",
      "14\n",
      "WS1 {'id': 'WS1', 'filename': 'WS1-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS1-C2.vtt'}\n",
      "30\n",
      "WS3 {'id': 'WS3', 'filename': 'WS3-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS3-C2.vtt'}\n",
      "24\n",
      "WS4 {'id': 'WS4', 'filename': 'WS4-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS4-C2.vtt'}\n",
      "26\n",
      "WS6 {'id': 'WS6', 'filename': 'WS6-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS6-C2.vtt'}\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "all_wsh = process_all_workshops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "38739656-cd84-4c51-94d2-36bb77978b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(collection, query_text, n_results=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Query the collection for relevant documents\"\"\"\n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    \n",
    "    query_params = {\n",
    "        \"query_embeddings\": [query_embedding],\n",
    "        \"n_results\": n_results\n",
    "    }\n",
    "    \n",
    "    if workshop_filter:\n",
    "        if isinstance(workshop_filter, str):\n",
    "            query_params[\"where\"] = {\"workshop_id\": workshop_filter}\n",
    "        elif isinstance(workshop_filter, list):\n",
    "            query_params[\"where\"] = {\"workshop_id\": {\"$in\": workshop_filter}}\n",
    "    \n",
    "    results = collection.query(**query_params)\n",
    "    return results\n",
    "\n",
    "def retrieve_relevant_chunks(question, collection_name=COLLECTION_NAME, n_results=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Retrieve chunks from vector database for a given question\"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, collection_name)\n",
    "    \n",
    "    results = query_collection(collection, question, n_results=n_results, workshop_filter=workshop_filter)\n",
    "    print(results)\n",
    "    \n",
    "    chunks = []\n",
    "    if results and 'documents' in results and results['documents'] and len(results['documents'][0]) > 0:\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            chunk = {\n",
    "                'text': results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'id': results['ids'][0][i],\n",
    "                'relevance': 1.0\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ffb16201-389f-411b-a95e-bcb45a8b6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(chunks, max_tokens=DEFAULT_MAX_TOKENS):\n",
    "    \"\"\"Combine multiple chunks into a single context\"\"\"\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    \n",
    "    sorted_chunks = sorted(chunks, key=lambda x: int(x['metadata'].get('position', 0)))\n",
    "    \n",
    "    combined_text = \"\"\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for chunk in sorted_chunks:\n",
    "        chunk_text = chunk['text']\n",
    "        chunk_tokens = int(chunk['metadata'].get('token_count', 0))\n",
    "        \n",
    "        if chunk_tokens == 0:\n",
    "            chunk_tokens = count_tokens(chunk_text)\n",
    "        \n",
    "        if total_tokens + chunk_tokens > max_tokens:\n",
    "            break\n",
    "        \n",
    "        if combined_text:\n",
    "            combined_text += \"\\n\\n--- Next Section ---\\n\\n\"\n",
    "        \n",
    "        combined_text += chunk_text\n",
    "        total_tokens += chunk_tokens\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "def get_context_for_question(question, collection_name=COLLECTION_NAME, max_chunks=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Get relevant context from the vector database for a question\"\"\"\n",
    "    chunks = retrieve_relevant_chunks(question, collection_name, max_chunks, workshop_filter)\n",
    "    print(chunks)\n",
    "    \n",
    "    sources = []\n",
    "    for chunk in chunks:\n",
    "        metadata = chunk['metadata']\n",
    "        text = chunk['text']\n",
    "        \n",
    "        source = {\n",
    "            'position': metadata.get('position', 'Unknown'),\n",
    "            'timestamp': metadata.get('timestamp', \"Unknown\"),\n",
    "            'speaker': metadata.get('speaker', \"Unknown\"),\n",
    "            'workshop_id': metadata.get('workshop_id', \"Unknown\"),\n",
    "            'text': text,\n",
    "            'relevance': chunk.get('relevance')\n",
    "        }\n",
    "        sources.append(source)\n",
    "    \n",
    "    context = combine_chunks(chunks)\n",
    "    return context, sources, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "59d03755-0dd7-422a-b7b0-cdeeabf7f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, workshop_filter=None):\n",
    "    \"\"\"Answer a question based on workshop transcripts\"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, COLLECTION_NAME)\n",
    "    \n",
    "    # Check if collection is empty and populate if needed\n",
    "    count = collection.count()\n",
    "    if count == 0:\n",
    "        process_all_workshops(COLLECTION_NAME)\n",
    "    \n",
    "    context, sources, chunks = get_context_for_question(\n",
    "        question=question,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        max_chunks=DEFAULT_MAX_CHUNKS,\n",
    "        workshop_filter=workshop_filter\n",
    "    )\n",
    "    \n",
    "    return context, sources, chunks\n",
    "\n",
    "def llm_answer_question(client, context, sources, chunk, question):\n",
    "    \"\"\"Generate LLM answer with workshop awareness\"\"\"\n",
    "    num_chunks = len(sources)\n",
    "    workshops_used = list(set([source.get('workshop_id', 'Unknown') for source in sources]))\n",
    "    \n",
    "    client = get_openai_client()\n",
    "    try:\n",
    "        enhanced_system_prompt = SYSTEM_PROMPT + f\"\\n\\nThe information provided comes from workshops: {', '.join(workshops_used)}.\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=COMPLETION_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": enhanced_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Workshop Transcript Sections:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message.content\n",
    "        \n",
    "        completion_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 0\n",
    "        prompt_tokens = response.usage.prompt_tokens if hasattr(response, 'usage') else count_tokens(context)\n",
    "        \n",
    "        context_info = {\n",
    "             \"num_chunks\": num_chunks,\n",
    "             \"context_tokens\": prompt_tokens,\n",
    "             \"completion_tokens\": completion_tokens,\n",
    "             \"embedding_tokens\": num_chunks * 1536,\n",
    "             \"workshops_used\": workshops_used,\n",
    "             \"chunks\": chunks\n",
    "         }\n",
    "        \n",
    "        return message, context_info\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Sorry, an error occurred: {str(e)}\"\n",
    "        return error_message, {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "16d3a1bc-ddb1-4287-ad53-510ee1673543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Sorry, an error occurred: name 'chunks' is not defined\",\n",
       " {'error': \"name 'chunks' is not defined\"})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_answer_question(client, context, source, chunk, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cad11881-634f-437c-a469-ba3151bce275",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_chroma_client()\n",
    "collection = get_or_create_collection(client, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95a21e7a-47fd-48c7-b82c-4087089b72d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf16c2f-c8d8-49d0-9a28-22ee0af8f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_openai_client()\n",
    "\n",
    "token_count = count_tokens(text)\n",
    "\n",
    "if token_count <= EMBEDDING_MAX_TOKENS:\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "else:\n",
    "    split_texts = split_large_chunk(text, EMBEDDING_MAX_TOKENS)\n",
    "    \n",
    "    embeddings = []\n",
    "    for split_text in split_texts:\n",
    "        response = client.embeddings.create(\n",
    "            input=split_text,\n",
    "            model=EMBEDDING_MODEL\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224955e5-146d-414f-be44-a2c334be64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    query_embedding = generate_embedding(query_text)\n",
    "    \n",
    "    query_params = {\n",
    "        \"query_embeddings\": [query_embedding],\n",
    "        \"n_results\": n_results\n",
    "    }\n",
    "    \n",
    "    if workshop_filter:\n",
    "        if isinstance(workshop_filter, str):\n",
    "            query_params[\"where\"] = {\"workshop_id\": workshop_filter}\n",
    "        elif isinstance(workshop_filter, list):\n",
    "            query_params[\"where\"] = {\"workshop_id\": {\"$in\": workshop_filter}}\n",
    "    \n",
    "    results = collection.query(**query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e243ae8-14bd-42a5-a8f4-85c86530ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_chroma_client()\n",
    "collection = get_or_create_collection(client, collection_name)\n",
    "\n",
    "results = query_collection(collection, question, n_results=n_results, workshop_filter=workshop_filter)\n",
    "\n",
    "chunks = []\n",
    "if results and 'documents' in results and results['documents'] and len(results['documents'][0]) > 0:\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        chunk = {\n",
    "            'text': results['documents'][0][i],\n",
    "            'metadata': results['metadatas'][0][i],\n",
    "            'id': results['ids'][0][i],\n",
    "            'relevance': 1.0\n",
    "        }\n",
    "        chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b31a31-4e8c-4fbe-82b7-7433acfeea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    chunks = retrieve_relevant_chunks(question, collection_name, max_chunks, workshop_filter)\n",
    "    \n",
    "    sources = []\n",
    "    for chunk in chunks:\n",
    "        metadata = chunk['metadata']\n",
    "        text = chunk['text']\n",
    "        \n",
    "        source = {\n",
    "            'position': metadata.get('position', 'Unknown'),\n",
    "            'timestamp': metadata.get('timestamp', \"Unknown\"),\n",
    "            'speaker': metadata.get('speaker', \"Unknown\"),\n",
    "            'workshop_id': metadata.get('workshop_id', \"Unknown\"),\n",
    "            'text': text,\n",
    "            'relevance': chunk.get('relevance')\n",
    "        }\n",
    "        sources.append(source)\n",
    "    \n",
    "    context = combine_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09591b-9c72-4db3-b36a-62f6cb5c2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, COLLECTION_NAME)\n",
    "    \n",
    "    # Check if collection is empty and populate if needed\n",
    "    count = collection.count()\n",
    "    if count == 0:\n",
    "        process_all_workshops(COLLECTION_NAME)\n",
    "    \n",
    "    context, sources, chunks = get_context_for_question(\n",
    "        question=question,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        max_chunks=DEFAULT_MAX_CHUNKS,\n",
    "        workshop_filter=workshop_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4777636-58bb-4fad-8966-39f1b0a4b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b326c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(transcript_vtt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3dae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_wrap(string, n_chars=72):\n",
    "    # Wrap a string at the next space after n_chars\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_wrap(character_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2421119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48daeda7fc4f45838678fc7ce384a309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291e32ec9cc9421aba29ad54efe6a64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed263b1ae8746c79f63b530c8d7cb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee3e784a1854227b1d4f1bf5e56ddf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4e8d2c666f4b61a32b2e126fafa182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbfe76dbdb649d9bf48ccd3256a3da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6429cc9517d74a31bc421683b9e54418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92dda8054254113a31c3706593da9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb61865ad9554a5cbd203d901dba3ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79162d8dd57c4a0b8e4e4343c9902b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7cfbbceca14e2ba9b9d7cb43bad4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i d o t h i n k i n i n t h e s h o r t t e r m s e t t i n g s o m e t\n",
      "h i n g u p c o u l d b e s u p e r c o o l. s o m e o n e e l s e m e\n",
      "n t i o n e d t h a t 9 h u g o b o w n e - a n d e r s o n : w i t h r\n",
      "e s p e c t t o t h e t o o l o v e r w h e l m, i t ' d b e g r e a t.\n",
      "t h e y d o n ' t t h i n k t h a t n e c e s s a r i l y t h e y ' l l\n",
      "b e a b l e t o u s e a l l o f t h e m b y t h e e n d o f t h e c o u\n",
      "r s e, a n d t h a t ' s n o t a n e x p e c t a t i o n i t ' s m o r\n",
      "e f o\n",
      "\n",
      "Total chunks: 679\n"
     ]
    }
   ],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b0a83-5a52-4c79-87fb-873519e083d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import and Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path if needed\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "from vector_emb import (\n",
    "    answer_question, \n",
    "    llm_answer_question, \n",
    "    get_openai_client,\n",
    "    get_workshop_info,\n",
    "    get_collection_status,\n",
    "    format_sources\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ Imports and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcddb44-ecf6-4e81-8c44-fa51ac496b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 workshops: ['WS5', 'WS2', 'WS1', 'WS3', 'WS4', 'WS6']\n",
      "📚 Available workshops: ['WS5', 'WS2', 'WS1', 'WS3', 'WS4', 'WS6']\n",
      "📊 Total workshops: 6\n",
      "Retrieved existing collection 'workshop_chunks_all'\n",
      "Collection 'workshop_chunks_all' contains 193 total chunks\n",
      "Found 6 workshops: ['WS5', 'WS2', 'WS1', 'WS3', 'WS4', 'WS6']\n",
      "Workshop breakdown:\n",
      "  - WS5: 60 chunks ✓ Processed\n",
      "  - WS2: 1 chunks ✓ Processed\n",
      "  - WS1: 65 chunks ✓ Processed\n",
      "  - WS3: 1 chunks ✓ Processed\n",
      "  - WS4: 1 chunks ✓ Processed\n",
      "  - WS6: 65 chunks ✓ Processed\n",
      "\n",
      "📈 Collection status: {'total_chunks': 193, 'workshop_counts': {'WS5': 60, 'WS2': 1, 'WS1': 65, 'WS3': 1, 'WS4': 1, 'WS6': 65}}\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Check Workshop Status\n",
    "# See what workshops are available and if the collection is populated\n",
    "workshop_info = get_workshop_info()\n",
    "print(f\"📚 Available workshops: {workshop_info['workshop_ids']}\")\n",
    "print(f\"📊 Total workshops: {workshop_info['total_workshops']}\")\n",
    "\n",
    "collection_status = get_collection_status()\n",
    "print(f\"\\n📈 Collection status: {collection_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc8070-fcdb-452d-ae27-1c4c21f9f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Simple Q&A Function\n",
    "def ask_question(question, workshop_filter=None, show_sources=True):\n",
    "    \"\"\"\n",
    "    Ask a question and get an answer from the workshop transcripts\n",
    "    \n",
    "    Args:\n",
    "        question (str): Your question\n",
    "        workshop_filter (str or list): Filter by specific workshop(s), e.g., \"WS1\" or [\"WS1\", \"WS2\"]\n",
    "        show_sources (bool): Whether to display source information\n",
    "    \"\"\"\n",
    "    print(f\"🤔 Question: {question}\")\n",
    "    if workshop_filter:\n",
    "        print(f\"🎯 Filtering by workshop(s): {workshop_filter}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get relevant context and sources\n",
    "        context, sources, chunks = answer_question(question, workshop_filter=workshop_filter)\n",
    "        \n",
    "        if not context:\n",
    "            print(\"❌ No relevant context found for your question.\")\n",
    "            return\n",
    "        \n",
    "        # Generate LLM answer\n",
    "        client = get_openai_client()\n",
    "        answer, context_info = llm_answer_question(client, context, sources, chunks, question)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"🤖 Answer:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(answer)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if show_sources:\n",
    "            print(\"📚 Sources:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(format_sources([{\n",
    "                'workshop_id': source['workshop_id'],\n",
    "                'position': source['position'],\n",
    "                'speaker': source['speaker'],\n",
    "                'text': source['text'][:200] + \"...\" if len(source['text']) > 200 else source['text']\n",
    "            } for source in sources]))\n",
    "        \n",
    "        print(\"\\n📊 Context Info:\")\n",
    "        print(f\"- Chunks used: {context_info['num_chunks']}\")\n",
    "        print(f\"- Workshops referenced: {', '.join(context_info['workshops_used'])}\")\n",
    "        print(f\"- Context tokens: {context_info['context_tokens']}\")\n",
    "        print(f\"- Completion tokens: {context_info['completion_tokens']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "# Test the function\n",
    "ask_question(\"What is the main topic covered in the workshops?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de17440-fccd-46f3-b2a1-34b40fcdd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Interactive Q&A Loop\n",
    "def interactive_qa():\n",
    "    \"\"\"Run an interactive Q&A session\"\"\"\n",
    "    print(\"🎓 Workshop Q&A Session Started!\")\n",
    "    print(\"Type 'quit' to exit, 'workshops' to see available workshops\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n💭 Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"👋 Goodbye!\")\n",
    "            break\n",
    "        elif question.lower() == 'workshops':\n",
    "            info = get_workshop_info()\n",
    "            print(f\"Available workshops: {', '.join(info['workshop_ids'])}\")\n",
    "            continue\n",
    "        elif not question:\n",
    "            continue\n",
    "        \n",
    "        # Check if user wants to filter by workshop\n",
    "        workshop_filter = None\n",
    "        if question.startswith('@'):\n",
    "            parts = question.split(' ', 1)\n",
    "            if len(parts) == 2:\n",
    "                workshop_filter = parts[0][1:]  # Remove @ symbol\n",
    "                question = parts[1]\n",
    "                print(f\"🎯 Filtering by workshop: {workshop_filter}\")\n",
    "        \n",
    "        ask_question(question, workshop_filter=workshop_filter, show_sources=False)\n",
    "\n",
    "# Uncomment the line below to start interactive mode\n",
    "# interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1113e-7c12-458f-9283-195ad460f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Specific Workshop Questions\n",
    "# Example of asking questions about specific workshops\n",
    "\n",
    "# Ask about a specific workshop\n",
    "ask_question(\"What are the key concepts covered?\", workshop_filter=\"WS1\")\n",
    "\n",
    "# Ask about multiple workshops\n",
    "ask_question(\"What are the differences between the approaches?\", workshop_filter=[\"WS1\", \"WS2\"])\n",
    "\n",
    "# General question across all workshops\n",
    "ask_question(\"What are the most important takeaways?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
