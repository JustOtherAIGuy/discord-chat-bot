{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3aaa9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import glob\n",
    "from process_transcript import chunk_workshop_transcript, count_tokens, robust_chunk_workshop\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# Support both local and Modal paths - notebook compatible version\n",
    "# Check for Modal environment first, then use relative paths for local notebook\n",
    "if os.path.exists(\"/root/data\"):\n",
    "    DATA_DIR = \"/root/data\"\n",
    "    CHROMA_DB_PATH = \"/root/chroma_db\"\n",
    "else:\n",
    "    # For notebook environment, go up one directory from src to project root\n",
    "    current_dir = os.getcwd()\n",
    "    if current_dir.endswith('/src'):\n",
    "        project_root = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    DATA_DIR = os.path.join(project_root, \"data\")\n",
    "    CHROMA_DB_PATH = os.path.join(project_root, \"chroma_db\")\n",
    "\n",
    "COLLECTION_NAME = \"workshop_chunks_all\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "DEFAULT_MAX_TOKENS = 12000\n",
    "DEFAULT_MAX_CHUNKS = 5\n",
    "COMPLETION_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MAX_TOKENS = 7000\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful workshop assistant.\n",
    "Answer questions based only on the workshop transcript sections provided.\n",
    "If you don't know the answer or can't find it in the provided sections, say so.\n",
    "When referencing information, mention which workshop(s) the information comes from.\"\"\"\n",
    "\n",
    "def discover_workshops(data_dir=DATA_DIR):\n",
    "    \"\"\"Discover all workshop VTT files in the data directory\"\"\"\n",
    "    try:\n",
    "        pattern = os.path.join(data_dir, \"*.vtt\")\n",
    "        vtt_files = glob.glob(pattern)\n",
    "        \n",
    "        workshops = {}\n",
    "        for vtt_file in vtt_files:\n",
    "            filename = os.path.basename(vtt_file)\n",
    "            workshop_id = filename.split('-')[0] if '-' in filename else filename.split('.')[0]\n",
    "            \n",
    "            workshops[workshop_id] = {\n",
    "                'id': workshop_id,\n",
    "                'filename': filename,\n",
    "                'path': vtt_file\n",
    "            }\n",
    "        \n",
    "        return workshops\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering workshops: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2e07dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_vtt = discover_workshops(data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c719eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pastor/projects/discord-chat-bot/data/WS5-C2.vtt'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_vtt['WS5']['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "edfc019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vtt_content(file_path):\n",
    "    \"\"\"Load VTT file and extract clean text content\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    content_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if (not line or \n",
    "            line == 'WEBVTT' or \n",
    "            '-->' in line or \n",
    "            re.match(r'^\\d+:\\d+:\\d+', line) or\n",
    "            re.match(r'^[A-Z]+(\\s*:.*)?$', line)):\n",
    "            continue\n",
    "        content_lines.append(line)\n",
    "    \n",
    "    return \" \".join(content_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6614546b-286a-4eb6-93fb-f339d0493730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(encoding.encode(text))\n",
    "    except:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29d7da19-b0c2-4ee5-89e0-3b5c036c605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_chunk(text: str, max_tokens: int = EMBEDDING_MAX_TOKENS) -> List[str]:\n",
    "    \"\"\"Split a large chunk into smaller pieces that fit within token limits\"\"\"\n",
    "    if count_tokens(text) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    sentences = re.split(r'[.!?]+\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        if sentence_tokens > max_tokens:\n",
    "            words = sentence.split()\n",
    "            word_chunk = \"\"\n",
    "            word_tokens = 0\n",
    "            \n",
    "            for word in words:\n",
    "                word_token_count = count_tokens(word)\n",
    "                if word_tokens + word_token_count > max_tokens and word_chunk:\n",
    "                    chunks.append(word_chunk.strip())\n",
    "                    word_chunk = word\n",
    "                    word_tokens = word_token_count\n",
    "                else:\n",
    "                    word_chunk += \" \" + word if word_chunk else word\n",
    "                    word_tokens += word_token_count\n",
    "            \n",
    "            if word_chunk:\n",
    "                chunks.append(word_chunk.strip())\n",
    "            continue\n",
    "        \n",
    "        if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk += \". \" + sentence if current_chunk else sentence\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e66e9782-096b-4450-b9d4-a1133752746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding for text with safe token splitting\"\"\"\n",
    "    client = get_openai_client()\n",
    "    \n",
    "    token_count = count_tokens(text)\n",
    "    \n",
    "    if token_count <= EMBEDDING_MAX_TOKENS:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=EMBEDDING_MODEL\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    else:\n",
    "        split_texts = split_large_chunk(text, EMBEDDING_MAX_TOKENS)\n",
    "        \n",
    "        embeddings = []\n",
    "        for split_text in split_texts:\n",
    "            response = client.embeddings.create(\n",
    "                input=split_text,\n",
    "                model=EMBEDDING_MODEL\n",
    "            )\n",
    "            embeddings.append(response.data[0].embedding)\n",
    "        \n",
    "        avg_embedding = np.mean(embeddings, axis=0).tolist()\n",
    "        return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3879f29c-c1c6-4d4b-ab4b-c281901cb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_client():\n",
    "    \"\"\"Initialize OpenAI client with API key\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "    return OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c2c729cf-38e5-42cf-b5e5-ac1d4847421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "DEFAULT_MAX_TOKENS = 12000\n",
    "DEFAULT_MAX_CHUNKS = 5\n",
    "COMPLETION_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MAX_TOKENS = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad82ec32-edc6-470f-97da-a4132776573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121771\n",
      "30201\n",
      "e you in 46 h for the next workshop as well. So thanks everyone, and thanks for a great 1st session.\n",
      "transcript 1 e you in 46 h for the next workshop as well. So thanks everyone, and thanks for a great 1st session.\n",
      "embedding\n",
      "0.003587765106931329\n",
      "59521\n",
      "13381\n",
      "d build a session soon. And see you in the next workshop next week. Thanks, everyone. Thank you. Bye\n",
      "transcript 2 d build a session soon. And see you in the next workshop next week. Thanks, everyone. Thank you. Bye\n",
      "embedding\n",
      "0.01878167036920786\n",
      "106880\n",
      "23917\n",
      "nk you for all the great engagement and great questions. All right. Thanks, everyone. Thank you, Hik\n",
      "transcript 3 nk you for all the great engagement and great questions. All right. Thanks, everyone. Thank you, Hik\n",
      "embedding\n",
      "0.008148182008881122\n",
      "111936\n",
      "25005\n",
      "all in the next call and see you on Discord. So thanks once again, Eddie. Appreciate you, man. Yeah.\n",
      "transcript 4 all in the next call and see you on Discord. So thanks once again, Eddie. Appreciate you, man. Yeah.\n",
      "embedding\n",
      "0.0027106375200673938\n",
      "113025\n",
      "27532\n",
      "ouple of hours, so I can can do that all right. Thanks, everyone, and I'll see you in the next talk.\n",
      "transcript 5 ouple of hours, so I can can do that all right. Thanks, everyone, and I'll see you in the next talk.\n",
      "embedding\n",
      "0.008926754118874669\n",
      "119279\n",
      "29262\n",
      "ryone see you on the next call, and thanks. Once again Philip Ciao. 603 Phillip Carter: See you bye.\n",
      "transcript 6 ryone see you on the next call, and thanks. Once again Philip Ciao. 603 Phillip Carter: See you bye.\n",
      "embedding\n",
      "0.013556907186284661\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "for i in range(1, 7):\n",
    "    transcript_vtt = load_vtt_content(files_vtt[f'WS{i}']['path'])\n",
    "    print(len(transcript_vtt))\n",
    "    print(count_tokens(transcript_vtt))\n",
    "    chunk = split_large_chunk(transcript_vtt)\n",
    "    print(chunk[-1][-100::])\n",
    "    print(f\"transcript {i} {transcript_vtt[-100:]}\")\n",
    "    print(\"embedding\")\n",
    "    emb_trans = generate_embedding(transcript_vtt)\n",
    "    print(emb_trans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "73e7020e-9e0a-48b0-beb6-fe80a571220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_client():\n",
    "    \"\"\"Initialize and return a ChromaDB client with persistence\"\"\"\n",
    "    os.makedirs(CHROMA_DB_PATH, exist_ok=True)\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "    return client\n",
    "\n",
    "def get_or_create_collection(client, collection_name=COLLECTION_NAME):\n",
    "    \"\"\"Get or create a collection in ChromaDB\"\"\"\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "    except:\n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"Workshop transcript chunks\"}\n",
    "        )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c1d46113-483d-4813-8357-cbdb8e9ef4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunks_to_collection(collection, chunks, workshop_id):\n",
    "    \"\"\"Add multiple chunks to the collection with workshop metadata\"\"\"\n",
    "    ids = []\n",
    "    documents = []\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            embedding = generate_embedding(chunk['text'])\n",
    "            chunk_id = f\"{workshop_id}_{chunk['chunk_id']}\"\n",
    "            \n",
    "            ids.append(chunk_id)\n",
    "            documents.append(chunk['text'])\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "            metadata = {\n",
    "                'workshop_id': workshop_id,\n",
    "                'position': chunk['position'],\n",
    "                'token_count': chunk['token_count'],\n",
    "                'source': chunk['source'],\n",
    "                'timestamp': chunk.get('timestamp', 'Unknown'),\n",
    "                'speaker': chunk.get('speaker', 'Unknown'),\n",
    "                'original_chunk_id': chunk['chunk_id']\n",
    "            }\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    if ids:\n",
    "        try:\n",
    "            collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            return len(ids)\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4284baf8-2e18-4500-a83c-2c5af09d6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workshop(workshop_data: Dict[str, str], collection_name: str = COLLECTION_NAME) -> int:\n",
    "    \"\"\"Process a single workshop\"\"\"\n",
    "    try:\n",
    "        workshop_id = workshop_data['id']\n",
    "        workshop_path = workshop_data['path']\n",
    "        \n",
    "        client = get_chroma_client()\n",
    "        collection = get_or_create_collection(client, collection_name)\n",
    "        \n",
    "        # Check if already processed\n",
    "        try:\n",
    "            existing_results = collection.query(\n",
    "                query_embeddings=[[0.0] * 1536],\n",
    "                n_results=1,\n",
    "                where={\"workshop_id\": workshop_id}\n",
    "            )\n",
    "            if existing_results and existing_results['ids'] and len(existing_results['ids'][0]) > 0:\n",
    "                return 0\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        chunks = robust_chunk_workshop(workshop_path, workshop_id)\n",
    "        \n",
    "        if not chunks:\n",
    "            return 0\n",
    "        \n",
    "        num_added = add_chunks_to_collection(collection, chunks, workshop_id)\n",
    "        return num_added\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing workshop {workshop_data.get('id', 'Unknown')}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def process_all_workshops(collection_name=COLLECTION_NAME):\n",
    "    \"\"\"Process all discovered workshops\"\"\"\n",
    "    workshops = discover_workshops()\n",
    "    \n",
    "    if not workshops:\n",
    "        return []\n",
    "    \n",
    "    processed_workshops = []\n",
    "    \n",
    "    for workshop_id, workshop_info in workshops.items():\n",
    "        print(workshop_id, workshop_info)\n",
    "        try:\n",
    "            num_chunks = process_workshop(workshop_info, collection_name)\n",
    "            print(num_chunks)\n",
    "            if num_chunks > 0:\n",
    "                processed_workshops.append(workshop_id)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return processed_workshops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "799fa9f6-59b4-4899-9273-5d1b67ba2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS5 {'id': 'WS5', 'filename': 'WS5-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS5-C2.vtt'}\n",
      "0\n",
      "WS2 {'id': 'WS2', 'filename': 'WS2-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS2-C2.vtt'}\n",
      "0\n",
      "WS1 {'id': 'WS1', 'filename': 'WS1-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS1-C2.vtt'}\n",
      "0\n",
      "WS3 {'id': 'WS3', 'filename': 'WS3-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS3-C2.vtt'}\n",
      "0\n",
      "WS4 {'id': 'WS4', 'filename': 'WS4-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS4-C2.vtt'}\n",
      "0\n",
      "WS6 {'id': 'WS6', 'filename': 'WS6-C2.vtt', 'path': '/home/pastor/projects/discord-chat-bot/data/WS6-C2.vtt'}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_wsh = process_all_workshops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "38739656-cd84-4c51-94d2-36bb77978b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(collection, query_text, n_results=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Query the collection for relevant documents\"\"\"\n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    \n",
    "    query_params = {\n",
    "        \"query_embeddings\": [query_embedding],\n",
    "        \"n_results\": n_results\n",
    "    }\n",
    "    \n",
    "    if workshop_filter:\n",
    "        if isinstance(workshop_filter, str):\n",
    "            query_params[\"where\"] = {\"workshop_id\": workshop_filter}\n",
    "        elif isinstance(workshop_filter, list):\n",
    "            query_params[\"where\"] = {\"workshop_id\": {\"$in\": workshop_filter}}\n",
    "    \n",
    "    results = collection.query(**query_params)\n",
    "    return results\n",
    "\n",
    "def retrieve_relevant_chunks(question, collection_name=COLLECTION_NAME, n_results=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Retrieve chunks from vector database for a given question\"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, collection_name)\n",
    "    \n",
    "    results = query_collection(collection, question, n_results=n_results, workshop_filter=workshop_filter)\n",
    "    print(results)\n",
    "    \n",
    "    chunks = []\n",
    "    if results and 'documents' in results and results['documents'] and len(results['documents'][0]) > 0:\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            chunk = {\n",
    "                'text': results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'id': results['ids'][0][i],\n",
    "                'relevance': 1.0\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ffb16201-389f-411b-a95e-bcb45a8b6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(chunks, max_tokens=DEFAULT_MAX_TOKENS):\n",
    "    \"\"\"Combine multiple chunks into a single context\"\"\"\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    \n",
    "    sorted_chunks = sorted(chunks, key=lambda x: int(x['metadata'].get('position', 0)))\n",
    "    \n",
    "    combined_text = \"\"\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for chunk in sorted_chunks:\n",
    "        chunk_text = chunk['text']\n",
    "        chunk_tokens = int(chunk['metadata'].get('token_count', 0))\n",
    "        \n",
    "        if chunk_tokens == 0:\n",
    "            chunk_tokens = count_tokens(chunk_text)\n",
    "        \n",
    "        if total_tokens + chunk_tokens > max_tokens:\n",
    "            break\n",
    "        \n",
    "        if combined_text:\n",
    "            combined_text += \"\\n\\n--- Next Section ---\\n\\n\"\n",
    "        \n",
    "        combined_text += chunk_text\n",
    "        total_tokens += chunk_tokens\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "def get_context_for_question(question, collection_name=COLLECTION_NAME, max_chunks=DEFAULT_MAX_CHUNKS, workshop_filter=None):\n",
    "    \"\"\"Get relevant context from the vector database for a question\"\"\"\n",
    "    chunks = retrieve_relevant_chunks(question, collection_name, max_chunks, workshop_filter)\n",
    "    print(chunks)\n",
    "    \n",
    "    sources = []\n",
    "    for chunk in chunks:\n",
    "        metadata = chunk['metadata']\n",
    "        text = chunk['text']\n",
    "        \n",
    "        source = {\n",
    "            'position': metadata.get('position', 'Unknown'),\n",
    "            'timestamp': metadata.get('timestamp', \"Unknown\"),\n",
    "            'speaker': metadata.get('speaker', \"Unknown\"),\n",
    "            'workshop_id': metadata.get('workshop_id', \"Unknown\"),\n",
    "            'text': text,\n",
    "            'relevance': chunk.get('relevance')\n",
    "        }\n",
    "        sources.append(source)\n",
    "    \n",
    "    context = combine_chunks(chunks)\n",
    "    return context, sources, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "59d03755-0dd7-422a-b7b0-cdeeabf7f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, workshop_filter=None):\n",
    "    \"\"\"Answer a question based on workshop transcripts\"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, COLLECTION_NAME)\n",
    "    \n",
    "    # Check if collection is empty and populate if needed\n",
    "    count = collection.count()\n",
    "    if count == 0:\n",
    "        process_all_workshops(COLLECTION_NAME)\n",
    "    \n",
    "    context, sources, chunks = get_context_for_question(\n",
    "        question=question,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        max_chunks=DEFAULT_MAX_CHUNKS,\n",
    "        workshop_filter=workshop_filter\n",
    "    )\n",
    "    \n",
    "    return context, sources, chunks\n",
    "\n",
    "def llm_answer_question(client, context, sources, chunk, question):\n",
    "    \"\"\"Generate LLM answer with workshop awareness\"\"\"\n",
    "    num_chunks = len(sources)\n",
    "    workshops_used = list(set([source.get('workshop_id', 'Unknown') for source in sources]))\n",
    "    \n",
    "    client = get_openai_client()\n",
    "    try:\n",
    "        enhanced_system_prompt = SYSTEM_PROMPT + f\"\\n\\nThe information provided comes from workshops: {', '.join(workshops_used)}.\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=COMPLETION_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": enhanced_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Workshop Transcript Sections:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message.content\n",
    "        \n",
    "        completion_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 0\n",
    "        prompt_tokens = response.usage.prompt_tokens if hasattr(response, 'usage') else count_tokens(context)\n",
    "        \n",
    "        context_info = {\n",
    "             \"num_chunks\": num_chunks,\n",
    "             \"context_tokens\": prompt_tokens,\n",
    "             \"completion_tokens\": completion_tokens,\n",
    "             \"embedding_tokens\": num_chunks * 1536,\n",
    "             \"workshops_used\": workshops_used,\n",
    "             \"chunks\": chunks\n",
    "         }\n",
    "        \n",
    "        return message, context_info\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Sorry, an error occurred: {str(e)}\"\n",
    "        return error_message, {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "16d3a1bc-ddb1-4287-ad53-510ee1673543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Sorry, an error occurred: name 'chunks' is not defined\",\n",
       " {'error': \"name 'chunks' is not defined\"})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_answer_question(client, context, source, chunk, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cad11881-634f-437c-a469-ba3151bce275",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_chroma_client()\n",
    "collection = get_or_create_collection(client, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95a21e7a-47fd-48c7-b82c-4087089b72d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf16c2f-c8d8-49d0-9a28-22ee0af8f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_openai_client()\n",
    "\n",
    "token_count = count_tokens(text)\n",
    "\n",
    "if token_count <= EMBEDDING_MAX_TOKENS:\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "else:\n",
    "    split_texts = split_large_chunk(text, EMBEDDING_MAX_TOKENS)\n",
    "    \n",
    "    embeddings = []\n",
    "    for split_text in split_texts:\n",
    "        response = client.embeddings.create(\n",
    "            input=split_text,\n",
    "            model=EMBEDDING_MODEL\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224955e5-146d-414f-be44-a2c334be64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    query_embedding = generate_embedding(query_text)\n",
    "    \n",
    "    query_params = {\n",
    "        \"query_embeddings\": [query_embedding],\n",
    "        \"n_results\": n_results\n",
    "    }\n",
    "    \n",
    "    if workshop_filter:\n",
    "        if isinstance(workshop_filter, str):\n",
    "            query_params[\"where\"] = {\"workshop_id\": workshop_filter}\n",
    "        elif isinstance(workshop_filter, list):\n",
    "            query_params[\"where\"] = {\"workshop_id\": {\"$in\": workshop_filter}}\n",
    "    \n",
    "    results = collection.query(**query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e243ae8-14bd-42a5-a8f4-85c86530ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_chroma_client()\n",
    "collection = get_or_create_collection(client, collection_name)\n",
    "\n",
    "results = query_collection(collection, question, n_results=n_results, workshop_filter=workshop_filter)\n",
    "\n",
    "chunks = []\n",
    "if results and 'documents' in results and results['documents'] and len(results['documents'][0]) > 0:\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        chunk = {\n",
    "            'text': results['documents'][0][i],\n",
    "            'metadata': results['metadatas'][0][i],\n",
    "            'id': results['ids'][0][i],\n",
    "            'relevance': 1.0\n",
    "        }\n",
    "        chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b31a31-4e8c-4fbe-82b7-7433acfeea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    chunks = retrieve_relevant_chunks(question, collection_name, max_chunks, workshop_filter)\n",
    "    \n",
    "    sources = []\n",
    "    for chunk in chunks:\n",
    "        metadata = chunk['metadata']\n",
    "        text = chunk['text']\n",
    "        \n",
    "        source = {\n",
    "            'position': metadata.get('position', 'Unknown'),\n",
    "            'timestamp': metadata.get('timestamp', \"Unknown\"),\n",
    "            'speaker': metadata.get('speaker', \"Unknown\"),\n",
    "            'workshop_id': metadata.get('workshop_id', \"Unknown\"),\n",
    "            'text': text,\n",
    "            'relevance': chunk.get('relevance')\n",
    "        }\n",
    "        sources.append(source)\n",
    "    \n",
    "    context = combine_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09591b-9c72-4db3-b36a-62f6cb5c2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    client = get_chroma_client()\n",
    "    collection = get_or_create_collection(client, COLLECTION_NAME)\n",
    "    \n",
    "    # Check if collection is empty and populate if needed\n",
    "    count = collection.count()\n",
    "    if count == 0:\n",
    "        process_all_workshops(COLLECTION_NAME)\n",
    "    \n",
    "    context, sources, chunks = get_context_for_question(\n",
    "        question=question,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        max_chunks=DEFAULT_MAX_CHUNKS,\n",
    "        workshop_filter=workshop_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4777636-58bb-4fad-8966-39f1b0a4b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b326c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(transcript_vtt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3dae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_wrap(string, n_chars=72):\n",
    "    # Wrap a string at the next space after n_chars\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_wrap(character_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2421119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48daeda7fc4f45838678fc7ce384a309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291e32ec9cc9421aba29ad54efe6a64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed263b1ae8746c79f63b530c8d7cb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee3e784a1854227b1d4f1bf5e56ddf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4e8d2c666f4b61a32b2e126fafa182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbfe76dbdb649d9bf48ccd3256a3da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6429cc9517d74a31bc421683b9e54418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92dda8054254113a31c3706593da9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb61865ad9554a5cbd203d901dba3ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79162d8dd57c4a0b8e4e4343c9902b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7cfbbceca14e2ba9b9d7cb43bad4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i d o t h i n k i n i n t h e s h o r t t e r m s e t t i n g s o m e t\n",
      "h i n g u p c o u l d b e s u p e r c o o l. s o m e o n e e l s e m e\n",
      "n t i o n e d t h a t 9 h u g o b o w n e - a n d e r s o n : w i t h r\n",
      "e s p e c t t o t h e t o o l o v e r w h e l m, i t ' d b e g r e a t.\n",
      "t h e y d o n ' t t h i n k t h a t n e c e s s a r i l y t h e y ' l l\n",
      "b e a b l e t o u s e a l l o f t h e m b y t h e e n d o f t h e c o u\n",
      "r s e, a n d t h a t ' s n o t a n e x p e c t a t i o n i t ' s m o r\n",
      "e f o\n",
      "\n",
      "Total chunks: 679\n"
     ]
    }
   ],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b0a83-5a52-4c79-87fb-873519e083d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import and Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path if needed\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "from vector_emb import (\n",
    "    answer_question, \n",
    "    llm_answer_question, \n",
    "    get_openai_client,\n",
    "    get_workshop_info,\n",
    "    get_collection_status,\n",
    "    format_sources\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ Imports and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcddb44-ecf6-4e81-8c44-fa51ac496b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 workshops: ['WS5', 'WS2', 'WS1', 'WS3', 'WS4', 'WS6']\n",
      "📚 Available workshops: ['WS5', 'WS2', 'WS1', 'WS3', 'WS4', 'WS6']\n",
      "📊 Total workshops: 6\n",
      "Retrieved existing collection 'workshop_chunks_all'\n",
      "Collection 'workshop_chunks_all' contains 193 total chunks\n",
      "Found 6 workshops: ['WS5', 'WS2', 'WS1', 'WS3', 'WS4', 'WS6']\n",
      "Workshop breakdown:\n",
      "  - WS5: 60 chunks ✓ Processed\n",
      "  - WS2: 1 chunks ✓ Processed\n",
      "  - WS1: 65 chunks ✓ Processed\n",
      "  - WS3: 1 chunks ✓ Processed\n",
      "  - WS4: 1 chunks ✓ Processed\n",
      "  - WS6: 65 chunks ✓ Processed\n",
      "\n",
      "📈 Collection status: {'total_chunks': 193, 'workshop_counts': {'WS5': 60, 'WS2': 1, 'WS1': 65, 'WS3': 1, 'WS4': 1, 'WS6': 65}}\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Check Workshop Status\n",
    "# See what workshops are available and if the collection is populated\n",
    "workshop_info = get_workshop_info()\n",
    "print(f\"📚 Available workshops: {workshop_info['workshop_ids']}\")\n",
    "print(f\"📊 Total workshops: {workshop_info['total_workshops']}\")\n",
    "\n",
    "collection_status = get_collection_status()\n",
    "print(f\"\\n📈 Collection status: {collection_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc8070-fcdb-452d-ae27-1c4c21f9f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Simple Q&A Function\n",
    "def ask_question(question, workshop_filter=None, show_sources=True):\n",
    "    \"\"\"\n",
    "    Ask a question and get an answer from the workshop transcripts\n",
    "    \n",
    "    Args:\n",
    "        question (str): Your question\n",
    "        workshop_filter (str or list): Filter by specific workshop(s), e.g., \"WS1\" or [\"WS1\", \"WS2\"]\n",
    "        show_sources (bool): Whether to display source information\n",
    "    \"\"\"\n",
    "    print(f\"🤔 Question: {question}\")\n",
    "    if workshop_filter:\n",
    "        print(f\"🎯 Filtering by workshop(s): {workshop_filter}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get relevant context and sources\n",
    "        context, sources, chunks = answer_question(question, workshop_filter=workshop_filter)\n",
    "        \n",
    "        if not context:\n",
    "            print(\"❌ No relevant context found for your question.\")\n",
    "            return\n",
    "        \n",
    "        # Generate LLM answer\n",
    "        client = get_openai_client()\n",
    "        answer, context_info = llm_answer_question(client, context, sources, chunks, question)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"🤖 Answer:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(answer)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if show_sources:\n",
    "            print(\"📚 Sources:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(format_sources([{\n",
    "                'workshop_id': source['workshop_id'],\n",
    "                'position': source['position'],\n",
    "                'speaker': source['speaker'],\n",
    "                'text': source['text'][:200] + \"...\" if len(source['text']) > 200 else source['text']\n",
    "            } for source in sources]))\n",
    "        \n",
    "        print(\"\\n📊 Context Info:\")\n",
    "        print(f\"- Chunks used: {context_info['num_chunks']}\")\n",
    "        print(f\"- Workshops referenced: {', '.join(context_info['workshops_used'])}\")\n",
    "        print(f\"- Context tokens: {context_info['context_tokens']}\")\n",
    "        print(f\"- Completion tokens: {context_info['completion_tokens']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "# Test the function\n",
    "ask_question(\"What is the main topic covered in the workshops?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de17440-fccd-46f3-b2a1-34b40fcdd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Interactive Q&A Loop\n",
    "def interactive_qa():\n",
    "    \"\"\"Run an interactive Q&A session\"\"\"\n",
    "    print(\"🎓 Workshop Q&A Session Started!\")\n",
    "    print(\"Type 'quit' to exit, 'workshops' to see available workshops\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n💭 Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"👋 Goodbye!\")\n",
    "            break\n",
    "        elif question.lower() == 'workshops':\n",
    "            info = get_workshop_info()\n",
    "            print(f\"Available workshops: {', '.join(info['workshop_ids'])}\")\n",
    "            continue\n",
    "        elif not question:\n",
    "            continue\n",
    "        \n",
    "        # Check if user wants to filter by workshop\n",
    "        workshop_filter = None\n",
    "        if question.startswith('@'):\n",
    "            parts = question.split(' ', 1)\n",
    "            if len(parts) == 2:\n",
    "                workshop_filter = parts[0][1:]  # Remove @ symbol\n",
    "                question = parts[1]\n",
    "                print(f\"🎯 Filtering by workshop: {workshop_filter}\")\n",
    "        \n",
    "        ask_question(question, workshop_filter=workshop_filter, show_sources=False)\n",
    "\n",
    "# Uncomment the line below to start interactive mode\n",
    "# interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1113e-7c12-458f-9283-195ad460f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Specific Workshop Questions\n",
    "# Example of asking questions about specific workshops\n",
    "\n",
    "# Ask about a specific workshop\n",
    "ask_question(\"What are the key concepts covered?\", workshop_filter=\"WS1\")\n",
    "\n",
    "# Ask about multiple workshops\n",
    "ask_question(\"What are the differences between the approaches?\", workshop_filter=[\"WS1\", \"WS2\"])\n",
    "\n",
    "# General question across all workshops\n",
    "ask_question(\"What are the most important takeaways?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
